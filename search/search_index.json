{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS Certified Solutions Architect \u2013 Professional (SAP-C01) Exam Guide Version 2.0 SAP-C01 1 Introduction The AWS Certified Solutions Architect \u2013 Professional (SAP-C01) exam is intended for individuals who perform a solutions architect role. The exam validates a candidate\u2019s advanced technical skills and experience in designing distributed applications and systems on the AWS platform. The exam also validates a candidate\u2019s ability to complete the following tasks: Design and deploy dynamically scalable, highly available, fault-tolerant, and reliable applications on AWS Select appropriate AWS services to design and deploy an application based on given requirements Migrate complex, multi-tier applications on AWS Design and deploy enterprise-wide scalable operations on AWS Implement cost-control strategies Target candidate description The target candidate should have 2 or more years of experience designing and deploying cloud architecture on AWS. The target candidate has the ability to evaluate cloud application requirements and make architectural recommendations for implementation, deployment, and provisioning applications on AWS. The target candidate is capable of providing best practice guidance on architectural design spanning multiple applications and projects, or an enterprise. Recommended AWS knowledge The target candidate should have the following knowledge: Explain and apply the five pillars of the AWS Well-Architected Framework Map business objectives to application/architecture requirements Design a hybrid architecture using key AWS technologies Architect a continuous integration/continuous delivery (CI/CD) process What is considered out of scope for the target candidate? The following is a non-exhaustive list of related job tasks that the target candidate is not expected to be able to perform. These items are considered out of scope for the exam: Machine learning Amazon GameLift Front-end development for mobile apps 12-factor app methodology In-depth knowledge of operating systems Enterprise applications Database schemas for high-scale persistent stores To view a detailed list of specific tools and technologies that might be covered on the exam, as well as lists of in-scope and out-of-scope AWS services, refer to the Appendix. Exam content Response types There are two types of questions on the exam: Multiple choice : Has one correct response and three incorrect responses (distractors) Multiple response : Has two or more correct responses out of five or more response options Select one or more responses that best completes the statement or answers the question. Distractors, orincorrect answers, are response options that a candidate with incomplete knowledge or skill might choose. Distractors are generally plausible responses that match the content area. Unanswered questions are scored as incorrect; there is no penalty for guessing. The exam includes 65 questions that will affect your score. Unscored content The exam includes 10 unscored questions that do not affect your score. AWS collects information about candidate performance on these unscored questions to evaluate these questions for future use as scored questions. These unscored questions are not identified on the exam. Exam results The AWS Certified Solutions Architect \u2013 Professional (SAP-C01) exam is a pass or fail exam. The exam is scored against a minimum standard established by AWS professionals who follow certification industry best practices and guidelines. Your results for the exam are reported as a scaled score of 100\u20131,000. The minimum passing score is 750. Your score shows how you performed on the exam as a whole and whether or not you passed. Scaled scoring models help equate scores across multiple exam forms that might have slightly different difficulty levels. Your score report may contain a table of classifications of your performance at each section level. This information is intended to provide general feedback about your exam performance. The exam uses a compensatory scoring model, which means that you do not need to achieve a passing score in each section. You need to pass only the overall exam. Each section of the exam has a specific weighting, so some sections have more questions than others. The table contains general information that highlights your strengths and weaknesses. Use caution when interpreting section-level feedback. Content outline This exam guide includes weightings, test domains, and objectives for the exam. It is not a comprehensive listing of the content on the exam. However, additional context for each of the objectives is available to help guide your preparation for the exam. The following table lists the main content domains and their weightings. The table precedes the complete exam content outline, which includes the additional context. The percentage in each domain represents only scored content. Domain % of Exam Domain 1: Design for Organizational Complexity 12.5% Domain 2: Design for New Solutions 31% Domain 3: Migration Planning 15% Domain 4: Cost Control 12.5% Domain 5: Continuous Improvement 29% TOTAL 100% Domain 1: Design for Organizational Complexity 1.1. Determine cross-account authentication and access strategy for complex organizations. Analyze the organizational structure Evaluate the current authentication infrastructure Analyze the AWS resources at an account level Determine an auditing strategy for authentication and access 1.2 Determine how to design networks for complex organizations. Outline an IP addressing strategy for VPCs Determine DNS strategy Classify network traffic and security Determine connectivity needs for hybrid environments Determine a way to audit network traffic 1.3 Determine how to design a multi-account AWS environment for complex organizations. Determine how to use AWS Organizations Implement the most appropriate account structure for proper cost allocation, agility, and security Recommend a central audit and event notification strategy Decide on an access strategy Domain 2: Design for New Solutions 2.1 Determine security requirements and controls when designing and implementing a solution. Implement infrastructure as code Determine prevention controls for large-scale web applications Determine roles and responsibilities of applications Determine a secure method to manage credentials for the solutions/applications Enable detection controls and security services for large-scale applications Enforce host and network security boundaries Enable encryption in transit and at rest 2.2 Determine a solution design and implementation strategy to meet reliability requirements. Design a highly available application environment Determine advanced techniques to detect for failure and service recoverability Determine processes and components to monitor and recover from regional service disruptions with regional failover 2.3 Determine a solution design to ensure business continuity. Architect an automated, cost-effective back-up solution that supports business continuity across multiple AWS Regions Determine an architecture that provides application and infrastructure availability in the event of a service disruption 2.4 Determine a solution design to meet performance objectives. Design internet-scale application architectures Design an architecture for performance according to business objectives Apply design patterns to meet business objectives with caches, buffering, and replicas 2.5 Determine a deployment strategy to meet business requirements when designing and implementing a solution. Determine resource provisioning strategy to meet business objectives Determine a migration process to change the version of a servicew Determine services to meet deployment strategy Determine patch management strategy Domain 3: Migration Planning 3.1 Select existing workloads and processes for potential migration to the cloud. Complete an application migration assessment Classify applications according to the six Rs (re-host, re-platform, re-purchase, refactor, retire, and retain) 3.2 Select migration tools and/or services for new and migrated solutions based on detailed AWS knowledge. Select an appropriate database transfer mechanism Select an appropriate data transfer service Select an appropriate data transfer target Select an appropriate server migration mechanism Apply the appropriate security methods to the migration tools 3.3 Determine a new cloud architecture for an existing solution. Evaluate business applications and determine the target cloud architecture Break down the functionality of applications into services Determine target database platforms 3.4 Determine a strategy for migrating existing on-premises workloads to the cloud. Determine the desired prioritization strategy of the organization Analyze data volume and rate of change to determine a data transfer strategy Evaluate cutover strategies Assess internal and external compliance requirements for a successful migration Domain 4: Cost Control 4.1 Select a cost-effective pricing model for a solution. Purchase resources based on usage requirements Identify when to use different storage tiers 4.2 Determine which controls to design and implement that will ensure cost optimization. Determine an AWS-generated cost allocation tags strategy that allows mapping cost to business units Determine a mechanism to monitor when underutilized resources are present Determine a way to manage commonly deployed resources to achieve governance Define a way to plan costs that do not exceed the budget amount 4.3 Identify opportunities to reduce cost in an existing architecture. Distinguish opportunities to use AWS Managed Services Determine which services are most cost-effective in meeting business objectives Domain 5: Continuous Improvement for Existing Solutions 5.1 Troubleshoot solutions architectures. Assess an existing application architecture for deficiencies Analyze application and infrastructure logs Test possible solutions in non-production environment 5.2 Determine a strategy to improve an existing solution for operational excellence. Determine the most appropriate logging and monitoring strategy Recommend the appropriate AWS offering(s) to enable configuration management automation 5.3 Determine a strategy to improve the reliability of an existing solution. Evaluate existing architecture to determine areas that are not sufficiently reliable Remediate single points of failure Enable data replication, self-healing, and elastic features and services Test the reliability of the new solution 5.4 Determine a strategy to improve the performance of an existing solution. Reconcile current performance metrics against performance targets Identify and examine performance bottlenecks Recommend and test potential remediation solutions 5.5 Determine a strategy to improve the security of an existing solution. Evaluate AWS Secrets Manager strategy Audit the environment for security vulnerabilities Enable manual and/or automated responses to the detection of vulnerabilities 5.6 Determine how to improve the deployment of an existing solution. Evaluate appropriate tooling to enable infrastructure as code Evaluate current deployment processes for improvement opportunities Test automated deployment and rollback strategies Appendix Which key tools, technologies, and concepts might be covered on the exam? The following is a non-exhaustive list of the tools and technologies that could appear on the exam. This list is subject to change and is provided to help you understand the general scope of services, features, or technologies on the exam. The general tools and technologies in this list appear in no particular order. AWS services are grouped according to their primary functions. While some of these technologies will likely be covered more than others on the exam, the order and placement of them in this list is no indication of relative weight or importance: Compute Cost management Database Disaster recovery High availability Management and governance Microservices and component decoupling Migration and data transfer Networking, connectivity, and content delivery Security Serverless design principles Storage AWS services and features Analytics: Amazon Athena Amazon Elasticsearch Service Amazon EMR AWS Glue Amazon Kinesis Amazon QuickSight AWS Billing and Cost Management: AWS Budgets Cost Explorer Application integration: Amazon MQ Amazon Simple Notification Service (Amazon SNS) Amazon Simple Queue Service (Amazon SQS) AWS Step Functions Business applications: Amazon Alexa Amazon Alexa for Business Amazon Simple Email Service (Amazon SES) Blockchain: Amazon Managed Blockchain Compute: AWS Batch Amazon EC2 AWS Elastic Beanstalk Amazon Elastic Container Service (Amazon ECS) Amazon Elastic Kubernetes Service (Amazon EKS) Elastic Load Balancing AWS Fargate AWS Lambda Amazon Lightsail AWS Outposts Containers: Amazon Elastic Container Registry (Amazon ECR) Database: Amazon Aurora Amazon DynamoDB Amazon ElastiCache Amazon Neptune Amazon RDS Amazon Redshift Developer tools: AWS Cloud9 AWS CodeBuild AWS CodeCommit AWS CodeDeploy AWS CodePipeline End user computing: Amazon AppStream 2.0 Amazon WorkSpaces Front-end web and mobile: AWS AppSync Machine learning: Amazon Comprehend Amazon Forecast Amazon Lex Amazon Rekognition Amazon SageMaker Amazon Transcribe Amazon Translate Management and governance: AWS Auto Scaling AWS Backup AWS CloudFormation AWS CloudTrail Amazon CloudWatch AWS Compute Optimizer AWS Config AWS Control Tower Amazon EventBridge AWS License Manager AWS Organizations AWS Resource Access Manager AWS Service Catalog AWS Systems Manager AWS Trusted Advisor AWS Well-Architected Tool Media services: Amazon Elastic Transcoder Migration and transfer: AWS Database Migration Service (AWS DMS) AWS DataSync AWS Migration Hub AWS Server Migration Service (AWS SMS) AWS Snowball AWS Transfer Family Networking and content delivery: Amazon API Gateway Amazon CloudFront AWS Direct Connect AWS Global Accelerator Amazon Route 53 AWS Transit Gateway Amazon VPC Security, identity, and compliance: AWS Artifact AWS Certificate Manager (ACM) Amazon Cognito AWS Directory Service Amazon GuardDuty AWS Identity and Access Management (IAM) Amazon Inspector AWS Key Management Service (AWS KMS) Amazon Macie AWS Resource Access Manager AWS Secrets Manager AWS Security Hub AWS Shield AWS Single Sign-On AWS WAF Storage: Amazon Elastic Block Store (Amazon EBS) Amazon Elastic File System (Amazon EFS) Amazon FSx Amazon S3 Amazon S3 Glacier AWS Storage Gateway","title":"Home"},{"location":"#introduction","text":"The AWS Certified Solutions Architect \u2013 Professional (SAP-C01) exam is intended for individuals who perform a solutions architect role. The exam validates a candidate\u2019s advanced technical skills and experience in designing distributed applications and systems on the AWS platform. The exam also validates a candidate\u2019s ability to complete the following tasks: Design and deploy dynamically scalable, highly available, fault-tolerant, and reliable applications on AWS Select appropriate AWS services to design and deploy an application based on given requirements Migrate complex, multi-tier applications on AWS Design and deploy enterprise-wide scalable operations on AWS Implement cost-control strategies","title":"Introduction"},{"location":"#target-candidate-description","text":"The target candidate should have 2 or more years of experience designing and deploying cloud architecture on AWS. The target candidate has the ability to evaluate cloud application requirements and make architectural recommendations for implementation, deployment, and provisioning applications on AWS. The target candidate is capable of providing best practice guidance on architectural design spanning multiple applications and projects, or an enterprise.","title":"Target candidate description"},{"location":"#recommended-aws-knowledge","text":"The target candidate should have the following knowledge: Explain and apply the five pillars of the AWS Well-Architected Framework Map business objectives to application/architecture requirements Design a hybrid architecture using key AWS technologies Architect a continuous integration/continuous delivery (CI/CD) process","title":"Recommended AWS knowledge"},{"location":"#what-is-considered-out-of-scope-for-the-target-candidate","text":"The following is a non-exhaustive list of related job tasks that the target candidate is not expected to be able to perform. These items are considered out of scope for the exam: Machine learning Amazon GameLift Front-end development for mobile apps 12-factor app methodology In-depth knowledge of operating systems Enterprise applications Database schemas for high-scale persistent stores To view a detailed list of specific tools and technologies that might be covered on the exam, as well as lists of in-scope and out-of-scope AWS services, refer to the Appendix.","title":"What is considered out of scope for the target candidate?"},{"location":"#exam-content","text":"","title":"Exam content"},{"location":"#response-types","text":"There are two types of questions on the exam: Multiple choice : Has one correct response and three incorrect responses (distractors) Multiple response : Has two or more correct responses out of five or more response options Select one or more responses that best completes the statement or answers the question. Distractors, orincorrect answers, are response options that a candidate with incomplete knowledge or skill might choose. Distractors are generally plausible responses that match the content area. Unanswered questions are scored as incorrect; there is no penalty for guessing. The exam includes 65 questions that will affect your score.","title":"Response types"},{"location":"#unscored-content","text":"The exam includes 10 unscored questions that do not affect your score. AWS collects information about candidate performance on these unscored questions to evaluate these questions for future use as scored questions. These unscored questions are not identified on the exam.","title":"Unscored content"},{"location":"#exam-results","text":"The AWS Certified Solutions Architect \u2013 Professional (SAP-C01) exam is a pass or fail exam. The exam is scored against a minimum standard established by AWS professionals who follow certification industry best practices and guidelines. Your results for the exam are reported as a scaled score of 100\u20131,000. The minimum passing score is 750. Your score shows how you performed on the exam as a whole and whether or not you passed. Scaled scoring models help equate scores across multiple exam forms that might have slightly different difficulty levels. Your score report may contain a table of classifications of your performance at each section level. This information is intended to provide general feedback about your exam performance. The exam uses a compensatory scoring model, which means that you do not need to achieve a passing score in each section. You need to pass only the overall exam. Each section of the exam has a specific weighting, so some sections have more questions than others. The table contains general information that highlights your strengths and weaknesses. Use caution when interpreting section-level feedback.","title":"Exam results"},{"location":"#content-outline","text":"This exam guide includes weightings, test domains, and objectives for the exam. It is not a comprehensive listing of the content on the exam. However, additional context for each of the objectives is available to help guide your preparation for the exam. The following table lists the main content domains and their weightings. The table precedes the complete exam content outline, which includes the additional context. The percentage in each domain represents only scored content. Domain % of Exam Domain 1: Design for Organizational Complexity 12.5% Domain 2: Design for New Solutions 31% Domain 3: Migration Planning 15% Domain 4: Cost Control 12.5% Domain 5: Continuous Improvement 29% TOTAL 100%","title":"Content outline"},{"location":"#domain-1-design-for-organizational-complexity","text":"1.1. Determine cross-account authentication and access strategy for complex organizations. Analyze the organizational structure Evaluate the current authentication infrastructure Analyze the AWS resources at an account level Determine an auditing strategy for authentication and access 1.2 Determine how to design networks for complex organizations. Outline an IP addressing strategy for VPCs Determine DNS strategy Classify network traffic and security Determine connectivity needs for hybrid environments Determine a way to audit network traffic 1.3 Determine how to design a multi-account AWS environment for complex organizations. Determine how to use AWS Organizations Implement the most appropriate account structure for proper cost allocation, agility, and security Recommend a central audit and event notification strategy Decide on an access strategy","title":"Domain 1: Design for Organizational Complexity"},{"location":"#domain-2-design-for-new-solutions","text":"2.1 Determine security requirements and controls when designing and implementing a solution. Implement infrastructure as code Determine prevention controls for large-scale web applications Determine roles and responsibilities of applications Determine a secure method to manage credentials for the solutions/applications Enable detection controls and security services for large-scale applications Enforce host and network security boundaries Enable encryption in transit and at rest 2.2 Determine a solution design and implementation strategy to meet reliability requirements. Design a highly available application environment Determine advanced techniques to detect for failure and service recoverability Determine processes and components to monitor and recover from regional service disruptions with regional failover 2.3 Determine a solution design to ensure business continuity. Architect an automated, cost-effective back-up solution that supports business continuity across multiple AWS Regions Determine an architecture that provides application and infrastructure availability in the event of a service disruption 2.4 Determine a solution design to meet performance objectives. Design internet-scale application architectures Design an architecture for performance according to business objectives Apply design patterns to meet business objectives with caches, buffering, and replicas 2.5 Determine a deployment strategy to meet business requirements when designing and implementing a solution. Determine resource provisioning strategy to meet business objectives Determine a migration process to change the version of a servicew Determine services to meet deployment strategy Determine patch management strategy","title":"Domain 2: Design for New Solutions"},{"location":"#domain-3-migration-planning","text":"3.1 Select existing workloads and processes for potential migration to the cloud. Complete an application migration assessment Classify applications according to the six Rs (re-host, re-platform, re-purchase, refactor, retire, and retain) 3.2 Select migration tools and/or services for new and migrated solutions based on detailed AWS knowledge. Select an appropriate database transfer mechanism Select an appropriate data transfer service Select an appropriate data transfer target Select an appropriate server migration mechanism Apply the appropriate security methods to the migration tools 3.3 Determine a new cloud architecture for an existing solution. Evaluate business applications and determine the target cloud architecture Break down the functionality of applications into services Determine target database platforms 3.4 Determine a strategy for migrating existing on-premises workloads to the cloud. Determine the desired prioritization strategy of the organization Analyze data volume and rate of change to determine a data transfer strategy Evaluate cutover strategies Assess internal and external compliance requirements for a successful migration","title":"Domain 3: Migration Planning"},{"location":"#domain-4-cost-control","text":"4.1 Select a cost-effective pricing model for a solution. Purchase resources based on usage requirements Identify when to use different storage tiers 4.2 Determine which controls to design and implement that will ensure cost optimization. Determine an AWS-generated cost allocation tags strategy that allows mapping cost to business units Determine a mechanism to monitor when underutilized resources are present Determine a way to manage commonly deployed resources to achieve governance Define a way to plan costs that do not exceed the budget amount 4.3 Identify opportunities to reduce cost in an existing architecture. Distinguish opportunities to use AWS Managed Services Determine which services are most cost-effective in meeting business objectives","title":"Domain 4: Cost Control"},{"location":"#domain-5-continuous-improvement-for-existing-solutions","text":"5.1 Troubleshoot solutions architectures. Assess an existing application architecture for deficiencies Analyze application and infrastructure logs Test possible solutions in non-production environment 5.2 Determine a strategy to improve an existing solution for operational excellence. Determine the most appropriate logging and monitoring strategy Recommend the appropriate AWS offering(s) to enable configuration management automation 5.3 Determine a strategy to improve the reliability of an existing solution. Evaluate existing architecture to determine areas that are not sufficiently reliable Remediate single points of failure Enable data replication, self-healing, and elastic features and services Test the reliability of the new solution 5.4 Determine a strategy to improve the performance of an existing solution. Reconcile current performance metrics against performance targets Identify and examine performance bottlenecks Recommend and test potential remediation solutions 5.5 Determine a strategy to improve the security of an existing solution. Evaluate AWS Secrets Manager strategy Audit the environment for security vulnerabilities Enable manual and/or automated responses to the detection of vulnerabilities 5.6 Determine how to improve the deployment of an existing solution. Evaluate appropriate tooling to enable infrastructure as code Evaluate current deployment processes for improvement opportunities Test automated deployment and rollback strategies","title":"Domain 5: Continuous Improvement for Existing Solutions"},{"location":"#appendix","text":"","title":"Appendix"},{"location":"#which-key-tools-technologies-and-concepts-might-be-covered-on-the-exam","text":"The following is a non-exhaustive list of the tools and technologies that could appear on the exam. This list is subject to change and is provided to help you understand the general scope of services, features, or technologies on the exam. The general tools and technologies in this list appear in no particular order. AWS services are grouped according to their primary functions. While some of these technologies will likely be covered more than others on the exam, the order and placement of them in this list is no indication of relative weight or importance: Compute Cost management Database Disaster recovery High availability Management and governance Microservices and component decoupling Migration and data transfer Networking, connectivity, and content delivery Security Serverless design principles Storage","title":"Which key tools, technologies, and concepts might be covered on the exam?"},{"location":"#aws-services-and-features","text":"Analytics: Amazon Athena Amazon Elasticsearch Service Amazon EMR AWS Glue Amazon Kinesis Amazon QuickSight AWS Billing and Cost Management: AWS Budgets Cost Explorer Application integration: Amazon MQ Amazon Simple Notification Service (Amazon SNS) Amazon Simple Queue Service (Amazon SQS) AWS Step Functions Business applications: Amazon Alexa Amazon Alexa for Business Amazon Simple Email Service (Amazon SES) Blockchain: Amazon Managed Blockchain Compute: AWS Batch Amazon EC2 AWS Elastic Beanstalk Amazon Elastic Container Service (Amazon ECS) Amazon Elastic Kubernetes Service (Amazon EKS) Elastic Load Balancing AWS Fargate AWS Lambda Amazon Lightsail AWS Outposts Containers: Amazon Elastic Container Registry (Amazon ECR) Database: Amazon Aurora Amazon DynamoDB Amazon ElastiCache Amazon Neptune Amazon RDS Amazon Redshift Developer tools: AWS Cloud9 AWS CodeBuild AWS CodeCommit AWS CodeDeploy AWS CodePipeline End user computing: Amazon AppStream 2.0 Amazon WorkSpaces Front-end web and mobile: AWS AppSync Machine learning: Amazon Comprehend Amazon Forecast Amazon Lex Amazon Rekognition Amazon SageMaker Amazon Transcribe Amazon Translate Management and governance: AWS Auto Scaling AWS Backup AWS CloudFormation AWS CloudTrail Amazon CloudWatch AWS Compute Optimizer AWS Config AWS Control Tower Amazon EventBridge AWS License Manager AWS Organizations AWS Resource Access Manager AWS Service Catalog AWS Systems Manager AWS Trusted Advisor AWS Well-Architected Tool Media services: Amazon Elastic Transcoder Migration and transfer: AWS Database Migration Service (AWS DMS) AWS DataSync AWS Migration Hub AWS Server Migration Service (AWS SMS) AWS Snowball AWS Transfer Family Networking and content delivery: Amazon API Gateway Amazon CloudFront AWS Direct Connect AWS Global Accelerator Amazon Route 53 AWS Transit Gateway Amazon VPC Security, identity, and compliance: AWS Artifact AWS Certificate Manager (ACM) Amazon Cognito AWS Directory Service Amazon GuardDuty AWS Identity and Access Management (IAM) Amazon Inspector AWS Key Management Service (AWS KMS) Amazon Macie AWS Resource Access Manager AWS Secrets Manager AWS Security Hub AWS Shield AWS Single Sign-On AWS WAF Storage: Amazon Elastic Block Store (Amazon EBS) Amazon Elastic File System (Amazon EFS) Amazon FSx Amazon S3 Amazon S3 Glacier AWS Storage Gateway","title":"AWS services and features"},{"location":"aws-sa-pro-slack/","text":"questions topic AWS MSK (kafka), DataSync, many Redshift and EMR questions and RAM. policy questions where you had to decifer policy code EC2, networking, ECB \"availability\" questions and migration questions. There were lots and lots and LOTS of questions on: AWS Organisations Migrations RAM TGW ECS GWLB, Client VPN, S3, GA, DDB, CodeDeploy, CodePipeline, Transfer Family, RDS Proxy Many had an Organizations angle, I had MSK too few on prem/cloud connectivity and DR, DX, VPN, and Transit GW DDB for global availability and Global tables in the middle of the answers, but the words 'data corruption..' were in the Q another very similar answer included backup which would allow recovery from corruptions. AWS Organizations (easily 5-10), networking (VPN, Direct Connect, DC & Transit Gateway, VPC Peering), Aurora vs RDS and DynamoDB, S3 access logs, CF Stack Sets, Storage Gateway, Resource Sharing, Transfer Family Lots of organisation / RAM questions/ cross region services and a question on GWLB. , lots and lots of questions about DX so make sure you have that down 3 questions about RDS proxy as well, so worth having a look at that. AWS Wavelength was a distractor used in two questions, and ground station was used to distract from a simple data migration job from a site to S3 one question about Fully Managed Apache Kafka (MSK) -- HA or fault tolerant ??? I recall one question on AWS Control Tower one question where 2 options were to send the notification by SNS vs 2 options by SES question where 2 options were with \u201cs3 sync\u201d and 2 options with Datasync agent. Direct Connect, SAML, Disaster Recovery (RTO/RPO) best options, Hybrid Cloud, AWS Organization, RDS Connection Pool (RDS Proxy), Secret Management, DDoS and Web Protection, Client VPN, Route53 Resolver Endpoints, and one Question on Workspace whitepapers AWS Organizations User Guide, Building a Scalable and Secure Multi-VPC AWS Network Infrastructure Whitepaper, and Serverless Architectures with AWS Lambda Whitepaper.","title":"Aws sa pro slack"},{"location":"aws-sa-pro-slack/#questions-topic","text":"AWS MSK (kafka), DataSync, many Redshift and EMR questions and RAM. policy questions where you had to decifer policy code EC2, networking, ECB \"availability\" questions and migration questions. There were lots and lots and LOTS of questions on: AWS Organisations Migrations RAM TGW ECS GWLB, Client VPN, S3, GA, DDB, CodeDeploy, CodePipeline, Transfer Family, RDS Proxy Many had an Organizations angle, I had MSK too few on prem/cloud connectivity and DR, DX, VPN, and Transit GW DDB for global availability and Global tables in the middle of the answers, but the words 'data corruption..' were in the Q another very similar answer included backup which would allow recovery from corruptions. AWS Organizations (easily 5-10), networking (VPN, Direct Connect, DC & Transit Gateway, VPC Peering), Aurora vs RDS and DynamoDB, S3 access logs, CF Stack Sets, Storage Gateway, Resource Sharing, Transfer Family Lots of organisation / RAM questions/ cross region services and a question on GWLB. , lots and lots of questions about DX so make sure you have that down 3 questions about RDS proxy as well, so worth having a look at that. AWS Wavelength was a distractor used in two questions, and ground station was used to distract from a simple data migration job from a site to S3 one question about Fully Managed Apache Kafka (MSK) -- HA or fault tolerant ??? I recall one question on AWS Control Tower one question where 2 options were to send the notification by SNS vs 2 options by SES question where 2 options were with \u201cs3 sync\u201d and 2 options with Datasync agent. Direct Connect, SAML, Disaster Recovery (RTO/RPO) best options, Hybrid Cloud, AWS Organization, RDS Connection Pool (RDS Proxy), Secret Management, DDoS and Web Protection, Client VPN, Route53 Resolver Endpoints, and one Question on Workspace","title":"questions topic"},{"location":"aws-sa-pro-slack/#whitepapers","text":"AWS Organizations User Guide, Building a Scalable and Secure Multi-VPC AWS Network Infrastructure Whitepaper, and Serverless Architectures with AWS Lambda Whitepaper.","title":"whitepapers"},{"location":"neal-davis/","text":"Migraiton tools https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-tools/aws-services.html AWS Migration Hub AWS Migration Hub provides a single place to store IT asset inventory data while tracking migrations to any AWS Region. After migration, use Migration Hub to accelerate the transformation of your applications to native AWS Centralized tracking Migration flexibility Discovery, assessment, and planning Fast-track application refactor AWS cloud Adoption Readiness Tool (CART) https://cloudreadiness.amazonaws.com/#/cart Completing our cloud readiness assessment with CART can transform your idea of moving to the cloud into a detailed plan that follows AWS Professional Services best practices. AWS Application Discovery Service Collects server specification information, performance data, and details of running processes and network connections AWS Organization The deny list SCP at the root level will not allow the restricted actions to be allowed at any level beneath so this will not work. Move SCP from root to OU to update OUs for actions like DMS","title":"Neal davis"},{"location":"neal-davis/#migraiton-tools","text":"https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-tools/aws-services.html","title":"Migraiton tools"},{"location":"neal-davis/#aws-migration-hub","text":"AWS Migration Hub provides a single place to store IT asset inventory data while tracking migrations to any AWS Region. After migration, use Migration Hub to accelerate the transformation of your applications to native AWS Centralized tracking Migration flexibility Discovery, assessment, and planning Fast-track application refactor","title":"AWS Migration Hub"},{"location":"neal-davis/#aws-cloud-adoption-readiness-tool-cart","text":"https://cloudreadiness.amazonaws.com/#/cart Completing our cloud readiness assessment with CART can transform your idea of moving to the cloud into a detailed plan that follows AWS Professional Services best practices.","title":"AWS cloud Adoption Readiness Tool (CART)"},{"location":"neal-davis/#aws-application-discovery-service","text":"Collects server specification information, performance data, and details of running processes and network connections","title":"AWS Application Discovery Service"},{"location":"neal-davis/#aws-organization","text":"The deny list SCP at the root level will not allow the restricted actions to be allowed at any level beneath so this will not work. Move SCP from root to OU to update OUs for actions like DMS","title":"AWS Organization"},{"location":"tutorials-dojo/","text":"Diagnostic Quiz AWS Transfer Family is a secure transfer service that enables you to transfer files into and out of AWS storage services. AWS Transfer Family supports transferring data from or to the following AWS storage services \u2013 Amazon Simple Storage Service (Amazon S3) storage and Amazon Elastic File System (Amazon EFS) Network File System (NFS) file system. S3 Transfer Acceleration -> is mainly suited for improving upload speeds to the Amazon S3 bucket A Direct Connect gateway is a globally available resource to enable connections to multiple Amazon VPCs across different regions or AWS accounts","title":"Tutorials dojo"},{"location":"tutorials-dojo/#diagnostic-quiz","text":"AWS Transfer Family is a secure transfer service that enables you to transfer files into and out of AWS storage services. AWS Transfer Family supports transferring data from or to the following AWS storage services \u2013 Amazon Simple Storage Service (Amazon S3) storage and Amazon Elastic File System (Amazon EFS) Network File System (NFS) file system. S3 Transfer Acceleration -> is mainly suited for improving upload speeds to the Amazon S3 bucket A Direct Connect gateway is a globally available resource to enable connections to multiple Amazon VPCs across different regions or AWS accounts","title":"Diagnostic Quiz"},{"location":"api-gateway/api-gateway/","text":"Overview A fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale Create and Manaage API Highly Available and Scalable Handles authorization, throttling, caching, CORS, transformations, OpenAPI spec Direct integrations with AWS services (dynamo, step-functions, SNS, lambda) Public services - both on-premise and AWS Supports creating HTTP APIs, REST APIs and Websocket APIs Authentication Endpoint Types Edge Optimized routed to the nearest cloudfront POP (point of presence) Regional clients in the same region Private endpoint accessible only withing a VPC via interface endpoint Stages Error Codes 4XX - Client Error - Invalid request on client side 5XX - Server Error - Valid request, backend issue API Gateway Timeout L imit - 29s HTTP Status Code Error code Retry 400 Bad Request Exception (generic) No 403 Access Denied Exception No 404 Not Found Exception No 409 Conflict Exception No 429 Limit Exceeded Exception No 429 Too Many Requests Exception Yes 502 Bad Gateway Exception, usually for an incompatible output returned from a Lambda proxy integration backend and occasionally for out-of-order invocations due to heavy loads. Yes if idempotent 503 Service Unavailable Exception Yes 504 Endpoint Request Timed-out Exception Yes if idempotent Caching Caching is done per stage. Methods and Resources Integrations API methods are integrated with a backend endpoint MOCK - test without backend HTTP - Backend http endpoint (configure both integration request and response) HTTP Proxy - pass through to integration unmodified and return to the client unmodified (backedn need to use supported format) AWS - allows an API expose AWS service actions AWS_PROXY (lambda) - low admin overhead lambda endpoint heavy lifting done by lambda instead of gateway Mapping Templates used for AWS and HTTP (non proxy) integrations modify or rename parameters modify the body or heaers of the request filtering - removing anything which is not needed uses Velocity Templae Language (VTL) Example - using mapping template to transform REST (gateway) to SOAP (backend) Stages and Deployments Changes made in API gateway are not LIVE The current API state needs to be deployed to a stage Stages can be environments (PROD, DEV, TEST) or versions (v1, v2, v3) for breaking changes Each stage has its own configuration versions are immutable in gateway and can be overwritten using configuration Swagger and OpenAPI OpenAPI (OAS) formally known as swagger Swagger = OpenAPI V2 OpenAPI V3 is a more recent version API description format for REST API's Endpoints ( /listcats ) and Operations ( GET /listcats ) input and output parameters & Authentication methods non tech information - contact info, licencse, terms of use... Import existing OpenAPI spec + update permissions on resource policy for lambda","title":"API Gateway"},{"location":"api-gateway/api-gateway/#overview","text":"A fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale Create and Manaage API Highly Available and Scalable Handles authorization, throttling, caching, CORS, transformations, OpenAPI spec Direct integrations with AWS services (dynamo, step-functions, SNS, lambda) Public services - both on-premise and AWS Supports creating HTTP APIs, REST APIs and Websocket APIs","title":"Overview"},{"location":"api-gateway/api-gateway/#authentication","text":"","title":"Authentication"},{"location":"api-gateway/api-gateway/#endpoint-types","text":"Edge Optimized routed to the nearest cloudfront POP (point of presence) Regional clients in the same region Private endpoint accessible only withing a VPC via interface endpoint","title":"Endpoint Types"},{"location":"api-gateway/api-gateway/#stages","text":"","title":"Stages"},{"location":"api-gateway/api-gateway/#error-codes","text":"4XX - Client Error - Invalid request on client side 5XX - Server Error - Valid request, backend issue API Gateway Timeout L imit - 29s HTTP Status Code Error code Retry 400 Bad Request Exception (generic) No 403 Access Denied Exception No 404 Not Found Exception No 409 Conflict Exception No 429 Limit Exceeded Exception No 429 Too Many Requests Exception Yes 502 Bad Gateway Exception, usually for an incompatible output returned from a Lambda proxy integration backend and occasionally for out-of-order invocations due to heavy loads. Yes if idempotent 503 Service Unavailable Exception Yes 504 Endpoint Request Timed-out Exception Yes if idempotent","title":"Error Codes"},{"location":"api-gateway/api-gateway/#caching","text":"Caching is done per stage.","title":"Caching"},{"location":"api-gateway/api-gateway/#methods-and-resources","text":"","title":"Methods and Resources"},{"location":"api-gateway/api-gateway/#integrations","text":"API methods are integrated with a backend endpoint MOCK - test without backend HTTP - Backend http endpoint (configure both integration request and response) HTTP Proxy - pass through to integration unmodified and return to the client unmodified (backedn need to use supported format) AWS - allows an API expose AWS service actions AWS_PROXY (lambda) - low admin overhead lambda endpoint heavy lifting done by lambda instead of gateway Mapping Templates used for AWS and HTTP (non proxy) integrations modify or rename parameters modify the body or heaers of the request filtering - removing anything which is not needed uses Velocity Templae Language (VTL) Example - using mapping template to transform REST (gateway) to SOAP (backend)","title":"Integrations"},{"location":"api-gateway/api-gateway/#stages-and-deployments","text":"Changes made in API gateway are not LIVE The current API state needs to be deployed to a stage Stages can be environments (PROD, DEV, TEST) or versions (v1, v2, v3) for breaking changes Each stage has its own configuration versions are immutable in gateway and can be overwritten using configuration","title":"Stages and Deployments"},{"location":"api-gateway/api-gateway/#swagger-and-openapi","text":"OpenAPI (OAS) formally known as swagger Swagger = OpenAPI V2 OpenAPI V3 is a more recent version API description format for REST API's Endpoints ( /listcats ) and Operations ( GET /listcats ) input and output parameters & Authentication methods non tech information - contact info, licencse, terms of use... Import existing OpenAPI spec + update permissions on resource policy for lambda","title":"Swagger and OpenAPI"},{"location":"app-services/datasync/","text":"AWS DataSync Data Transfer service TO and FROM AWS Use cases Data Migration Achiving cold data Data Protection Data movement for timely in-cloud processing DR/BC - Disaster Recovery/Business continuity ..designed to work at huge scale ( agent ) Retains metadata (e.g. permissions/timestamps ) Built in data validation Key Features Scalable - 10GBps per agent (~100TB per day) Bandwidth Limiters (avoid link saturations) Incremental and scheduled transfer options Compression and encyrption Automatic recovery from transit errors AWS Service Integration - S3, EFS, FSx Support transfering data between one AWS service to another Pay as you use.. per GB cost for data moved Components Task - A job within DataSync, defines what is being synced, how quickly, FROM where TO where Agent - Software to READ and WRITE to on-premise data stores using NFS or SMB Location - every task has two locations FROM and TO. e.g. - NFS, SMB (Server Message Block), EFS, FSx and S3 M","title":"Datasync"},{"location":"app-services/datasync/#aws-datasync","text":"Data Transfer service TO and FROM AWS Use cases Data Migration Achiving cold data Data Protection Data movement for timely in-cloud processing DR/BC - Disaster Recovery/Business continuity ..designed to work at huge scale ( agent ) Retains metadata (e.g. permissions/timestamps ) Built in data validation","title":"AWS DataSync"},{"location":"app-services/datasync/#key-features","text":"Scalable - 10GBps per agent (~100TB per day) Bandwidth Limiters (avoid link saturations) Incremental and scheduled transfer options Compression and encyrption Automatic recovery from transit errors AWS Service Integration - S3, EFS, FSx Support transfering data between one AWS service to another Pay as you use.. per GB cost for data moved","title":"Key Features"},{"location":"app-services/datasync/#components","text":"Task - A job within DataSync, defines what is being synced, how quickly, FROM where TO where Agent - Software to READ and WRITE to on-premise data stores using NFS or SMB Location - every task has two locations FROM and TO. e.g. - NFS, SMB (Server Message Block), EFS, FSx and S3 M","title":"Components"},{"location":"app-services/greengrass/","text":"AWS Greengrass Extends some AWS services to the edge Some compute, messaging, data management, sync and ML capabilities Locally run lambdas Locally run containers IOT Device Shadows locally - synced back to AWS Local messaging - MQTT Local hardware access for Lambda AWS IoT Greengrass makes it possible for customers to build IoT devices and application logic. Specifically, AWS IoT Greengrass provides cloud-based management of application logic that runs on devices. Locally deployed Lambda functions and connectors are triggered by local events, messages from the cloud, or other sources . In AWS IoT Greengrass, devices securely communicate on a local network and exchange messages with each other without having to connect to the cloud . AWS IoT Greengrass provides a local pub/sub message manager that can intelligently buffer messages if connectivity is lost so that inbound and outbound messages to the cloud are preserved.","title":"Greengrass"},{"location":"app-services/greengrass/#aws-greengrass","text":"Extends some AWS services to the edge Some compute, messaging, data management, sync and ML capabilities Locally run lambdas Locally run containers IOT Device Shadows locally - synced back to AWS Local messaging - MQTT Local hardware access for Lambda AWS IoT Greengrass makes it possible for customers to build IoT devices and application logic. Specifically, AWS IoT Greengrass provides cloud-based management of application logic that runs on devices. Locally deployed Lambda functions and connectors are triggered by local events, messages from the cloud, or other sources . In AWS IoT Greengrass, devices securely communicate on a local network and exchange messages with each other without having to connect to the cloud . AWS IoT Greengrass provides a local pub/sub message manager that can intelligently buffer messages if connectivity is lost so that inbound and outbound messages to the cloud are preserved.","title":"AWS Greengrass"},{"location":"app-services/iot/","text":"IOT (internet of things) AWS IOT core is a suite of products Used for managing 1,000,000s of IOT devices temp, wind, water sensors, lights, valve control.. Provisioning, updates and controls of IOT devices Unreliable links -- device shadows Rules and event-driven integration with AWS services","title":"Iot"},{"location":"app-services/iot/#iot-internet-of-things","text":"AWS IOT core is a suite of products Used for managing 1,000,000s of IOT devices temp, wind, water sensors, lights, valve control.. Provisioning, updates and controls of IOT devices Unreliable links -- device shadows Rules and event-driven integration with AWS services","title":"IOT (internet of things)"},{"location":"app-services/mechanical-turk/","text":"Mechanical Turk (MTurk) Amazon Mechanical Turk (MTurk) is a crowdsourcing marketplace that makes it easier for individuals and businesses to outsource their processes and jobs to a distributed workforce who can perform these tasks virtually. Managed human task outsourcing - API's - extend your app with humans Requester post Human Intelligence Tasks (HITs) Workers earn by completing HITs Pay per Task, Perfect for tasks suited for humans rather than ML Qualifications - worker attribute, can require a test and can be requirement to complete HITs Data collection, manual processing, image classification can create data to use for ML training List of 1000 vets and cupcakes store - find telephone, opneing hours Use Cases MTurk can be a great way to minimize the costs and time required for each stage of ML development. It is easy to collect and annotate the massive amounts of data required for training machine learning (ML) models with MTurk. Common examples include the moderation of web and social media content, categorization of products or images, and the collection of data from websites or other resources.","title":"Mechanical turk"},{"location":"app-services/mechanical-turk/#mechanical-turk-mturk","text":"Amazon Mechanical Turk (MTurk) is a crowdsourcing marketplace that makes it easier for individuals and businesses to outsource their processes and jobs to a distributed workforce who can perform these tasks virtually. Managed human task outsourcing - API's - extend your app with humans Requester post Human Intelligence Tasks (HITs) Workers earn by completing HITs Pay per Task, Perfect for tasks suited for humans rather than ML Qualifications - worker attribute, can require a test and can be requirement to complete HITs Data collection, manual processing, image classification can create data to use for ML training List of 1000 vets and cupcakes store - find telephone, opneing hours","title":"Mechanical Turk (MTurk)"},{"location":"app-services/mechanical-turk/#use-cases","text":"MTurk can be a great way to minimize the costs and time required for each stage of ML development. It is easy to collect and annotate the massive amounts of data required for training machine learning (ML) models with MTurk. Common examples include the moderation of web and social media content, categorization of products or images, and the collection of data from websites or other resources.","title":"Use Cases"},{"location":"app-services/media-convert/","text":"Elemental MediaConvert (MC) & Elastic Transcoder (ET) File-based video transcoding services MC ~= v2 of ET (almost) Serverless - pay for resources used Add jobs to pipelines (ET) or Queues (MC) File loaded from S3, processed, stored on S3 MC supports EventBridge (CWEvents) for job signaling When and what to use ?? ET is legacy, default to MC MC supports more codecs, and is designed for larger volume and paralled processing MC supports reserved pricing .. ET for WebM (VP8/VP9) ET for animated GIF ET for MP3, FLAC, Vorbis and WAV Everything else .. MC","title":"Media convert"},{"location":"app-services/media-convert/#elemental-mediaconvert-mc-elastic-transcoder-et","text":"File-based video transcoding services MC ~= v2 of ET (almost) Serverless - pay for resources used Add jobs to pipelines (ET) or Queues (MC) File loaded from S3, processed, stored on S3 MC supports EventBridge (CWEvents) for job signaling","title":"Elemental MediaConvert (MC) &amp; Elastic Transcoder (ET)"},{"location":"app-services/media-convert/#when-and-what-to-use","text":"ET is legacy, default to MC MC supports more codecs, and is designed for larger volume and paralled processing MC supports reserved pricing .. ET for WebM (VP8/VP9) ET for animated GIF ET for MP3, FLAC, Vorbis and WAV Everything else .. MC","title":"When and what to use ??"},{"location":"app-services/simple-workflow-service/","text":"Simple Workflow Service - SWF Overview Build workflows - cordination over distributed components Predecessor to Step Functions - uses Instances/Servers Same patterns/anti-patterns - long running workflows Activity Task and Activity Worker Decider schedule task provides inputs to activity worker process events while workflow is in progress ends the workflow when objective has been completed 1 years of maximum runtime SWF vs step-functions Default - Step Functions - serverless/lower admin AWS Flow Framework --> SWF External Signals to intervene in process -> SWF Launch child flows - return to parent -> SWF Bespoke/complex decisions -> SWF (customer decider applications) Mechanical Turk -> SWF (suggested AWS architecture) Over time Step Functions will replace SWF","title":"Simple workflow service"},{"location":"app-services/simple-workflow-service/#simple-workflow-service-swf","text":"","title":"Simple Workflow Service - SWF"},{"location":"app-services/simple-workflow-service/#overview","text":"Build workflows - cordination over distributed components Predecessor to Step Functions - uses Instances/Servers Same patterns/anti-patterns - long running workflows Activity Task and Activity Worker Decider schedule task provides inputs to activity worker process events while workflow is in progress ends the workflow when objective has been completed 1 years of maximum runtime","title":"Overview"},{"location":"app-services/simple-workflow-service/#swf-vs-step-functions","text":"Default - Step Functions - serverless/lower admin AWS Flow Framework --> SWF External Signals to intervene in process -> SWF Launch child flows - return to parent -> SWF Bespoke/complex decisions -> SWF (customer decider applications) Mechanical Turk -> SWF (suggested AWS architecture) Over time Step Functions will replace SWF","title":"SWF vs step-functions"},{"location":"cloudformation/cloudformation/","text":"AWS cloudformation IaaS logical resources => physical resources Stacks non-portable templates hardcoded s3 bucket name or AMI for EC2 instances template parameters - accept inputs from CLI/API/Console when a stack is created or updated psuedo paramters AWS make available parameters can be used even without specifying in template parameters - e.g- AWS::Region, AWS::AccountId, AWS::StackName, AWS::StackId intrinsic functions aws refrence template mappings key value mappings one key or top & second level !FindInMap instrinsic function improves template portability conditions evaluated to either TRUE or FALSE depends on in-built dependency order/tree (VPC => subnet => EC2) determined by CloudFormation explicit defined control dependency order wait conditions cloudformation signal configure cloudformation to hold wait for 'X' no. of success signals wait for timeout (12 hr max) If success signals received .. CREATE_COMPLETE if failure signal received.. creation fails if timeout is reached .. creation fails CreationPolicy or WaitCondition CreationPolicy nested stacks overcome the 500 Resource Limit of one stack modular templates ... code reuse make the installation process easier ..nested stack created by the root stack use only when everything is lifecycle linked cross-stack references outputs are normally not visible from other stacks nested references can reference them.. outputs are exported .. making them visible from other stacks Exports must have a unique name in the region Fn::ImportValue can be used instead of Ref stack sets deploy CFN stacks across many accounts & regions StackSets are containers in an admin account ...contain stack instances .. which reference stacks Stack instances & stacks are in target accounts Each stack = 1 region in 1 account Security = self-managed or sevice-managed keys and concepts concurrent accounts - more conncurent account=>faster the deployment failure tolerance - no. of failed individual stacks to determine the whole set to be failed retain stacks - to retain stack in aws account when the set it deleted scenarios Enable AWS Config AWS Config Rules - MFA, EIPS, EBS Encryption Create IAM Roles for cross-account access deletion policy DeletionPolicy By default, resources associated with CFT is deleted when the templete is deleted = which can cause data loss Delete (Default), Retain or (if supported) Snapshot Snapshot supported by EBS volume, ElastiCache, Neptune, RDS, Redshift Snapshots continue on pst Stack lifetime - you have to clean up ($$) ONLY APPLIES TO DELETE .. NOT REPLACE when resource is deleted from the template and cft is redeployed when the whole template is deleted stack roles CFN uses the permissions of the logged in identity requires permissions for AWS CFN can assume a role to gain the permission Lets you implement role seperation The identity creating the stack only need PassRole and doesnot need resource permissions CloudFormationInit & cnf-init Simple configuration management system Configuration directives stored in template AWS::CloudFormation::Init part of logical resource Procedural - HOW (User Data) ... vs WHAT (cfn-init) - idompotent cnf-init helper script - installed on EC2 OS (make it so) cnf-hup cfn-hup helper is a daemon which can be installed .. it detects changes in resource metadata .. running configurable actions when a change is detected . UpdateStack => update config on EC2 instances ChangeSets Change Sets let you preview changes ( A Change Set ) ..multiple different versions for a stakc (lots of change sets) Chosen changes can be applied by executing the change set CustomResources logical resources in a tempalte create, update and delete physical resources","title":"Cloudformation"},{"location":"cloudformation/cloudformation/#aws-cloudformation","text":"IaaS logical resources => physical resources Stacks non-portable templates hardcoded s3 bucket name or AMI for EC2 instances template parameters - accept inputs from CLI/API/Console when a stack is created or updated psuedo paramters AWS make available parameters can be used even without specifying in template parameters - e.g- AWS::Region, AWS::AccountId, AWS::StackName, AWS::StackId intrinsic functions aws refrence template mappings key value mappings one key or top & second level !FindInMap instrinsic function improves template portability conditions evaluated to either TRUE or FALSE depends on in-built dependency order/tree (VPC => subnet => EC2) determined by CloudFormation explicit defined control dependency order wait conditions cloudformation signal configure cloudformation to hold wait for 'X' no. of success signals wait for timeout (12 hr max) If success signals received .. CREATE_COMPLETE if failure signal received.. creation fails if timeout is reached .. creation fails CreationPolicy or WaitCondition CreationPolicy nested stacks overcome the 500 Resource Limit of one stack modular templates ... code reuse make the installation process easier ..nested stack created by the root stack use only when everything is lifecycle linked cross-stack references outputs are normally not visible from other stacks nested references can reference them.. outputs are exported .. making them visible from other stacks Exports must have a unique name in the region Fn::ImportValue can be used instead of Ref stack sets deploy CFN stacks across many accounts & regions StackSets are containers in an admin account ...contain stack instances .. which reference stacks Stack instances & stacks are in target accounts Each stack = 1 region in 1 account Security = self-managed or sevice-managed keys and concepts concurrent accounts - more conncurent account=>faster the deployment failure tolerance - no. of failed individual stacks to determine the whole set to be failed retain stacks - to retain stack in aws account when the set it deleted scenarios Enable AWS Config AWS Config Rules - MFA, EIPS, EBS Encryption Create IAM Roles for cross-account access deletion policy DeletionPolicy By default, resources associated with CFT is deleted when the templete is deleted = which can cause data loss Delete (Default), Retain or (if supported) Snapshot Snapshot supported by EBS volume, ElastiCache, Neptune, RDS, Redshift Snapshots continue on pst Stack lifetime - you have to clean up ($$) ONLY APPLIES TO DELETE .. NOT REPLACE when resource is deleted from the template and cft is redeployed when the whole template is deleted stack roles CFN uses the permissions of the logged in identity requires permissions for AWS CFN can assume a role to gain the permission Lets you implement role seperation The identity creating the stack only need PassRole and doesnot need resource permissions CloudFormationInit & cnf-init Simple configuration management system Configuration directives stored in template AWS::CloudFormation::Init part of logical resource Procedural - HOW (User Data) ... vs WHAT (cfn-init) - idompotent cnf-init helper script - installed on EC2 OS (make it so) cnf-hup cfn-hup helper is a daemon which can be installed .. it detects changes in resource metadata .. running configurable actions when a change is detected . UpdateStack => update config on EC2 instances ChangeSets Change Sets let you preview changes ( A Change Set ) ..multiple different versions for a stakc (lots of change sets) Chosen changes can be applied by executing the change set CustomResources logical resources in a tempalte create, update and delete physical resources","title":"AWS cloudformation"},{"location":"cloudfront/cloudfront/","text":"Overview CloudFront is a Content Delivery network (CDN) within AWS. - higher latencies and slow transfer speeds (customers across the globe) - solve by using caching and efficient global network Terms Origin - S3 origin or custom origin (publicly routable IP address) Distribution - configuration unit of CloudFront can have multiple origins configured Edge Location - Local cache of your data smaller than regions - 200 edge locations Regional Edge Cache only used by custom origin NOT S3 origin larger version of edge location. Provides another provides another layer of caching Cache Behavior Default ( ) - path-pattern * trusted signers/groups for private To delete all cache behaviors in an existing distribution, update the distribution configuration and include only an empty CacheBehaviors element. https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CacheBehavior.html TTL more frquent cache HITS == lower orign load Default TTL (defined in the behavior) == 24 hrs (validity period) Minimum TTL and Maximum TTL - default applies if not specified limits the object level TTL values Origin Header: Cache-Control max-age (seconds) Origin Header: Cache-Control s-maxage (seconds) Origin Header: Expires (Date & Time) Headers can be set using custom origin or s3 origin (via object metadata) Invalidations (cache) Cache invalidation - performed on a distribution applies to all edge locations - process takes time Examples of cache invalidations - correct errors /images/whiskers1.png /images/whiskers* /images/* /* Versioned file names .. file_v1/file_v2/file_v3 avaoid caching issues - names are different logging is more clear no frequent cached invalidations SSL CloudFront Default Domain Name (CNAME) ex - https://random.cloudfront.net/ SSL supported by default ... *.clodufront.net cert Alternate Domain Names (CNAMES) ex- cdn.something.com verify ownership (optionally HTTPS) using a matching certificate generate or import certs in ACM add certficate in us-east-1 for global service like CloudFront Cloudfront to allow on one of the options: HTTP or HTTPS, HTTP => HTTPS, HTTPS Only Two SSL connections Viewer => CloudFront & CloudFront => Origin both need valid public certificates (intermediate certs) SNI - Server Name Indication SNI is a TLS extension, allowing a host to be included -- free of cost host header routing ->> allows multiple hosts/certs behind a single IP old browsers don't support SNI .. CF charges extra for dedicated IP ($600) Origin Types an Architecture Origins aws s3 aws media package channel endpoints aws media storage container endpoints everything else (custom origin) -- web servers Custom origin options: SSL protocol - TLS certs origin id, domain name, path origin protocol custom ports custom headers connection configuration Caching Peformance & Optimization CACHE HIT = object returned from cache CACHE MISS = object returned from origin Headers, Cookies and Query String Parameters can be configured to be forwarded to the origin or not Forward what the application needs Cache based on what can change the objects The more things are involved in caching - the less efficient Origin Access Identity (OAI) - S3 Origin Onlyapplicable to S3 type of identity assocaited with Cloudfont Distribution CloudFront 'becomes' that OAI OAI can be used in the s3 bucket policies DENY all but ALLOW one or more OAI's in bucket policy Securing Custom origins custom origin to require some custom headers custom headers are injected within the cloudfront custom origin to host behind firewall which allows only public IP ranges of cloudfront Private Distributions Public - open access to objects Private ... requests required signed Cookie or URL 1 behavior --> whole distribution PUBLIC or PRIVATE Multiple behaviors - combination of PUBLIC and PRIVATE CloudFront Key created by root account user and tied to a specific AWS account account with cloudfront key can be added as TRUSTED SIGNER will require signed cookie/url Signed URL provides access to one object Legcy RTMP distributions cant use cookies use URL's if the client does not support cookies Signed Cookies Use for groups of files/ all files of a type - .jpeg maintain the URL with signed cookies CloudFront Geo Restriction CF Geo REstriction 3rd party Geolocation Whitelist or Blacklist - COUNTRY ONLY completely customizable GeoIP Database 99.8%+ Accurate more accurate - can be filter on many attributesc applies to the entire distribution CloudFront Field-Level Encryption Client <==> Origin can be encrypted using HTTPS data transmitted within is treated like any other plain data Field Level encryption happens at the edge happens separately from the HTTPS Tunnel using public/private key private key is needed to decrypt individual fields Lambda@Edge run lightweight lambda at edge locations adjust data between the viewer and origin only supports node.js and python runs in the AWS public space --> cannot access any VPC resource lambda layers not supported different limits than normal lambda function Use Cases A/B testing - Viewer Request Migration between s3 oriigns - Origin Request Different Objects based on Device - Origin Request Content By Country - Origin Request ElasticCache in-memory database .. high performance managed redis and memcached -- as a service can be used to cache data - for READ HEAVY workloads with low latency requirements Reduces database workloads (expensive) Can be used to store Session Data (Stateless Servers) requires application code changes Memcached Redis simple data structure - string advance structure- list, sets, sorted lists, bit-arrays No replication Multi-AZ Mulitple Nodes(Sharding) Replication(Scale Reads) No backups Backup & Restore Multi-threaded Transactions","title":"CloudFront"},{"location":"cloudfront/cloudfront/#overview","text":"CloudFront is a Content Delivery network (CDN) within AWS. - higher latencies and slow transfer speeds (customers across the globe) - solve by using caching and efficient global network","title":"Overview"},{"location":"cloudfront/cloudfront/#terms","text":"Origin - S3 origin or custom origin (publicly routable IP address) Distribution - configuration unit of CloudFront can have multiple origins configured Edge Location - Local cache of your data smaller than regions - 200 edge locations Regional Edge Cache only used by custom origin NOT S3 origin larger version of edge location. Provides another provides another layer of caching","title":"Terms"},{"location":"cloudfront/cloudfront/#cache-behavior","text":"Default ( ) - path-pattern * trusted signers/groups for private To delete all cache behaviors in an existing distribution, update the distribution configuration and include only an empty CacheBehaviors element. https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CacheBehavior.html","title":"Cache Behavior"},{"location":"cloudfront/cloudfront/#ttl","text":"more frquent cache HITS == lower orign load Default TTL (defined in the behavior) == 24 hrs (validity period) Minimum TTL and Maximum TTL - default applies if not specified limits the object level TTL values Origin Header: Cache-Control max-age (seconds) Origin Header: Cache-Control s-maxage (seconds) Origin Header: Expires (Date & Time) Headers can be set using custom origin or s3 origin (via object metadata)","title":"TTL"},{"location":"cloudfront/cloudfront/#invalidations-cache","text":"Cache invalidation - performed on a distribution applies to all edge locations - process takes time Examples of cache invalidations - correct errors /images/whiskers1.png /images/whiskers* /images/* /* Versioned file names .. file_v1/file_v2/file_v3 avaoid caching issues - names are different logging is more clear no frequent cached invalidations","title":"Invalidations (cache)"},{"location":"cloudfront/cloudfront/#ssl","text":"CloudFront Default Domain Name (CNAME) ex - https://random.cloudfront.net/ SSL supported by default ... *.clodufront.net cert Alternate Domain Names (CNAMES) ex- cdn.something.com verify ownership (optionally HTTPS) using a matching certificate generate or import certs in ACM add certficate in us-east-1 for global service like CloudFront Cloudfront to allow on one of the options: HTTP or HTTPS, HTTP => HTTPS, HTTPS Only Two SSL connections Viewer => CloudFront & CloudFront => Origin both need valid public certificates (intermediate certs)","title":"SSL"},{"location":"cloudfront/cloudfront/#sni-server-name-indication","text":"SNI is a TLS extension, allowing a host to be included -- free of cost host header routing ->> allows multiple hosts/certs behind a single IP old browsers don't support SNI .. CF charges extra for dedicated IP ($600)","title":"SNI - Server Name Indication"},{"location":"cloudfront/cloudfront/#origin-types-an-architecture","text":"Origins aws s3 aws media package channel endpoints aws media storage container endpoints everything else (custom origin) -- web servers Custom origin options: SSL protocol - TLS certs origin id, domain name, path origin protocol custom ports custom headers connection configuration","title":"Origin Types an Architecture"},{"location":"cloudfront/cloudfront/#caching-peformance-optimization","text":"CACHE HIT = object returned from cache CACHE MISS = object returned from origin Headers, Cookies and Query String Parameters can be configured to be forwarded to the origin or not Forward what the application needs Cache based on what can change the objects The more things are involved in caching - the less efficient","title":"Caching Peformance &amp; Optimization"},{"location":"cloudfront/cloudfront/#origin-access-identity-oai-s3-origin","text":"Onlyapplicable to S3 type of identity assocaited with Cloudfont Distribution CloudFront 'becomes' that OAI OAI can be used in the s3 bucket policies DENY all but ALLOW one or more OAI's in bucket policy","title":"Origin Access Identity (OAI) - S3 Origin"},{"location":"cloudfront/cloudfront/#securing-custom-origins","text":"custom origin to require some custom headers custom headers are injected within the cloudfront custom origin to host behind firewall which allows only public IP ranges of cloudfront","title":"Securing Custom origins"},{"location":"cloudfront/cloudfront/#private-distributions","text":"Public - open access to objects Private ... requests required signed Cookie or URL 1 behavior --> whole distribution PUBLIC or PRIVATE Multiple behaviors - combination of PUBLIC and PRIVATE CloudFront Key created by root account user and tied to a specific AWS account account with cloudfront key can be added as TRUSTED SIGNER will require signed cookie/url Signed URL provides access to one object Legcy RTMP distributions cant use cookies use URL's if the client does not support cookies Signed Cookies Use for groups of files/ all files of a type - .jpeg maintain the URL with signed cookies","title":"Private Distributions"},{"location":"cloudfront/cloudfront/#cloudfront-geo-restriction","text":"CF Geo REstriction 3rd party Geolocation Whitelist or Blacklist - COUNTRY ONLY completely customizable GeoIP Database 99.8%+ Accurate more accurate - can be filter on many attributesc applies to the entire distribution","title":"CloudFront Geo Restriction"},{"location":"cloudfront/cloudfront/#cloudfront-field-level-encryption","text":"Client <==> Origin can be encrypted using HTTPS data transmitted within is treated like any other plain data Field Level encryption happens at the edge happens separately from the HTTPS Tunnel using public/private key private key is needed to decrypt individual fields","title":"CloudFront Field-Level Encryption"},{"location":"cloudfront/cloudfront/#lambdaedge","text":"run lightweight lambda at edge locations adjust data between the viewer and origin only supports node.js and python runs in the AWS public space --> cannot access any VPC resource lambda layers not supported different limits than normal lambda function Use Cases A/B testing - Viewer Request Migration between s3 oriigns - Origin Request Different Objects based on Device - Origin Request Content By Country - Origin Request","title":"Lambda@Edge"},{"location":"cloudfront/cloudfront/#elasticcache","text":"in-memory database .. high performance managed redis and memcached -- as a service can be used to cache data - for READ HEAVY workloads with low latency requirements Reduces database workloads (expensive) Can be used to store Session Data (Stateless Servers) requires application code changes Memcached Redis simple data structure - string advance structure- list, sets, sorted lists, bit-arrays No replication Multi-AZ Mulitple Nodes(Sharding) Replication(Scale Reads) No backups Backup & Restore Multi-threaded Transactions","title":"ElasticCache"},{"location":"compute/asg/","text":"Overview Automatic Scaling and Self-healing for EC2 Uses Launch Templates or Configurations min,desired and max => min <= desired <= max desired is maintained by provisioning and terminating the instances ASGs are free use cool downs smaller instances - granularity Scaling Policies Manual Scaling - Manually adjust the desired capacity Scheduled Scaling - Time based adjustment Dynamic Scaling - Simple Scaling - Add 1 for CPU (metric) > 50 and Remove 1 for CPU (metric) < 50 - Stepped Scaling - Bigger +/- based on difference - Target Tracking - Desired Aggregate CPU = 40% .. ASG handle it Cooldown Peiod (s) - wait at end of the scaling activity to evaluate EC2 instance check for health check ASG + Load Balancers ALB --> Tagret Group (TG) --> ASG targets in TG is automatically updated by ASG app health check con be configured by TG Scaling Process Launch and Terminate - SUSPEND and RESUME AddToLoadBalancer - add to LB on launch AlarmNotification - accept notification from CW AZRebalance - balances instances evenly accross all of the AZs HealthCheck - instance health checks on/off ReplaceUnhealthy - Terminate unhealthy and replace ScheduledActions - Scheduled on/off Standby - use this for instances InService vs Standby ASG Lifecycle Hooks Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them. Custom Actions on instances during ASG actions instance launch or instance terminate transitions Instances are paused within the flow.. they wait until lifecycle actions completed using complete-lifecycle-action command or CompleteLifecycleAction operation OR until the timeout period ends (one hour by default) - CONTINUE or ABANDON Integrates with SNS, SQS and EventBridge","title":"Asg"},{"location":"compute/asg/#overview","text":"Automatic Scaling and Self-healing for EC2 Uses Launch Templates or Configurations min,desired and max => min <= desired <= max desired is maintained by provisioning and terminating the instances ASGs are free use cool downs smaller instances - granularity","title":"Overview"},{"location":"compute/asg/#scaling-policies","text":"Manual Scaling - Manually adjust the desired capacity Scheduled Scaling - Time based adjustment Dynamic Scaling - Simple Scaling - Add 1 for CPU (metric) > 50 and Remove 1 for CPU (metric) < 50 - Stepped Scaling - Bigger +/- based on difference - Target Tracking - Desired Aggregate CPU = 40% .. ASG handle it Cooldown Peiod (s) - wait at end of the scaling activity to evaluate EC2 instance check for health check","title":"Scaling Policies"},{"location":"compute/asg/#asg-load-balancers","text":"ALB --> Tagret Group (TG) --> ASG targets in TG is automatically updated by ASG app health check con be configured by TG","title":"ASG + Load Balancers"},{"location":"compute/asg/#scaling-process","text":"Launch and Terminate - SUSPEND and RESUME AddToLoadBalancer - add to LB on launch AlarmNotification - accept notification from CW AZRebalance - balances instances evenly accross all of the AZs HealthCheck - instance health checks on/off ReplaceUnhealthy - Terminate unhealthy and replace ScheduledActions - Scheduled on/off Standby - use this for instances InService vs Standby","title":"Scaling Process"},{"location":"compute/asg/#asg-lifecycle-hooks","text":"Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them. Custom Actions on instances during ASG actions instance launch or instance terminate transitions Instances are paused within the flow.. they wait until lifecycle actions completed using complete-lifecycle-action command or CompleteLifecycleAction operation OR until the timeout period ends (one hour by default) - CONTINUE or ABANDON Integrates with SNS, SQS and EventBridge","title":"ASG Lifecycle Hooks"},{"location":"compute/connection-draining/","text":"Connection Draining Only supported in Classic Load Balancer (CLB) gracefully disconnect the connections (ASG/EC2) allows in-flight request to complete timeout value : 1s - 3600s ( default : 300s) Timeout does not apply to failed health check applies when instance taken out of ASG - manually or ASG activities InService: Instance deregistraiton currently in progress AS waiting for all connections to complete of timeout - whichever occur first Registration Delay Supported in ALB, NLB, GWLB - defined on the target group stop sending requests to deregistration targtes existing connections can continue until they complete naturally enabled by default with value 300s 0-3600s X-Forwarded-For and Proxy protocol are two alternative versions of gaining visibility of original client IP address when using proxy servers or load balancers X-Forwarded-For Set of http/https headers (layer 7) Header is appended or added by proxies/LB Http Header -> X-Forwarded-For: client,proxy1,proxy2 Client is the left most in the list Supported in CLB and ALB but NOT SUPPORTED IN NLB PROXY protocol Proxy protocol works at Layer 4 TCP header works with layer 4 also works with http/https v1 (human readable) - CLB || v2 (binary encoded) - NLB Not supported in ALB Use Case -> unbroken https encryption between client and server ( tcp listener )","title":"Connection Draining"},{"location":"compute/connection-draining/#connection-draining","text":"Only supported in Classic Load Balancer (CLB) gracefully disconnect the connections (ASG/EC2) allows in-flight request to complete timeout value : 1s - 3600s ( default : 300s) Timeout does not apply to failed health check applies when instance taken out of ASG - manually or ASG activities InService: Instance deregistraiton currently in progress AS waiting for all connections to complete of timeout - whichever occur first","title":"Connection Draining"},{"location":"compute/connection-draining/#registration-delay","text":"Supported in ALB, NLB, GWLB - defined on the target group stop sending requests to deregistration targtes existing connections can continue until they complete naturally enabled by default with value 300s 0-3600s X-Forwarded-For and Proxy protocol are two alternative versions of gaining visibility of original client IP address when using proxy servers or load balancers","title":"Registration Delay"},{"location":"compute/connection-draining/#x-forwarded-for","text":"Set of http/https headers (layer 7) Header is appended or added by proxies/LB Http Header -> X-Forwarded-For: client,proxy1,proxy2 Client is the left most in the list Supported in CLB and ALB but NOT SUPPORTED IN NLB","title":"X-Forwarded-For"},{"location":"compute/connection-draining/#proxy-protocol","text":"Proxy protocol works at Layer 4 TCP header works with layer 4 also works with http/https v1 (human readable) - CLB || v2 (binary encoded) - NLB Not supported in ALB Use Case -> unbroken https encryption between client and server ( tcp listener )","title":"PROXY protocol"},{"location":"compute/ec2/","text":"Regional and Global AWS Architecture EC2 Purchase Options (Launch Types) On-Demand Instances are isolared but multiple customer instances run on shared hardware Per-second billing while an instance is running Default purchase option No upfront cost and discount Predictable pricing Apps which cannot be interrupted Spot Cheapest to launch EC2 instances SPOT pricing is AWS selling unused EC2 host capacity for upto 90% discount the spot price is based on the spare capacity at a given time As long as the customer price is lower than the spot price, instances are provisioned else they are terminated Use for non time critical workloads which are stateless and cost sensitive. workloads which can be re-run Never use SPOT for workloads that can't tolerate interruptions Reserved Long-term consumption of EC2 instances - at a discounted price Unused Reservation still billed Scheduled Reserved instances Ideal for long tern usage which which doesn't run constantly Batch processing daily for 5 hours starting at 23:00 Weekly data, sales analysis.. every Friday for 24 hours 100 hours of EC2 per month Doesn't support all isntance types or regions 1200 hours per year & 1 year term minimum Capacity Reservations Reserved purchases -> on-demand -> spot (in priority order) Regional Reservation - provides a bililing discount for valid instances launched in any any AZ in that region but don't reserve instances within an AZ Zonal Reservation - only apply to one AZ providing billing discounts and capacity reservation in that AZ On-demand capacity reservations can be booked to ensure you always have access to capacity in an AZ when you need it full on-demand price no term limits you pay regardless of if you consume it no savings Dedicated Hosts Pay for HOST No instance charges Use case - Licensing based on Sockets/Cores where host affinity is important Host affinity links instances to hosts can launch multiple instances with multiple instance types maximum capacity to launch the no. of instances customer still have to manage host capacity Dedicated Instances Don't own the host but do not share the host - extra cost for the instances, but dedicated hardware Requirement where hosts cannot be shared. EC2 Savings Plan A hourly commitment for a 1 or 3 year term A reservation of general compute \\$ amounts ($20 per hour for 3 years) or a specific EC2 Savings Plan - flexibility on size and OS Compute products (currently) - EC2 , Fargate & Lambda Products have an on-demand rate and a savings plan rate Resource usage consumes savings plan commitment at the reduced savings plan rate Beyond the commitment, the pricing is switched to on-demand rate EC2 Networking Elastic network interfaces - ENI - Elastic Network Interface - Primary ENI is created when instance is created and cannot be removed and deleted when instance is terminated - Additional ENIs (secondary ENIs) can be added in and removed from other subnets( multi homed ) but NOT in other AZs WHY multiple ENIs ? Security groups is attached to the ENI Multiple SGs to control access to EC2 instances - different protection rules Different NACL for different subnets and secondary ENIs can be placed into different subnets from subnet where primary ENI is created Each EC2 has 1 primary private IPv4 address - same for lifecycle of the EC2 primary IP is visible to OS depending on the instance type, 1 or more no. of secondary private IPs can be allocated to the instance and those IPs are also visilble to the OS secondary IPs can be attached and detached from the instances Public IPv4 address assigned to EC2 instance is not static and not visible to the OS elastic IP per private IPv4 (to make static) - associate with primary interface ENI can have 1 or more IPv6 addresses (all publicly routable) 1 MAC address 1 or more Security Groups ENI source/destination check disable them for instance instance running services such as network address translation, routing, or firewalls Use Cases for multiple ENIs Management or isolated nteworks Software Licensing (MAC) Security or Network Appliances Multi-homes instances with workloads/roles on specific subnets Low Budget & Simple High-Availability solutions Bootstrapping vs AMI Bootstrapping or AMI Baking are two ways to influence the speed of EC2 deployments - and how flexible those deployments can be. Software installation can be extracted from bootstrapping and move to AMI app, tools and languages dependencies baked into AMI quick and ready to deploy - launch many instances from the AMI Bootstrap running executable (binary and containers) Combination of AMI baking and bootstrap for maximum efficiency EC2 Placement Group HPC = High Performance Computing Cluster Placement Groups Pack instances close together Best Practise - launch all the instances at the same time (use same instances type) Performance focused - launched in single AZ - same rack and sometimes same host All members have direct connetions to each other low latency and node-to-node communications - typical of HPC applications use instances with high performance as well as use enhanced networking Tradeoff - No resiliency since all the instances are placed in the same AZ AZ is locked when launching first instance Can span VPC peers - but impacts performance Requires supported instance type Spread Placement Groups Strictly places a small group of instances across distinct underlying hardware(distinct rack) accross multiple AZs HARD LIMIT - 7 Instances per AZ Provides infrastructure isolation each racnk has its own network and power source Not supported for Dedicated Instances or Hosts Use Case - small no. of critical instances that need to be kept sepeated from each other Partition - groups of instances spread apart Maximum of 7 partitions per AZ each partition has it own rack - no sharing between partitions Instances can be placed in a specific partition OR auto placed Large distributed and replciated workloads such as Hadoop, Cassandra and Kafka Contain the impact of failure to specific part of an application","title":"EC2"},{"location":"compute/ec2/#regional-and-global-aws-architecture","text":"","title":"Regional and Global AWS Architecture"},{"location":"compute/ec2/#ec2-purchase-options-launch-types","text":"","title":"EC2 Purchase Options (Launch Types)"},{"location":"compute/ec2/#on-demand","text":"Instances are isolared but multiple customer instances run on shared hardware Per-second billing while an instance is running Default purchase option No upfront cost and discount Predictable pricing Apps which cannot be interrupted","title":"On-Demand"},{"location":"compute/ec2/#spot","text":"Cheapest to launch EC2 instances SPOT pricing is AWS selling unused EC2 host capacity for upto 90% discount the spot price is based on the spare capacity at a given time As long as the customer price is lower than the spot price, instances are provisioned else they are terminated Use for non time critical workloads which are stateless and cost sensitive. workloads which can be re-run Never use SPOT for workloads that can't tolerate interruptions","title":"Spot"},{"location":"compute/ec2/#reserved","text":"Long-term consumption of EC2 instances - at a discounted price Unused Reservation still billed Scheduled Reserved instances Ideal for long tern usage which which doesn't run constantly Batch processing daily for 5 hours starting at 23:00 Weekly data, sales analysis.. every Friday for 24 hours 100 hours of EC2 per month Doesn't support all isntance types or regions 1200 hours per year & 1 year term minimum Capacity Reservations Reserved purchases -> on-demand -> spot (in priority order) Regional Reservation - provides a bililing discount for valid instances launched in any any AZ in that region but don't reserve instances within an AZ Zonal Reservation - only apply to one AZ providing billing discounts and capacity reservation in that AZ On-demand capacity reservations can be booked to ensure you always have access to capacity in an AZ when you need it full on-demand price no term limits you pay regardless of if you consume it no savings","title":"Reserved"},{"location":"compute/ec2/#dedicated-hosts","text":"Pay for HOST No instance charges Use case - Licensing based on Sockets/Cores where host affinity is important Host affinity links instances to hosts can launch multiple instances with multiple instance types maximum capacity to launch the no. of instances customer still have to manage host capacity","title":"Dedicated Hosts"},{"location":"compute/ec2/#dedicated-instances","text":"Don't own the host but do not share the host - extra cost for the instances, but dedicated hardware Requirement where hosts cannot be shared.","title":"Dedicated Instances"},{"location":"compute/ec2/#ec2-savings-plan","text":"A hourly commitment for a 1 or 3 year term A reservation of general compute \\$ amounts ($20 per hour for 3 years) or a specific EC2 Savings Plan - flexibility on size and OS Compute products (currently) - EC2 , Fargate & Lambda Products have an on-demand rate and a savings plan rate Resource usage consumes savings plan commitment at the reduced savings plan rate Beyond the commitment, the pricing is switched to on-demand rate","title":"EC2 Savings Plan"},{"location":"compute/ec2/#ec2-networking","text":"Elastic network interfaces - ENI - Elastic Network Interface - Primary ENI is created when instance is created and cannot be removed and deleted when instance is terminated - Additional ENIs (secondary ENIs) can be added in and removed from other subnets( multi homed ) but NOT in other AZs WHY multiple ENIs ? Security groups is attached to the ENI Multiple SGs to control access to EC2 instances - different protection rules Different NACL for different subnets and secondary ENIs can be placed into different subnets from subnet where primary ENI is created Each EC2 has 1 primary private IPv4 address - same for lifecycle of the EC2 primary IP is visible to OS depending on the instance type, 1 or more no. of secondary private IPs can be allocated to the instance and those IPs are also visilble to the OS secondary IPs can be attached and detached from the instances Public IPv4 address assigned to EC2 instance is not static and not visible to the OS elastic IP per private IPv4 (to make static) - associate with primary interface ENI can have 1 or more IPv6 addresses (all publicly routable) 1 MAC address 1 or more Security Groups ENI source/destination check disable them for instance instance running services such as network address translation, routing, or firewalls Use Cases for multiple ENIs Management or isolated nteworks Software Licensing (MAC) Security or Network Appliances Multi-homes instances with workloads/roles on specific subnets Low Budget & Simple High-Availability solutions","title":"EC2 Networking"},{"location":"compute/ec2/#bootstrapping-vs-ami","text":"Bootstrapping or AMI Baking are two ways to influence the speed of EC2 deployments - and how flexible those deployments can be. Software installation can be extracted from bootstrapping and move to AMI app, tools and languages dependencies baked into AMI quick and ready to deploy - launch many instances from the AMI Bootstrap running executable (binary and containers) Combination of AMI baking and bootstrap for maximum efficiency","title":"Bootstrapping vs AMI"},{"location":"compute/ec2/#ec2-placement-group","text":"HPC = High Performance Computing","title":"EC2 Placement Group"},{"location":"compute/ec2/#cluster-placement-groups","text":"Pack instances close together Best Practise - launch all the instances at the same time (use same instances type) Performance focused - launched in single AZ - same rack and sometimes same host All members have direct connetions to each other low latency and node-to-node communications - typical of HPC applications use instances with high performance as well as use enhanced networking Tradeoff - No resiliency since all the instances are placed in the same AZ AZ is locked when launching first instance Can span VPC peers - but impacts performance Requires supported instance type","title":"Cluster Placement Groups"},{"location":"compute/ec2/#spread-placement-groups","text":"Strictly places a small group of instances across distinct underlying hardware(distinct rack) accross multiple AZs HARD LIMIT - 7 Instances per AZ Provides infrastructure isolation each racnk has its own network and power source Not supported for Dedicated Instances or Hosts Use Case - small no. of critical instances that need to be kept sepeated from each other","title":"Spread Placement Groups"},{"location":"compute/ec2/#partition-groups-of-instances-spread-apart","text":"Maximum of 7 partitions per AZ each partition has it own rack - no sharing between partitions Instances can be placed in a specific partition OR auto placed Large distributed and replciated workloads such as Hadoop, Cassandra and Kafka Contain the impact of failure to specific part of an application","title":"Partition - groups of instances spread apart"},{"location":"compute/elb/","text":"ELB architecture IPv4 or Dual stack ( IPv4 + IPv6) Public (internet-facing) vs Private (internal-facing: only private address) internet-facing can connect to both private and public instances A DNS record is asigned to the load balancer LB nodes are distributed in multiple AZs and resolved via DNS record assigned to the Load Balancer At least 8+ free IPs per subnet & /27 or larger subnet to allow for scale CROSS-ZONE LB Each node can forward traffic to a different zone that it is located in Enabled by default in ALB and disabled by default in NLB and CLB . disabling can create uneven distribution of load across compute behind Session State Server-side piece of information Use external session storage to avoid disruption from instances failures Application Load Balancer vs Network Load Balancer CLB doest not scale... every unique name requires an inidivual CLB because SNI is not supported SNI - Server Name Indication ALB - Applicaiton Load Balancer Layer 7 Load balancer.. listens on HTTP and/or HTTPS No other Layer 7 protocols (SMTP, SSH, Gaming) NO TCP/UDP/TLS Listerners L7 content type, cookies, custom headers, user location and app behavior HTTP HTTPS (SSL/TLS) always terminated on the ALB - no unbroken SSL (security teams) a new connection is made to the application ALB MUST have SSL certs if HTTPS is used ALBs are slower than NLB.. more levels of the network stack to process Health checks evaluate application health .. layer 7 Rules Rules direct connections which arrive at listener Proccessed in priority order Default rule = catchall Rule conditions : host-header, http-header, http-request-method, path-pattern, query-string & source-ip NLB - Network Load Balancer Layer 4 load balancer .. TCP, TLS,UDP, TCP_UDP No visibility or understanding of HTTP or HTTPS No headers, no cookies, not session stickiness Reallly Really Really Fast ( millions of rps, 25% of ALB latency ) can deal with SMTP, SSH, Servers, finanacial apps (not https) Health checks JUST check ICM/TCP Handshake ( not app aware ) NLB's can have static IP's - useful for whitelisting Forward TCP to instances - unbroken encryption Used with private link to provide services to other VPCs Session Stickiness Applications not leveraging extenal session storage can use session stickiness feature from ELB to manage the user session internally Locks a session to 1 backend instance AWSALB cookie is used - 1s to 7 days duration new cookie is issued if backend instance goes down (unheathy) new cookie is issued if the cookie itself expires can create uneven load for backend instances Key points - logout, lost,carts, lost progress => lost session state Gateway Load Balancer - GWLB Gateway Load Balancers enable you to deploy, scale, and manage virtual appliances, such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems. Run and scale 3rd party appliances Example - firewalls, intrusion detection and prevention systems Inbound and Outbound traffic ( transparent inspection an protection) GWLB Endpoints (VPC) -> traffic enters/leaves via these endpoints GWLB -> balances accross multiple backend appliances forward traffic without modifying it using tunnel GENEVE protocol -> tunneling protocol (between GWLB and backend instances) Traffic and metadata is tunnelled using GENEVE protocol IGW is configured with ingress routed table (gateway route table)","title":"ASG"},{"location":"compute/elb/#elb-architecture","text":"IPv4 or Dual stack ( IPv4 + IPv6) Public (internet-facing) vs Private (internal-facing: only private address) internet-facing can connect to both private and public instances A DNS record is asigned to the load balancer LB nodes are distributed in multiple AZs and resolved via DNS record assigned to the Load Balancer At least 8+ free IPs per subnet & /27 or larger subnet to allow for scale CROSS-ZONE LB Each node can forward traffic to a different zone that it is located in Enabled by default in ALB and disabled by default in NLB and CLB . disabling can create uneven distribution of load across compute behind","title":"ELB architecture"},{"location":"compute/elb/#session-state","text":"Server-side piece of information Use external session storage to avoid disruption from instances failures","title":"Session State"},{"location":"compute/elb/#application-load-balancer-vs-network-load-balancer","text":"CLB doest not scale... every unique name requires an inidivual CLB because SNI is not supported SNI - Server Name Indication","title":"Application Load Balancer vs Network Load Balancer"},{"location":"compute/elb/#alb-applicaiton-load-balancer","text":"Layer 7 Load balancer.. listens on HTTP and/or HTTPS No other Layer 7 protocols (SMTP, SSH, Gaming) NO TCP/UDP/TLS Listerners L7 content type, cookies, custom headers, user location and app behavior HTTP HTTPS (SSL/TLS) always terminated on the ALB - no unbroken SSL (security teams) a new connection is made to the application ALB MUST have SSL certs if HTTPS is used ALBs are slower than NLB.. more levels of the network stack to process Health checks evaluate application health .. layer 7 Rules Rules direct connections which arrive at listener Proccessed in priority order Default rule = catchall Rule conditions : host-header, http-header, http-request-method, path-pattern, query-string & source-ip","title":"ALB - Applicaiton Load Balancer"},{"location":"compute/elb/#nlb-network-load-balancer","text":"Layer 4 load balancer .. TCP, TLS,UDP, TCP_UDP No visibility or understanding of HTTP or HTTPS No headers, no cookies, not session stickiness Reallly Really Really Fast ( millions of rps, 25% of ALB latency ) can deal with SMTP, SSH, Servers, finanacial apps (not https) Health checks JUST check ICM/TCP Handshake ( not app aware ) NLB's can have static IP's - useful for whitelisting Forward TCP to instances - unbroken encryption Used with private link to provide services to other VPCs","title":"NLB - Network Load Balancer"},{"location":"compute/elb/#session-stickiness","text":"Applications not leveraging extenal session storage can use session stickiness feature from ELB to manage the user session internally Locks a session to 1 backend instance AWSALB cookie is used - 1s to 7 days duration new cookie is issued if backend instance goes down (unheathy) new cookie is issued if the cookie itself expires can create uneven load for backend instances Key points - logout, lost,carts, lost progress => lost session state","title":"Session Stickiness"},{"location":"compute/elb/#gateway-load-balancer-gwlb","text":"Gateway Load Balancers enable you to deploy, scale, and manage virtual appliances, such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems. Run and scale 3rd party appliances Example - firewalls, intrusion detection and prevention systems Inbound and Outbound traffic ( transparent inspection an protection) GWLB Endpoints (VPC) -> traffic enters/leaves via these endpoints GWLB -> balances accross multiple backend appliances forward traffic without modifying it using tunnel GENEVE protocol -> tunneling protocol (between GWLB and backend instances) Traffic and metadata is tunnelled using GENEVE protocol IGW is configured with ingress routed table (gateway route table)","title":"Gateway Load Balancer - GWLB"},{"location":"containers/ecs/","text":"","title":"Ecs"},{"location":"data-analytics/aws-batch/","text":"Overview AWS Managed Batch processing Product Batch processing - jobs that can run without end user interaction or can be scheduled to run as resources permit Lets customer define jobs and it handles compute and orchestration Let overcome 15 minutes limitation of AWS Lambda It can be dropped into many event-driven workflows for jobs which go beyond the capabilities of Lambda. Components Job Script, Exeuctable or Docker container submitted to batch can reference other jobs by name or ID cna be dependent on the successful completion of other jobs Job Definitions MetaData for a job IAM Resources Resource config - cpu and memory Environment vars and mount points Job Queues Jobs are submitted to a queue Batch job resides until it is scheduled onto a compute environment. can associatew one or more compute environments with a job queue Compute Environment Managed Compute Environment Batch manages the capacity based on workolads You can choose EC2 isntances - On-demand or Spot Instances - size./type You can determine your own max spot price VPC - Public/Private -- requires VPC gateways Unmanaged Compute Environment You manage your own compute resources in the enviroment Resources like AWS ECS cluster Redirect batch to use your own managed resources High or low priority environment Integration","title":"AWS Batch"},{"location":"data-analytics/aws-batch/#overview","text":"AWS Managed Batch processing Product Batch processing - jobs that can run without end user interaction or can be scheduled to run as resources permit Lets customer define jobs and it handles compute and orchestration Let overcome 15 minutes limitation of AWS Lambda It can be dropped into many event-driven workflows for jobs which go beyond the capabilities of Lambda.","title":"Overview"},{"location":"data-analytics/aws-batch/#components","text":"Job Script, Exeuctable or Docker container submitted to batch can reference other jobs by name or ID cna be dependent on the successful completion of other jobs Job Definitions MetaData for a job IAM Resources Resource config - cpu and memory Environment vars and mount points Job Queues Jobs are submitted to a queue Batch job resides until it is scheduled onto a compute environment. can associatew one or more compute environments with a job queue Compute Environment Managed Compute Environment Batch manages the capacity based on workolads You can choose EC2 isntances - On-demand or Spot Instances - size./type You can determine your own max spot price VPC - Public/Private -- requires VPC gateways Unmanaged Compute Environment You manage your own compute resources in the enviroment Resources like AWS ECS cluster Redirect batch to use your own managed resources High or low priority environment","title":"Components"},{"location":"data-analytics/aws-batch/#integration","text":"","title":"Integration"},{"location":"data-analytics/emr/","text":"MapReduce MapReduce is a process for large scale parallel processing of large datasets Data Analysis Architecture - huge scale, parallel processing MAP & REDUCE Data is seperated into splits and each assigned to mapper Perform Operations at scale - customisable Recombine Data into Results HDFS - Hadoop File System Stored across multiple data nodes Highly Fault-tolerant - replicated between nodes Name Node - provides the namespace for file system & controls access to HDFS Block .. segment of data on HDFS.. generally 64 MB EMR Elastic Map Reduce ( EMR ) is the AWS Managed implementation of EMR/Hadoop within AWS. AWS Managed Implementation of Apache Hadoop and Spark, HBase, Presto, Flink, Hive, Pig... Can be operated long-term .. or use ad-hoc ( transient ) clusters Runs in One AZ in a VPC using EC2 for compute Auto Scales - Spot, Instance Fleet, Reserved, On-demand Big Data processing, manipulation, analytics , indexing , transformation and more .. data pipeline* Node Type Master Node (at least one in the cluster) coordinate the distribution of data and tasks among other nodes for processing tracks the status of tasks and monitors the health of the cluster Core Node (zero or more nodes) run tasks tracker and store data in the HDFS can also run task Task Node (optional) node with software components that only runs tasks does not store data in HDFS ideal for SPOT based scaling EMRFS - resilient file system supported natively within EMR backed by s3 reigionally resilient Persists past the lifetime of the cluster","title":"EMR"},{"location":"data-analytics/emr/#mapreduce","text":"MapReduce is a process for large scale parallel processing of large datasets Data Analysis Architecture - huge scale, parallel processing MAP & REDUCE Data is seperated into splits and each assigned to mapper Perform Operations at scale - customisable Recombine Data into Results HDFS - Hadoop File System Stored across multiple data nodes Highly Fault-tolerant - replicated between nodes Name Node - provides the namespace for file system & controls access to HDFS Block .. segment of data on HDFS.. generally 64 MB","title":"MapReduce"},{"location":"data-analytics/emr/#emr","text":"Elastic Map Reduce ( EMR ) is the AWS Managed implementation of EMR/Hadoop within AWS. AWS Managed Implementation of Apache Hadoop and Spark, HBase, Presto, Flink, Hive, Pig... Can be operated long-term .. or use ad-hoc ( transient ) clusters Runs in One AZ in a VPC using EC2 for compute Auto Scales - Spot, Instance Fleet, Reserved, On-demand Big Data processing, manipulation, analytics , indexing , transformation and more .. data pipeline* Node Type Master Node (at least one in the cluster) coordinate the distribution of data and tasks among other nodes for processing tracks the status of tasks and monitors the health of the cluster Core Node (zero or more nodes) run tasks tracker and store data in the HDFS can also run task Task Node (optional) node with software components that only runs tasks does not store data in HDFS ideal for SPOT based scaling EMRFS - resilient file system supported natively within EMR backed by s3 reigionally resilient Persists past the lifetime of the cluster","title":"EMR"},{"location":"data-analytics/kinesis/","text":"Concepts Kinesis is a scalable streaming servcice Producers send data into a kinesis stream Streams can scale from low to near infinite data rates Public service & highly available by design Streams store a 24-hour moving window of data (can be extended to 7 days for extra cost) Multiple consumers access data from that moving window kinesis-cheat-sheet SQS vs Kinesis SQS Kinesis Async communication and decoupling of applicaion with worker pools Huge scaled ingestion of data - throughput One Production group <-> One consumption group Multiple consumers - realtime or periodically No persistence of messages, no window Window= 24 hrs by default ( max 7 days ) Data ingestion, analytics, monitoring, app clicks Kinesis Data Firehose Fully managed service to load data for data lakes, data stores ad analytical services . Automatic scaling .. fully serverless and resilient Near Real Time Delivery (~60s) Supports transformation of data on the fly ( lambda ) Biling - volume through firehose Exception - copying the data to intermediary S3 bucket before sending it to Redshift Kinesis Data Analytics Amazon Kinesis Data Analytics is the easiest way to analyze streaming data, gain actionable insights, and respond to your business and customer needs in real time. Real time processing of data using SQL Ingests from Kinesis Data Streams or Firehose Supported Destinations Firehose OR any fireshose supported destinations S3, Splunk, HTTP, Redshift, ElasticSearch AWS Lambda - realtime Kinesis Data Streams - realtime Use cases Streaming data needing real-time SQL processing Time-series anlytics - elections/e-sports Real-time dashboards - leaderboards for games Real-time metrics - security & response teams","title":"Kinesis"},{"location":"data-analytics/kinesis/#concepts","text":"Kinesis is a scalable streaming servcice Producers send data into a kinesis stream Streams can scale from low to near infinite data rates Public service & highly available by design Streams store a 24-hour moving window of data (can be extended to 7 days for extra cost) Multiple consumers access data from that moving window kinesis-cheat-sheet","title":"Concepts"},{"location":"data-analytics/kinesis/#sqs-vs-kinesis","text":"SQS Kinesis Async communication and decoupling of applicaion with worker pools Huge scaled ingestion of data - throughput One Production group <-> One consumption group Multiple consumers - realtime or periodically No persistence of messages, no window Window= 24 hrs by default ( max 7 days ) Data ingestion, analytics, monitoring, app clicks","title":"SQS vs Kinesis"},{"location":"data-analytics/kinesis/#kinesis-data-firehose","text":"Fully managed service to load data for data lakes, data stores ad analytical services . Automatic scaling .. fully serverless and resilient Near Real Time Delivery (~60s) Supports transformation of data on the fly ( lambda ) Biling - volume through firehose Exception - copying the data to intermediary S3 bucket before sending it to Redshift","title":"Kinesis Data Firehose"},{"location":"data-analytics/kinesis/#kinesis-data-analytics","text":"Amazon Kinesis Data Analytics is the easiest way to analyze streaming data, gain actionable insights, and respond to your business and customer needs in real time. Real time processing of data using SQL Ingests from Kinesis Data Streams or Firehose Supported Destinations Firehose OR any fireshose supported destinations S3, Splunk, HTTP, Redshift, ElasticSearch AWS Lambda - realtime Kinesis Data Streams - realtime","title":"Kinesis Data Analytics"},{"location":"data-analytics/kinesis/#use-cases","text":"Streaming data needing real-time SQL processing Time-series anlytics - elections/e-sports Real-time dashboards - leaderboards for games Real-time metrics - security & response teams","title":"Use cases"},{"location":"data-analytics/quicksight/","text":"Overview Business Analytics & Intelligence ( BA/BI) service Visualisation, Ad-hoc Analysis Discovery and Integration with AWS Data sources and works with external data sources Dashboard and visualizations QuickSight Sources Athena, Aurora, Redshift, Redshift Spectrum S3, AWS IOT JIRA, Github, Twitter, Salesforce Microsoft SQL Server, MySQL, PostgresSQL Apache Spark, Snowflake, Presto, Teradata","title":"AWS QuickSights"},{"location":"data-analytics/quicksight/#overview","text":"Business Analytics & Intelligence ( BA/BI) service Visualisation, Ad-hoc Analysis Discovery and Integration with AWS Data sources and works with external data sources Dashboard and visualizations","title":"Overview"},{"location":"data-analytics/quicksight/#quicksight-sources","text":"Athena, Aurora, Redshift, Redshift Spectrum S3, AWS IOT JIRA, Github, Twitter, Salesforce Microsoft SQL Server, MySQL, PostgresSQL Apache Spark, Snowflake, Presto, Teradata","title":"QuickSight Sources"},{"location":"data-analytics/redshift/","text":"Overview Petabyte scaled Data warehouse OLAP ( Column based) not OLTP (row/transaction) Pay as you use . similar structure to RDS Directly Query S3 using Redshift Spectrum - withouot loading data to Redshift Directly Query other DBs using federated query Integrates with AWS tooling such as Quicksight SQL-like interface JDBC/ODBC connections Architecture Server based ( not serverless ) One AZ in a VPC - network cost/performance Leader Node - Query input, planning and aggregation Compute Node - performning queries of data VPC Security, IAM Permissions, KMS at rest Encryption, CW Monitoring Redshift Enhanced VPC Routing - VPC Networking!! traffic is routed via VPC configuration Resilience and Recovery S3 for backups Automated every ~8 hours or 5GB of data 1 day retention (configurable to 35 days ) Manual snaps can be taken at any time deleted by an admin as required Snapshots can be restored in different AZ or regions Can be configured to copy snapshots to another region for DR - with a seperate configurable retention period","title":"RedShift"},{"location":"data-analytics/redshift/#overview","text":"Petabyte scaled Data warehouse OLAP ( Column based) not OLTP (row/transaction) Pay as you use . similar structure to RDS Directly Query S3 using Redshift Spectrum - withouot loading data to Redshift Directly Query other DBs using federated query Integrates with AWS tooling such as Quicksight SQL-like interface JDBC/ODBC connections","title":"Overview"},{"location":"data-analytics/redshift/#architecture","text":"Server based ( not serverless ) One AZ in a VPC - network cost/performance Leader Node - Query input, planning and aggregation Compute Node - performning queries of data VPC Security, IAM Permissions, KMS at rest Encryption, CW Monitoring Redshift Enhanced VPC Routing - VPC Networking!! traffic is routed via VPC configuration","title":"Architecture"},{"location":"data-analytics/redshift/#resilience-and-recovery","text":"S3 for backups Automated every ~8 hours or 5GB of data 1 day retention (configurable to 35 days ) Manual snaps can be taken at any time deleted by an admin as required Snapshots can be restored in different AZ or regions Can be configured to copy snapshots to another region for DR - with a seperate configurable retention period","title":"Resilience and Recovery"},{"location":"databases/athena/","text":"Overview Serverless Interactive Querying Service - NO INFRASTRUCTURE Ad-hoc queries on data - pay only data consumed Schema-on-read - table-like translation Original data never changed - remains on S3 Schema translates data => relational-like when read Ouput can be sent to other AWS services SQL like queries Queries where loading/transformation is not desired Cost effective Querying AWS logs - VPC Flow Logs,CloudTrail, ELB logs, cost reports AWS Glue Data Catalog and Web Server Logs w/ Athena Federated Query .. other data sources (non s3 sources) using custom and pre-built connectors","title":"Athena"},{"location":"databases/athena/#overview","text":"Serverless Interactive Querying Service - NO INFRASTRUCTURE Ad-hoc queries on data - pay only data consumed Schema-on-read - table-like translation Original data never changed - remains on S3 Schema translates data => relational-like when read Ouput can be sent to other AWS services SQL like queries Queries where loading/transformation is not desired Cost effective Querying AWS logs - VPC Flow Logs,CloudTrail, ELB logs, cost reports AWS Glue Data Catalog and Web Server Logs w/ Athena Federated Query .. other data sources (non s3 sources) using custom and pre-built connectors","title":"Overview"},{"location":"databases/aurora/","text":"Overview Cluster Mode A single primary instance + 0 or more replicas primary instance - reads and writes secondary instance - only reads upto 15 replicas - can be anyone to be primary during failover No local storage - uses cluster volume (SSD based) faster provisioning & improved availability & performance All SSD Based - high IOPs, low latency Pay for only what is being used Volumes are replicated synchronously High water mark - billed for most used No performance degradation during replication Replicas can be added and removed without requiring storage provisioning Endpoints Cluster Endpoint - points to primary instance Reader Endpoint - pooints to replicas (load-balance between multiple replicas) Custom Endpoint - custom endpoint using db identifiers Cost No Free Tier Option Does not support micro instances Beyond RDS singleAZ (micro) Aurora offers better value Compute - hourly charge, per second, 10 minutes minimum Storage - GB-Month consumed, IO cost per request 100% DB Size in backups are included Restore, Clone and Backtrack Backups in Aurora work in the same ways as RDS Restores creates a new cluster Backtrack can be used which allow in-place rewinds to a previous point in time Fast clone make a new database MUCH faster than copying all the data - copy-on-write just refrence the origin storage store the difference Aurora Serverless Scalable => ACU - Aurora Capacity Units MIN & MAX ACU is set for cluster Scalable between MIN and MAX based on the load Consumption billing per-second basis Same reslience as Aurora (6 copies accross AZs) Use Cases Infrequently used applications New applications Variable workloads Unpredictable workloads Development and test databases Multi-tenant applications Aurora Multi-Master Default Aurora mode is Single-Master Failover takes time - replica promoted to R/W In Multi-Master mode all instances are R/W quoram of nodes to agree on the data to be written Traffic is instantly switched to different master instance if one master fails","title":"Aurora"},{"location":"databases/aurora/#overview","text":"Cluster Mode A single primary instance + 0 or more replicas primary instance - reads and writes secondary instance - only reads upto 15 replicas - can be anyone to be primary during failover No local storage - uses cluster volume (SSD based) faster provisioning & improved availability & performance All SSD Based - high IOPs, low latency Pay for only what is being used Volumes are replicated synchronously High water mark - billed for most used No performance degradation during replication Replicas can be added and removed without requiring storage provisioning Endpoints Cluster Endpoint - points to primary instance Reader Endpoint - pooints to replicas (load-balance between multiple replicas) Custom Endpoint - custom endpoint using db identifiers","title":"Overview"},{"location":"databases/aurora/#cost","text":"No Free Tier Option Does not support micro instances Beyond RDS singleAZ (micro) Aurora offers better value Compute - hourly charge, per second, 10 minutes minimum Storage - GB-Month consumed, IO cost per request 100% DB Size in backups are included","title":"Cost"},{"location":"databases/aurora/#restore-clone-and-backtrack","text":"Backups in Aurora work in the same ways as RDS Restores creates a new cluster Backtrack can be used which allow in-place rewinds to a previous point in time Fast clone make a new database MUCH faster than copying all the data - copy-on-write just refrence the origin storage store the difference","title":"Restore, Clone and Backtrack"},{"location":"databases/aurora/#aurora-serverless","text":"Scalable => ACU - Aurora Capacity Units MIN & MAX ACU is set for cluster Scalable between MIN and MAX based on the load Consumption billing per-second basis Same reslience as Aurora (6 copies accross AZs)","title":"Aurora Serverless"},{"location":"databases/aurora/#use-cases","text":"Infrequently used applications New applications Variable workloads Unpredictable workloads Development and test databases Multi-tenant applications","title":"Use Cases"},{"location":"databases/aurora/#aurora-multi-master","text":"Default Aurora mode is Single-Master Failover takes time - replica promoted to R/W In Multi-Master mode all instances are R/W quoram of nodes to agree on the data to be written Traffic is instantly switched to different master instance if one master fails","title":"Aurora Multi-Master"},{"location":"databases/dynamodb/","text":"Concepts NoSQL - DBaaS - Key/Value & Document No self managed servers or infrastructure Manual/Automatic provisioned performance IN/OUT or On-Demand Highly Resiliient .. accross AZs and optionally global Really fast .. single-digit milliseconds (SSD based) Backups, point-time-recovery, encryption at rest Event-Driven Integration Tables A table is grouping of ITEMS with the same primary key PRIMARY KEY Simple partition key ( PK ) => Simple Partition Key ( PK ) and Sort Key ( SK ) => Composite DDB has no rigid attribute schema Each iterm MUST have a unique value for PK and SK Item max size - 400KB Capacity - Speed WCU = Write Capacity Unit RCU = Read Capacity Unit - 1 WCU = 1KB per second - 1 RCU = 4KB per second On-Demand Backups Full copy of table - retained until removed Restore Same or Cross region With or Without Indexes adjust Encryption Settings Point-in-time Recovery (PITR) Enabled on the table - disabled by default 35 day recovery window Continuous record of changes allows replay to any point in the window 1 second grnularity Operations Provisioned On-Demand for unkonw, unpredicatable, low admin RCU and WCU set at tables cost per million R or W unts every operation consumes at least 1 RCU/WCU* > Every table has a RCU and WCU burst pool (300 seconds) > Provisioned Throughput Exception Query - using PK and SK (optional) Capacity consumed is the size of all returned items Scan - moves through a table consuming the capacity of every ITEM Consistency Model Eventual Cosistent Strongly Consistent Writes are replicated to non-leader nodes eventually ( ms ) Writes are replicated to non-leader nodes eventually ( ms ) No guaranted updated data - reads from non-leader nodes Always read from the leader node gurantees updated data Cheaper than eventual Pricier than eventual Indexes GSI - Global Secondary Indexes can be created at any time 20 per base table by default alternative PK and SK GSIs have their own RCU and WCU allocations Attributes - ALL, KEYS_ONLY, INCLUDE ALWAYS EVENTUALLY CONSISTENT LSI - Local Secondary Indexes alternative view for a table must be created with table ( when strong consistency is required ) 5 LSI's per base table alternative SK on the table shares the RCU and WCU with the table Attributes - ALL, KEYS_ONLY, INCLUDE Streams Time ordered list of ITEM Changes in a table 24-Hour rolling window Enabled on a per table basis Records INSERTS, UPDATES and DELETES Different view types influence what is in the stream View Types KEYS_ONLY NEW_IMAGE OLD_IMAGE NEW_AND_OLD_IMAGES Triggers Item changes generate an event That event contains the data which changed - what depending on the view type A action is taken on that data AWS => Streams + Lambda Reporting & Analytics Aggregation, Messaging or Notifications DynamoDB Accelerator - DAX In-memory cache designed specifically for DynamoDB Applications calls DAX using DAX SDK which eliminates the overhead of application checking cache for every call. DAX is deployed within the VPC Primary NODE (Writes) and Replicas (Read) Nodes are HA .. Primary failure = election In-Memory cache - Scaling - faster reads, reduced costs Both vertical and horizontal scaling Supports write-through writes the data as well as cache it Global Tables Global table provides multi-master cross-region replication Tables are created in mutiple regions and added to the same global table (becoming replica tables) Last writer wins is used for conflict resolution Reads and Writes replication between regions Strongly consistent reads ONLY in the same region as writes Global eventual consistency for different regions tables Multi-master replication Data is replicated within a second All tables can be used for Read and Write operations Provides Global HA and Global DR/BC Time-To-Live - TTL Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. While deleting the item once the item TTL is expired, no capacity units is consumed for the actions.","title":"DynamoDB"},{"location":"databases/dynamodb/#concepts","text":"NoSQL - DBaaS - Key/Value & Document No self managed servers or infrastructure Manual/Automatic provisioned performance IN/OUT or On-Demand Highly Resiliient .. accross AZs and optionally global Really fast .. single-digit milliseconds (SSD based) Backups, point-time-recovery, encryption at rest Event-Driven Integration","title":"Concepts"},{"location":"databases/dynamodb/#tables","text":"A table is grouping of ITEMS with the same primary key PRIMARY KEY Simple partition key ( PK ) => Simple Partition Key ( PK ) and Sort Key ( SK ) => Composite DDB has no rigid attribute schema Each iterm MUST have a unique value for PK and SK Item max size - 400KB Capacity - Speed WCU = Write Capacity Unit RCU = Read Capacity Unit - 1 WCU = 1KB per second - 1 RCU = 4KB per second","title":"Tables"},{"location":"databases/dynamodb/#on-demand-backups","text":"Full copy of table - retained until removed Restore Same or Cross region With or Without Indexes adjust Encryption Settings","title":"On-Demand Backups"},{"location":"databases/dynamodb/#point-in-time-recovery-pitr","text":"Enabled on the table - disabled by default 35 day recovery window Continuous record of changes allows replay to any point in the window 1 second grnularity","title":"Point-in-time Recovery (PITR)"},{"location":"databases/dynamodb/#operations","text":"Provisioned On-Demand for unkonw, unpredicatable, low admin RCU and WCU set at tables cost per million R or W unts every operation consumes at least 1 RCU/WCU* > Every table has a RCU and WCU burst pool (300 seconds) > Provisioned Throughput Exception Query - using PK and SK (optional) Capacity consumed is the size of all returned items Scan - moves through a table consuming the capacity of every ITEM","title":"Operations"},{"location":"databases/dynamodb/#consistency-model","text":"Eventual Cosistent Strongly Consistent Writes are replicated to non-leader nodes eventually ( ms ) Writes are replicated to non-leader nodes eventually ( ms ) No guaranted updated data - reads from non-leader nodes Always read from the leader node gurantees updated data Cheaper than eventual Pricier than eventual","title":"Consistency Model"},{"location":"databases/dynamodb/#indexes","text":"GSI - Global Secondary Indexes can be created at any time 20 per base table by default alternative PK and SK GSIs have their own RCU and WCU allocations Attributes - ALL, KEYS_ONLY, INCLUDE ALWAYS EVENTUALLY CONSISTENT LSI - Local Secondary Indexes alternative view for a table must be created with table ( when strong consistency is required ) 5 LSI's per base table alternative SK on the table shares the RCU and WCU with the table Attributes - ALL, KEYS_ONLY, INCLUDE","title":"Indexes"},{"location":"databases/dynamodb/#streams","text":"Time ordered list of ITEM Changes in a table 24-Hour rolling window Enabled on a per table basis Records INSERTS, UPDATES and DELETES Different view types influence what is in the stream View Types KEYS_ONLY NEW_IMAGE OLD_IMAGE NEW_AND_OLD_IMAGES","title":"Streams"},{"location":"databases/dynamodb/#triggers","text":"Item changes generate an event That event contains the data which changed - what depending on the view type A action is taken on that data AWS => Streams + Lambda Reporting & Analytics Aggregation, Messaging or Notifications","title":"Triggers"},{"location":"databases/dynamodb/#dynamodb-accelerator-dax","text":"In-memory cache designed specifically for DynamoDB Applications calls DAX using DAX SDK which eliminates the overhead of application checking cache for every call. DAX is deployed within the VPC Primary NODE (Writes) and Replicas (Read) Nodes are HA .. Primary failure = election In-Memory cache - Scaling - faster reads, reduced costs Both vertical and horizontal scaling Supports write-through writes the data as well as cache it","title":"DynamoDB Accelerator - DAX"},{"location":"databases/dynamodb/#global-tables","text":"Global table provides multi-master cross-region replication Tables are created in mutiple regions and added to the same global table (becoming replica tables) Last writer wins is used for conflict resolution Reads and Writes replication between regions Strongly consistent reads ONLY in the same region as writes Global eventual consistency for different regions tables Multi-master replication Data is replicated within a second All tables can be used for Read and Write operations Provides Global HA and Global DR/BC","title":"Global Tables"},{"location":"databases/dynamodb/#time-to-live-ttl","text":"Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. While deleting the item once the item TTL is expired, no capacity units is consumed for the actions.","title":"Time-To-Live - TTL"},{"location":"databases/elasticsearch/","text":"Overview Managed implementation of Elasticsearch ELK - ES, Kibana and Logstash It is not serverless .. it runs in a VPC using compute Log analytics, monitoring, security analytics, full text search and indexing, clickstream analytics ELK Elasticsearch - Search and Indexing Kibana - visualisation/ dashboards Logstash - similar to CWLogs, needs a Logstash agent installed on anything to ingest the data","title":"ElasticSearch"},{"location":"databases/elasticsearch/#overview","text":"Managed implementation of Elasticsearch ELK - ES, Kibana and Logstash It is not serverless .. it runs in a VPC using compute Log analytics, monitoring, security analytics, full text search and indexing, clickstream analytics","title":"Overview"},{"location":"databases/elasticsearch/#elk","text":"Elasticsearch - Search and Indexing Kibana - visualisation/ dashboards Logstash - similar to CWLogs, needs a Logstash agent installed on anything to ingest the data","title":"ELK"},{"location":"databases/neptune/","text":"Overview Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets Managed Graph Database Service Neptune runs in a VPC - private by default MultiAZ & Scales via read replicas Continous backup to s3 -- point in time recovery Like RDS but for GRAPH style data Nodes Use Cases Graph style data Social Media.. anything involing fluid relationships Fraud Prevention Recommendation Engines Network and IT Operations... dependancies Biology and other life sciences","title":"Neptune"},{"location":"databases/neptune/#overview","text":"Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets Managed Graph Database Service Neptune runs in a VPC - private by default MultiAZ & Scales via read replicas Continous backup to s3 -- point in time recovery Like RDS but for GRAPH style data Nodes","title":"Overview"},{"location":"databases/neptune/#use-cases","text":"Graph style data Social Media.. anything involing fluid relationships Fraud Prevention Recommendation Engines Network and IT Operations... dependancies Biology and other life sciences","title":"Use Cases"},{"location":"databases/quantum-ledger/","text":"Overview Amazon QLDB is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log owned by a central trusted authority Immutable append-only ledger based database Cryptographically verfiable transaction log Transparent - full history is always accessible Serverless - Provides Ledge and Tables - not servers 3AZ resilience and replication within each AZ Can stream data to Amazon Kinesis Document DB Model ACID - Transfer A->B .. either both work or none work Use cases Finance - account balances & transactions Medical - full history of data changed Logistics - track movement of objects Legal - track usage and change of data(custody)","title":"Quantum Ledger"},{"location":"databases/quantum-ledger/#overview","text":"Amazon QLDB is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log owned by a central trusted authority Immutable append-only ledger based database Cryptographically verfiable transaction log Transparent - full history is always accessible Serverless - Provides Ledge and Tables - not servers 3AZ resilience and replication within each AZ Can stream data to Amazon Kinesis Document DB Model ACID - Transfer A->B .. either both work or none work","title":"Overview"},{"location":"databases/quantum-ledger/#use-cases","text":"Finance - account balances & transactions Medical - full history of data changed Logistics - track movement of objects Legal - track usage and change of data(custody)","title":"Use cases"},{"location":"databases/rds/","text":"Overview Database Server as a service Managed Database Instances ( 1+ databases) Supported DB Engines: MySQL MariaDB PostgresSQL Oracle Microsoft SQL Server RDS Database Instance DB Engine instances like db.m5 , db.r5 , db.t3 => cpu and memory for each db type Database Cname Single or Multiple AZ EBS backed volume Security groups to control access to the RDS Multi-AZ Extra cost for replicas - NOT FREE TIER Synchronize replciation with standby instance Standby cannot be directly accessed 60-120s failover - brief interruption Same region only (other AZs in the VPC) Backups taken from Standby (removes performance impact) Failvoer events - AZ outage, primary failure, manual failover, instance type change and software patching RDS Backups and Restores Manual Snapshots and Automatic Backups AWS managed S3 buckets S3 buckets are regional resilience Manual snapshots are performed manually and live past the termination of RDS Snapshots from either primary or standby (mult-AZ) First Snap is FULL Then onward = Incremental Backup windows - could impact performance Automatic backups can be taken for an RDS instance with 0 (disabled) 35 Day retention only last within the lifecycle of the RDS a final snapshot can be created before deleting the RDS Automatic backups also use S3 for storing transaction logs every 5 minutes - allowing for point in time recovery . Logs can be replayed on top of snapshots to restore RDS in point in time Snapshots can be restored .. but create a new RDS instance. RDS Read Replicas Asynchronous replications Performance Improvements 5X direct read-replicas per DB instance Each provide an additional instance of read performance Replicas also can have replicas - data lgging issue Global performance improvements - cross region replicas Availability Performance Snapshots and backups improve RPO RR's can be promoted quickly if the primary instance fails - low RTO data corruption is not handled - corrupted data is also replicated RDS Security SSL/TLS - in transit EBS Volume encryption - KMS Handled by Host/EBS AWS or Customer Managed CMK generates data keys Data keys used for encryption operations Storage, Logs, Snapshots & replicas are encrypted encryption can't be removed MSSQL and Oracle support TDE (Transparent Data Encryption) Encryption handled within the DB Engine RDS Oracle supports integration with CloudHSM CloudHSM is much stronger RDS IAM Authentication Not AUTHORIZATION","title":"RDS"},{"location":"databases/rds/#overview","text":"Database Server as a service Managed Database Instances ( 1+ databases) Supported DB Engines: MySQL MariaDB PostgresSQL Oracle Microsoft SQL Server RDS Database Instance DB Engine instances like db.m5 , db.r5 , db.t3 => cpu and memory for each db type Database Cname Single or Multiple AZ EBS backed volume Security groups to control access to the RDS","title":"Overview"},{"location":"databases/rds/#multi-az","text":"Extra cost for replicas - NOT FREE TIER Synchronize replciation with standby instance Standby cannot be directly accessed 60-120s failover - brief interruption Same region only (other AZs in the VPC) Backups taken from Standby (removes performance impact) Failvoer events - AZ outage, primary failure, manual failover, instance type change and software patching","title":"Multi-AZ"},{"location":"databases/rds/#rds-backups-and-restores","text":"Manual Snapshots and Automatic Backups AWS managed S3 buckets S3 buckets are regional resilience Manual snapshots are performed manually and live past the termination of RDS Snapshots from either primary or standby (mult-AZ) First Snap is FULL Then onward = Incremental Backup windows - could impact performance Automatic backups can be taken for an RDS instance with 0 (disabled) 35 Day retention only last within the lifecycle of the RDS a final snapshot can be created before deleting the RDS Automatic backups also use S3 for storing transaction logs every 5 minutes - allowing for point in time recovery . Logs can be replayed on top of snapshots to restore RDS in point in time Snapshots can be restored .. but create a new RDS instance.","title":"RDS Backups and Restores"},{"location":"databases/rds/#rds-read-replicas","text":"Asynchronous replications Performance Improvements 5X direct read-replicas per DB instance Each provide an additional instance of read performance Replicas also can have replicas - data lgging issue Global performance improvements - cross region replicas Availability Performance Snapshots and backups improve RPO RR's can be promoted quickly if the primary instance fails - low RTO data corruption is not handled - corrupted data is also replicated","title":"RDS Read Replicas"},{"location":"databases/rds/#rds-security","text":"SSL/TLS - in transit EBS Volume encryption - KMS Handled by Host/EBS AWS or Customer Managed CMK generates data keys Data keys used for encryption operations Storage, Logs, Snapshots & replicas are encrypted encryption can't be removed MSSQL and Oracle support TDE (Transparent Data Encryption) Encryption handled within the DB Engine RDS Oracle supports integration with CloudHSM CloudHSM is much stronger","title":"RDS Security"},{"location":"databases/rds/#rds-iam-authentication","text":"Not AUTHORIZATION","title":"RDS IAM Authentication"},{"location":"deployment/ci-cd/","text":"AWS CodeCommit similar to gihub authentication - iam user authorization - iam identity policy notifications - on events triggers - on events AWS CodePipeline CD tool Orchestration tool for code, build, test and deploy controls the flow from source through build and deployment Pipelines are built form STAGES STAGES can have sequential or parallel ACTIONS Movement between stages can require manual approval Artifacts can be loaded into an action , and generated from an action Stage Changes => Event Bridge (eg. success, failed, cancelled) CloudTrail or Console UI to view/interact AWS CodeBuild Code Build as a service - fully managed Pay for the resource consumed during builds Alternate to part of jenkins functionality Used for builds and tests Uses docker for build environment, can be customized Integrates with AWS Services : KMS,IAM, VPC, CloudTrail, S3... Sourced from -> Github or CodeCommit or CodePipeline or S3 Customised via buildspec.yml file (root of source) Logs => CloudWatch, S3 Metrics => CloudWatch Events => EventBridge (event-driven response) supported languages - java,ruby,python,go,.net,php,node.js PHASES install - install packages in build environment pre_build - sign-in or install dependencies build - commands for the build process post_build - package things up, push docker image, explicit notifications also support environment variables - shell, variables, parameter-store, secrets-manager artifacts - where to put AWS CodeDeploy Code Deployment as a service Alternatives - jenkins, ansible, chef, puppet, cloudformation Deploy code.. not resources Deploys to EC2, On-premises, Lambda and ECS Also deploys web, configuration, EXE files, Packages, Scripts, media and more .. CodeDeploy integrates with AWS services and AWS Code* tools For On-premise and EC2 - Codedeploy agent needs to be installed Appsec.yml - yaml or json reference link Files (EC2/On-premise) Permissions (EC2/On-premise) Resources (ECS/Lambda) Hooks (ECS/Lambda/EC2/On-premise) - reference link Lifecycle hooks are different for each type of deployments.","title":"CICD"},{"location":"deployment/ci-cd/#aws-codecommit","text":"similar to gihub authentication - iam user authorization - iam identity policy notifications - on events triggers - on events","title":"AWS CodeCommit"},{"location":"deployment/ci-cd/#aws-codepipeline","text":"CD tool Orchestration tool for code, build, test and deploy controls the flow from source through build and deployment Pipelines are built form STAGES STAGES can have sequential or parallel ACTIONS Movement between stages can require manual approval Artifacts can be loaded into an action , and generated from an action Stage Changes => Event Bridge (eg. success, failed, cancelled) CloudTrail or Console UI to view/interact","title":"AWS CodePipeline"},{"location":"deployment/ci-cd/#aws-codebuild","text":"Code Build as a service - fully managed Pay for the resource consumed during builds Alternate to part of jenkins functionality Used for builds and tests Uses docker for build environment, can be customized Integrates with AWS Services : KMS,IAM, VPC, CloudTrail, S3... Sourced from -> Github or CodeCommit or CodePipeline or S3 Customised via buildspec.yml file (root of source) Logs => CloudWatch, S3 Metrics => CloudWatch Events => EventBridge (event-driven response) supported languages - java,ruby,python,go,.net,php,node.js PHASES install - install packages in build environment pre_build - sign-in or install dependencies build - commands for the build process post_build - package things up, push docker image, explicit notifications also support environment variables - shell, variables, parameter-store, secrets-manager artifacts - where to put","title":"AWS CodeBuild"},{"location":"deployment/ci-cd/#aws-codedeploy","text":"Code Deployment as a service Alternatives - jenkins, ansible, chef, puppet, cloudformation Deploy code.. not resources Deploys to EC2, On-premises, Lambda and ECS Also deploys web, configuration, EXE files, Packages, Scripts, media and more .. CodeDeploy integrates with AWS services and AWS Code* tools For On-premise and EC2 - Codedeploy agent needs to be installed Appsec.yml - yaml or json reference link Files (EC2/On-premise) Permissions (EC2/On-premise) Resources (ECS/Lambda) Hooks (ECS/Lambda/EC2/On-premise) - reference link Lifecycle hooks are different for each type of deployments.","title":"AWS CodeDeploy"},{"location":"deployment/elastic-beanstalk/","text":"Elastic Beanstalk PaaS - Platform as a service Developer Focused - not end user Great for smaller development teams High level - Managed Application Environment User Provide code and EB handles the environment Focus on code, low infrastructure overhead Fully customisable - uses AWS products under the covers Requires application changes .. does not come for free Databases OUTSIDE of Elastic BeanStalk DB in env are lost if env is deleted EB - Platforms Built-in languages Go, java, java + tomcat, .NET Core(linux) & .NET (Windows) nodejs, python, pho, ruby Docker single container docker multi container docker (ECS behind the scene) preconfigured docker for unsupported runtime (glassfish java8) Cutom use packer to create custom platform EB - Application Collection of things relating to an application Versions of application is stored in S3 and sourcable (zip or war) Environments containers of infrastructure running a specific versions each environment has it own CNAME CNAME Swap to enable blue/green deployment web server tier - usually service traffic from load balancers worker tier - processing requests from queue like SQS EB - Deployment Policies All at once - deploy to all at once, brief outage Rolling - deploy in rolling batches Rolling with additional batch - as above, with new batch to maintain capacity during the process 150 % of capacity depending on the configuration both versions serving traffic at one point Immutable - all new instances with new version create another set of ASG delete the old ASG Traffic Splitting - fresh instances, with a traffic split (A/B testing) EB and RDS RDS provisioned with EB is associated with EB and will be deleted once EB environment is deleted. Different data in different environment (different RDS) environment properties: RDS_HOSTNAME , RDS_PORT , RDS_DB_NAME , RDS_USERNAME , RDS_PASSWORD RDS provisioned outside EB should configure above properties as environemnt variables Steps to decouple exsiting RDS create snapshot of RDS enable termination protection create another EB environment with same verison without RDS ensure the EB new environment can connect to RDS make the CNAME switch from old to new environment delete the old EB environment clean up the failed cloudformation stack and retain the RDS created via CFN stack EB customisation link to extensions files extension folder - .ebextensions config file (yaml or json) ends with .config EB & HTTPS apply SSL cert to the load balancer directly via EB console OR apply via .ebextensions/securelistener-[alb][nlb].config option_settings : aws:elbv2:listener:443 : ListernerEnabled : 'true' Protocol : HTTPS SSLCertificateArns : arn:.... EB cloning Clone an existing env to another Update the new env once the cloning is complete RDS data is not copied during cloning Changes outside the EB will not be copied EB and Docker Single container application Dockerfile docker-compose Mutiple container applications ECS cluster EC2 instances provisioned in the cluster and ELB for HA Dockerrun.aws.json (V2) file in root of application source code Docker image will have to be stored in docker registry like ECR","title":"Elastic BeanStalk"},{"location":"deployment/elastic-beanstalk/#elastic-beanstalk","text":"PaaS - Platform as a service Developer Focused - not end user Great for smaller development teams High level - Managed Application Environment User Provide code and EB handles the environment Focus on code, low infrastructure overhead Fully customisable - uses AWS products under the covers Requires application changes .. does not come for free Databases OUTSIDE of Elastic BeanStalk DB in env are lost if env is deleted","title":"Elastic Beanstalk"},{"location":"deployment/elastic-beanstalk/#eb-platforms","text":"Built-in languages Go, java, java + tomcat, .NET Core(linux) & .NET (Windows) nodejs, python, pho, ruby Docker single container docker multi container docker (ECS behind the scene) preconfigured docker for unsupported runtime (glassfish java8) Cutom use packer to create custom platform","title":"EB - Platforms"},{"location":"deployment/elastic-beanstalk/#eb-application","text":"Collection of things relating to an application Versions of application is stored in S3 and sourcable (zip or war) Environments containers of infrastructure running a specific versions each environment has it own CNAME CNAME Swap to enable blue/green deployment web server tier - usually service traffic from load balancers worker tier - processing requests from queue like SQS","title":"EB - Application"},{"location":"deployment/elastic-beanstalk/#eb-deployment-policies","text":"All at once - deploy to all at once, brief outage Rolling - deploy in rolling batches Rolling with additional batch - as above, with new batch to maintain capacity during the process 150 % of capacity depending on the configuration both versions serving traffic at one point Immutable - all new instances with new version create another set of ASG delete the old ASG Traffic Splitting - fresh instances, with a traffic split (A/B testing)","title":"EB - Deployment Policies"},{"location":"deployment/elastic-beanstalk/#eb-and-rds","text":"RDS provisioned with EB is associated with EB and will be deleted once EB environment is deleted. Different data in different environment (different RDS) environment properties: RDS_HOSTNAME , RDS_PORT , RDS_DB_NAME , RDS_USERNAME , RDS_PASSWORD RDS provisioned outside EB should configure above properties as environemnt variables Steps to decouple exsiting RDS create snapshot of RDS enable termination protection create another EB environment with same verison without RDS ensure the EB new environment can connect to RDS make the CNAME switch from old to new environment delete the old EB environment clean up the failed cloudformation stack and retain the RDS created via CFN stack","title":"EB and RDS"},{"location":"deployment/elastic-beanstalk/#eb-customisation","text":"link to extensions files extension folder - .ebextensions config file (yaml or json) ends with .config","title":"EB customisation"},{"location":"deployment/elastic-beanstalk/#eb-https","text":"apply SSL cert to the load balancer directly via EB console OR apply via .ebextensions/securelistener-[alb][nlb].config option_settings : aws:elbv2:listener:443 : ListernerEnabled : 'true' Protocol : HTTPS SSLCertificateArns : arn:....","title":"EB &amp; HTTPS"},{"location":"deployment/elastic-beanstalk/#eb-cloning","text":"Clone an existing env to another Update the new env once the cloning is complete RDS data is not copied during cloning Changes outside the EB will not be copied","title":"EB cloning"},{"location":"deployment/elastic-beanstalk/#eb-and-docker","text":"Single container application Dockerfile docker-compose Mutiple container applications ECS cluster EC2 instances provisioned in the cluster and ELB for HA Dockerrun.aws.json (V2) file in root of application source code Docker image will have to be stored in docker registry like ECR","title":"EB and Docker"},{"location":"deployment/opsworks/","text":"AWS OpsWorks AWS OpsWorks provides managed implementations of Puppet and Chef in a product which integrates with other AWS Products and services. 3 Modes Puppet Enterprise - AWS Managed Puppet Master Server desired state of the infra Chef Automate - AWS Managed Chef Servers infra as code with ruby Opsworks - AWS Integrated Chef, No Servers Use when you already have one of them Requirement to automate... (suitable for infra engineer) Key terms to lookout for => Recipes, Cookbook or Manifests Terms Stacks - Core Component : container of resources Layers - each layer is a specific function within a stack Recipes & Cookbooks Lifecycle Events - Setup, Configure, Deploy, Undeploy and Shutdown Instances - EC2/On-premise 24/7 time-based load-based Apps","title":"AWS Opsworks"},{"location":"deployment/opsworks/#aws-opsworks","text":"AWS OpsWorks provides managed implementations of Puppet and Chef in a product which integrates with other AWS Products and services. 3 Modes Puppet Enterprise - AWS Managed Puppet Master Server desired state of the infra Chef Automate - AWS Managed Chef Servers infra as code with ruby Opsworks - AWS Integrated Chef, No Servers Use when you already have one of them Requirement to automate... (suitable for infra engineer) Key terms to lookout for => Recipes, Cookbook or Manifests","title":"AWS OpsWorks"},{"location":"deployment/opsworks/#terms","text":"Stacks - Core Component : container of resources Layers - each layer is a specific function within a stack Recipes & Cookbooks Lifecycle Events - Setup, Configure, Deploy, Undeploy and Shutdown Instances - EC2/On-premise 24/7 time-based load-based Apps","title":"Terms"},{"location":"deployment/service-catalog/","text":"what is service catalog? Document or Database created by an IT Team Organised collection of proucts Offered by the IT team Key Product Information - Product Owner, Cost, Requirements, Support Information, Dependencies Defines approval of provisioning from IT and Customer Side Managed costs and scale service delivery AWS service catalog regional service allow end users to deploy infrastructue without having actual infrastructure permission Self-service Portal for end users launch predefined (by admin) products end users permission can be controlled Admins can define those products and the permissions required to launch them Build products into portfolios","title":"AWS Service Catalog"},{"location":"deployment/service-catalog/#what-is-service-catalog","text":"Document or Database created by an IT Team Organised collection of proucts Offered by the IT team Key Product Information - Product Owner, Cost, Requirements, Support Information, Dependencies Defines approval of provisioning from IT and Customer Side Managed costs and scale service delivery","title":"what is service catalog?"},{"location":"deployment/service-catalog/#aws-service-catalog","text":"regional service allow end users to deploy infrastructue without having actual infrastructure permission Self-service Portal for end users launch predefined (by admin) products end users permission can be controlled Admins can define those products and the permissions required to launch them Build products into portfolios","title":"AWS service catalog"},{"location":"deployment/systems-manager/","text":"AWS Systems Manager Agent View and control AWS and on-premises infrastructure Agent based - installed on windows and Linux AWS AMI's Manage Inventory & Patch Assets Run commands & Managed Desired State Parameter Store .. configuration and secrets Securely connect to EC2. even in private VPCs Run Command Systems Manager Run Command is a foundational feature of Systems manager which allows for commands to be executed on managed instances at scale . AWS Systems Manager documents Run command documents on managed instances No SSH/RDP Access Required Instances , Tags or Resource Groups Command documents can be reused and can have parameters Rate Control - Concurrency & Error Threshold Output Options - S3 and SNS Target of an EventBridge rule Patch Manager Systems Manager Patch Manager allows for the patching of windows or linux managed instances running in AWS or on-premises . Concepts - Patch Baseline - Patch Groups - Maintenance Windows - Run Command - Concurrency & Error Threshold - Compliance Patch Baseline - Predefined Patch Baselines - Various OS (you can also create your own) - Linux - AWS-[ OS ]DefaultPatchBaseline, expliitly define patches - AWS- AmazonLinux2 DefaultPatchBaseline - AWS- Ubuntu DefaultPatchBaseline - Windows - AWSDefaultPatchBaseline - Critical and Security Updates - AWS-WindowsPredefinedPatchBaseline-OS - same as above - AWS-WindowsPredefinedPatchBaseline-OS-Applications - +MS App Updates Patch Manager - hybrid activations - generates activation code and id for on-premise isntances - Patch Manager will pick the baseline of patch group if an insance is registered to a patch group else it will pick the default for the instance. Missing 5 = Systems Manager inventory for checking the desired state of instances.","title":"AWS Systems Manager"},{"location":"deployment/systems-manager/#aws-systems-manager","text":"","title":"AWS Systems Manager"},{"location":"deployment/systems-manager/#agent","text":"View and control AWS and on-premises infrastructure Agent based - installed on windows and Linux AWS AMI's Manage Inventory & Patch Assets Run commands & Managed Desired State Parameter Store .. configuration and secrets Securely connect to EC2. even in private VPCs","title":"Agent"},{"location":"deployment/systems-manager/#run-command","text":"Systems Manager Run Command is a foundational feature of Systems manager which allows for commands to be executed on managed instances at scale . AWS Systems Manager documents Run command documents on managed instances No SSH/RDP Access Required Instances , Tags or Resource Groups Command documents can be reused and can have parameters Rate Control - Concurrency & Error Threshold Output Options - S3 and SNS Target of an EventBridge rule","title":"Run Command"},{"location":"deployment/systems-manager/#patch-manager","text":"Systems Manager Patch Manager allows for the patching of windows or linux managed instances running in AWS or on-premises . Concepts - Patch Baseline - Patch Groups - Maintenance Windows - Run Command - Concurrency & Error Threshold - Compliance Patch Baseline - Predefined Patch Baselines - Various OS (you can also create your own) - Linux - AWS-[ OS ]DefaultPatchBaseline, expliitly define patches - AWS- AmazonLinux2 DefaultPatchBaseline - AWS- Ubuntu DefaultPatchBaseline - Windows - AWSDefaultPatchBaseline - Critical and Security Updates - AWS-WindowsPredefinedPatchBaseline-OS - same as above - AWS-WindowsPredefinedPatchBaseline-OS-Applications - +MS App Updates Patch Manager - hybrid activations - generates activation code and id for on-premise isntances - Patch Manager will pick the baseline of patch group if an insance is registered to a patch group else it will pick the default for the instance. Missing 5 = Systems Manager inventory for checking the desired state of instances.","title":"Patch Manager"},{"location":"dr-bc/dr-bc-architecture/","text":"Overview Efffective DR/BC costs money all of the time Need extra resource (backup or standby resources) Executing a DR/BC process takes time Cost Trade Off: Faster to Recover - more costilier Backup and Restore Have backups in place -- in an event of disaster restore them in different site/location Cost effective but will takes longer to restore in case disaster happens Pilot Light Secondary environment with bare minimum set up for faster recovery auto sync data for Databases which are ready to use provisioned but stopped instances Warm Standby Standby secondary site running the fully functional version of primary infra but with less resources and size. cheaper instance type lower no. of intances in ASG can be upgraded/updated when failover is required Little costlier than pilot light ( cheaper than active/active ) but faster to recover in case of disaster Active/Active Have exact and fully functional version of primary infrastructure running in secondary site/location The most costliest ( 200% ) set-up but does not require recovery during disaster event Since both sites run fully functional apps, they can be load balanced as well provides better performance and improves HA","title":"DR Architecture"},{"location":"dr-bc/dr-bc-architecture/#overview","text":"Efffective DR/BC costs money all of the time Need extra resource (backup or standby resources) Executing a DR/BC process takes time Cost Trade Off: Faster to Recover - more costilier","title":"Overview"},{"location":"dr-bc/dr-bc-architecture/#backup-and-restore","text":"Have backups in place -- in an event of disaster restore them in different site/location Cost effective but will takes longer to restore in case disaster happens","title":"Backup and Restore"},{"location":"dr-bc/dr-bc-architecture/#pilot-light","text":"Secondary environment with bare minimum set up for faster recovery auto sync data for Databases which are ready to use provisioned but stopped instances","title":"Pilot Light"},{"location":"dr-bc/dr-bc-architecture/#warm-standby","text":"Standby secondary site running the fully functional version of primary infra but with less resources and size. cheaper instance type lower no. of intances in ASG can be upgraded/updated when failover is required Little costlier than pilot light ( cheaper than active/active ) but faster to recover in case of disaster","title":"Warm Standby"},{"location":"dr-bc/dr-bc-architecture/#activeactive","text":"Have exact and fully functional version of primary infrastructure running in secondary site/location The most costliest ( 200% ) set-up but does not require recovery during disaster event Since both sites run fully functional apps, they can be load balanced as well provides better performance and improves HA","title":"Active/Active"},{"location":"dr-bc/dr-compute/","text":"Overview NO truly global compute services in AWS EC2 Always assume it can be shutdown or replaced EC2 host fails in one AZ reattach EBS to new instance Run EC2 instances with ASG in multiple AZs ASG will launch EC2 instances in case of instance failures ECS EC2 mode or Fargate mode EC2 mode underlying ec2 instances fails -> containers fail underlying ec2 can be run in ASG with multiple AZ to tolerate AZ failure Fargate mode underlying ec2s managed by AWS fails -> containers fail spread the containers across multiple AZ with configuration Lambda Stateless VPC lambda function ENIs per subnet in the VPC to launch in any subnets Public lambda function can be launched from any AZs Lambda Service is impated when there is a region failure","title":"DR Compute"},{"location":"dr-bc/dr-compute/#overview","text":"NO truly global compute services in AWS","title":"Overview"},{"location":"dr-bc/dr-compute/#ec2","text":"Always assume it can be shutdown or replaced EC2 host fails in one AZ reattach EBS to new instance Run EC2 instances with ASG in multiple AZs ASG will launch EC2 instances in case of instance failures","title":"EC2"},{"location":"dr-bc/dr-compute/#ecs","text":"EC2 mode or Fargate mode EC2 mode underlying ec2 instances fails -> containers fail underlying ec2 can be run in ASG with multiple AZ to tolerate AZ failure Fargate mode underlying ec2s managed by AWS fails -> containers fail spread the containers across multiple AZ with configuration","title":"ECS"},{"location":"dr-bc/dr-compute/#lambda","text":"Stateless VPC lambda function ENIs per subnet in the VPC to launch in any subnets Public lambda function can be launched from any AZs Lambda Service is impated when there is a region failure","title":"Lambda"},{"location":"dr-bc/dr-database/","text":"Regional Overview DB running in EC2 Running DB in EC2 instances will not provide HA or resiliency EC2 and EBS - single AZ product by themselves RDS Runs in subnet group which spans over mutliple AZs Primary instance with standby instance in different AZ Synchcronous replication between primary and standby instances Failover happens when primary instance goes down and standy becomes primary Both primary and standby has to be down from multi-AZ failure to cause complete failure in the region Aurora Can handle multi AZ failures Volume(virtual database storage) runs in cluster mode which spans multiple AZ Both primary (also write) and replicas read from same cluster volume upto 15 read-only replicas failover process is automated if primary instance goes down Only region failover would significantly impact service DynamodDB Replcations between multiple nodes in mutliple AZs Fails when the region goes down Global Overview DynamoDB Global Tables Default dynamoDB table located in single region Global Table provides multi master replications between regional replicas last write wins replication method Same region write - strongly consistent read | Alternate region - eventual consistency read Aurora Global Databases Read/Write cluster in one region and Read cluster in different regions read cluster can be promoted to read/write cluster Storage level replication - 1s lag NON multi master RDS cross-region replication Cross Region Read Replica Asynchronous Replication - lag could be longer compared to aurora","title":"DR Database"},{"location":"dr-bc/dr-database/#regional","text":"","title":"Regional"},{"location":"dr-bc/dr-database/#overview","text":"","title":"Overview"},{"location":"dr-bc/dr-database/#db-running-in-ec2","text":"Running DB in EC2 instances will not provide HA or resiliency EC2 and EBS - single AZ product by themselves","title":"DB running in EC2"},{"location":"dr-bc/dr-database/#rds","text":"Runs in subnet group which spans over mutliple AZs Primary instance with standby instance in different AZ Synchcronous replication between primary and standby instances Failover happens when primary instance goes down and standy becomes primary Both primary and standby has to be down from multi-AZ failure to cause complete failure in the region","title":"RDS"},{"location":"dr-bc/dr-database/#aurora","text":"Can handle multi AZ failures Volume(virtual database storage) runs in cluster mode which spans multiple AZ Both primary (also write) and replicas read from same cluster volume upto 15 read-only replicas failover process is automated if primary instance goes down Only region failover would significantly impact service","title":"Aurora"},{"location":"dr-bc/dr-database/#dynamoddb","text":"Replcations between multiple nodes in mutliple AZs Fails when the region goes down","title":"DynamodDB"},{"location":"dr-bc/dr-database/#global","text":"","title":"Global"},{"location":"dr-bc/dr-database/#overview_1","text":"","title":"Overview"},{"location":"dr-bc/dr-database/#dynamodb-global-tables","text":"Default dynamoDB table located in single region Global Table provides multi master replications between regional replicas last write wins replication method Same region write - strongly consistent read | Alternate region - eventual consistency read","title":"DynamoDB Global Tables"},{"location":"dr-bc/dr-database/#aurora-global-databases","text":"Read/Write cluster in one region and Read cluster in different regions read cluster can be promoted to read/write cluster Storage level replication - 1s lag NON multi master","title":"Aurora Global Databases"},{"location":"dr-bc/dr-database/#rds-cross-region-replication","text":"Cross Region Read Replica Asynchronous Replication - lag could be longer compared to aurora","title":"RDS cross-region replication"},{"location":"dr-bc/dr-networking/","text":"Regional Public services like SNS and S3 operate from all AZs - only region failure would cause service failure VPC VPC - regionallly resilient along with IGW and VPC Router can withstand AZ failure only fails when region fails Subnet - AZ failure will also cause subnet failure nat gateway fails when AZ fails Interface Endpoint - AZ failure will also cause interface endpoint failure deploy multiple VPCE for regional HA Load Balancer Consists of LB nodes in 2+ AZs - regionally resilient as long as one node is functional, LB is functional Can withstand AZ failure only fails when entire region fails Global Network Route53 Global Service Route53 to point to services in mulitple regions provide global discoverability and resilience configuration to not route to unhealhty region (regional health check)","title":"DR Networking"},{"location":"dr-bc/dr-networking/#regional","text":"Public services like SNS and S3 operate from all AZs - only region failure would cause service failure","title":"Regional"},{"location":"dr-bc/dr-networking/#vpc","text":"VPC - regionallly resilient along with IGW and VPC Router can withstand AZ failure only fails when region fails Subnet - AZ failure will also cause subnet failure nat gateway fails when AZ fails Interface Endpoint - AZ failure will also cause interface endpoint failure deploy multiple VPCE for regional HA","title":"VPC"},{"location":"dr-bc/dr-networking/#load-balancer","text":"Consists of LB nodes in 2+ AZs - regionally resilient as long as one node is functional, LB is functional Can withstand AZ failure only fails when entire region fails","title":"Load Balancer"},{"location":"dr-bc/dr-networking/#global-network","text":"","title":"Global Network"},{"location":"dr-bc/dr-networking/#route53","text":"Global Service Route53 to point to services in mulitple regions provide global discoverability and resilience configuration to not route to unhealhty region (regional health check)","title":"Route53"},{"location":"dr-bc/dr-storage/","text":"Overview Instance Store Volumes Attached with EC2 and no recovery in case of failure - temporary solution Fails when .. hardware fails AZ fails EC2 host fails EBS - Elastic Block Storage Runs in one AZ No replication of data from one AZ to another Fails when .. AZ fails Snapshots of EBS volume can be stored in S3 but those snapshots can be used directly restore by creating new volume in different AZ if required S3 Replication runs in 3 AZs One Zone storage class does not replicate beyond one AZ but cheaper Snapshots stored in s3 gets regional resilience Snapshots shared and copy across multiple S3 buckets across multiple regions Cross-Region Replication EFS - Elastic File System Replicated accross multiple AZs mounted to Linux ec2 Fails when.. Region fails","title":"DR Storage"},{"location":"dr-bc/dr-storage/#overview","text":"","title":"Overview"},{"location":"dr-bc/dr-storage/#instance-store-volumes","text":"Attached with EC2 and no recovery in case of failure - temporary solution Fails when .. hardware fails AZ fails EC2 host fails","title":"Instance Store Volumes"},{"location":"dr-bc/dr-storage/#ebs-elastic-block-storage","text":"Runs in one AZ No replication of data from one AZ to another Fails when .. AZ fails Snapshots of EBS volume can be stored in S3 but those snapshots can be used directly restore by creating new volume in different AZ if required","title":"EBS - Elastic Block Storage"},{"location":"dr-bc/dr-storage/#s3","text":"Replication runs in 3 AZs One Zone storage class does not replicate beyond one AZ but cheaper Snapshots stored in s3 gets regional resilience Snapshots shared and copy across multiple S3 buckets across multiple regions Cross-Region Replication","title":"S3"},{"location":"dr-bc/dr-storage/#efs-elastic-file-system","text":"Replicated accross multiple AZs mounted to Linux ec2 Fails when.. Region fails","title":"EFS - Elastic File System"},{"location":"migration/datasync/","text":"AWS DataSync Data Transfer service TO and FROM AWS Use cases Data Migration Achiving cold data Data Protection Data movement for timely in-cloud processing DR/BC - Disaster Recovery/Business continuity ..designed to work at huge scale ( agent ) Retains metadata (e.g. permissions/timestamps ) Built in data validation Key Features Scalable - 10GBps per agent (~100TB per day) Bandwidth Limiters (avoid link saturations) Incremental and scheduled transfer options Compression and encyrption Automatic recovery from transit errors AWS Service Integration - S3, EFS, FSx Support transfering data between one AWS service to another Pay as you use.. per GB cost for data moved Components Task - A job within DataSync, defines what is being synced, how quickly, FROM where TO where Agent - Software to READ and WRITE to on-premise data stores using NFS or SMB Location - every task has two locations FROM and TO. e.g. - NFS, SMB (Server Message Block), EFS, FSx and S3 M","title":"AWS Datasync"},{"location":"migration/datasync/#aws-datasync","text":"Data Transfer service TO and FROM AWS Use cases Data Migration Achiving cold data Data Protection Data movement for timely in-cloud processing DR/BC - Disaster Recovery/Business continuity ..designed to work at huge scale ( agent ) Retains metadata (e.g. permissions/timestamps ) Built in data validation","title":"AWS DataSync"},{"location":"migration/datasync/#key-features","text":"Scalable - 10GBps per agent (~100TB per day) Bandwidth Limiters (avoid link saturations) Incremental and scheduled transfer options Compression and encyrption Automatic recovery from transit errors AWS Service Integration - S3, EFS, FSx Support transfering data between one AWS service to another Pay as you use.. per GB cost for data moved","title":"Key Features"},{"location":"migration/datasync/#components","text":"Task - A job within DataSync, defines what is being synced, how quickly, FROM where TO where Agent - Software to READ and WRITE to on-premise data stores using NFS or SMB Location - every task has two locations FROM and TO. e.g. - NFS, SMB (Server Message Block), EFS, FSx and S3 M","title":"Components"},{"location":"migration/migration-database/","text":"Database Migration Service (DMS) AWS Database Migration Service (AWS DMS) is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases , and other types of data stores. You can use AWS DMS to migrate your data into the AWS Cloud or between combinations of cloud and on-premises setups. Managed Database Migration Service Runs using a replication instance Source and Destination Endpoints point at .. Source and Target Databases One Endpoint must be on AWS Migraiton Types: Full Load - one off migration of all data Full Load + CDC (change data capture) - also captures ongoing changes in the source DB CDC Only - copy existing data using other native tooling and only apply data changes Does not support schema conversion but ... tool available called Schema Conversion Tool(SCT) assist with Schema Conversion Schema Conversion Tool (SCT) SCT is used when converting one database engine to another .. including DB -> s3 on-premises MSSQL -> RDS MySQL on-premises ORACLE -> Aurora SCT is not used when migrating between DB's of the same type .. On-premise MySQL -> RDS MySQL Works with database types like OLTP DB Types - MySQL, Oracle, MSSQL OLAP - Teradata, Oracle, Vertica, Greenplum DMS & Snowball Larger migrations might be multi-TB in size moving data over takes time and consumes capacity DMS can utilise snowball.. Steps : 1. Use SCT to extract data locally and move to a snowball device (generic fie format) 2. Ship the device back to AWS.They load onto an S3 bucket. 3. DMS migrates from S3 into the target store 4. CDC can capture data changes -> s3 (intermediary) -> write to target DB","title":"Database Migration"},{"location":"migration/migration-database/#database-migration-service-dms","text":"AWS Database Migration Service (AWS DMS) is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases , and other types of data stores. You can use AWS DMS to migrate your data into the AWS Cloud or between combinations of cloud and on-premises setups. Managed Database Migration Service Runs using a replication instance Source and Destination Endpoints point at .. Source and Target Databases One Endpoint must be on AWS Migraiton Types: Full Load - one off migration of all data Full Load + CDC (change data capture) - also captures ongoing changes in the source DB CDC Only - copy existing data using other native tooling and only apply data changes Does not support schema conversion but ... tool available called Schema Conversion Tool(SCT) assist with Schema Conversion","title":"Database Migration Service (DMS)"},{"location":"migration/migration-database/#schema-conversion-tool-sct","text":"SCT is used when converting one database engine to another .. including DB -> s3 on-premises MSSQL -> RDS MySQL on-premises ORACLE -> Aurora SCT is not used when migrating between DB's of the same type .. On-premise MySQL -> RDS MySQL Works with database types like OLTP DB Types - MySQL, Oracle, MSSQL OLAP - Teradata, Oracle, Vertica, Greenplum","title":"Schema Conversion Tool (SCT)"},{"location":"migration/migration-database/#dms-snowball","text":"Larger migrations might be multi-TB in size moving data over takes time and consumes capacity DMS can utilise snowball.. Steps : 1. Use SCT to extract data locally and move to a snowball device (generic fie format) 2. Ship the device back to AWS.They load onto an S3 bucket. 3. DMS migrates from S3 into the target store 4. CDC can capture data changes -> s3 (intermediary) -> write to target DB","title":"DMS &amp; Snowball"},{"location":"migration/migration-virtual-machines/","text":"Applicaiton Discovery Service Discover on-premises infrastructure vms, cpu, memory including utilization Different modes: Agentless (Application Discovery Agentless Connector) ...OVA appliance, integrates with VMWARE - inventory, vm usage measure performance and resource usage - limited to metrics that has been exposed externally Agent Based discovery for in VM data gathering. network, processes, usage, performance more detailed metrics and informations .. dependancies between servers (network activity) Integration with AWS Migraiton Hub and Amazon Athena hub - tracks migration of different types within AWS athena - perform adhoc analysis of data generation Server Migration Service (SMS) AWS Server Migration Service (SMS) is an agentless service which makes it easier and faster for you to migrate thousands of on-premises workloads to AWS. AWS SMS allows you to automate, schedule, and track incremental replications of live server volumes , making it easier for you to coordinate large-scale server migrations. - Migrate whole VM's (OS, Data, Apps .. migrated as is) - Runs on Agentless mode - connector runs as VM on-premise - Integrates with VMWare, Hyper-V and AzureVM only - Import/Export (anothe AWS service) - downtime - Incremental replication of live volumes - Orchestrate multi-server migrations - Creates AMIs which can be used to create EC2 instances - Integrates with AWS Migration Hub","title":"Virtual Machinne Migration"},{"location":"migration/migration-virtual-machines/#applicaiton-discovery-service","text":"Discover on-premises infrastructure vms, cpu, memory including utilization Different modes: Agentless (Application Discovery Agentless Connector) ...OVA appliance, integrates with VMWARE - inventory, vm usage measure performance and resource usage - limited to metrics that has been exposed externally Agent Based discovery for in VM data gathering. network, processes, usage, performance more detailed metrics and informations .. dependancies between servers (network activity) Integration with AWS Migraiton Hub and Amazon Athena hub - tracks migration of different types within AWS athena - perform adhoc analysis of data generation","title":"Applicaiton Discovery Service"},{"location":"migration/migration-virtual-machines/#server-migration-service-sms","text":"AWS Server Migration Service (SMS) is an agentless service which makes it easier and faster for you to migrate thousands of on-premises workloads to AWS. AWS SMS allows you to automate, schedule, and track incremental replications of live server volumes , making it easier for you to coordinate large-scale server migrations. - Migrate whole VM's (OS, Data, Apps .. migrated as is) - Runs on Agentless mode - connector runs as VM on-premise - Integrates with VMWare, Hyper-V and AzureVM only - Import/Export (anothe AWS service) - downtime - Incremental replication of live volumes - Orchestrate multi-server migrations - Creates AMIs which can be used to create EC2 instances - Integrates with AWS Migration Hub","title":"Server Migration Service (SMS)"},{"location":"migration/migration/","text":"6 R's of Cloud Migration https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/ Rehosting - Lift and Shift Migrate as it is (App Virtual Machine - EC2, DB => EC2) +Reduced Admin Overhead (IAAS) +Potentially easier to optmise when in AWS Cost savings -- burst instances not taking full advantage of cloud kicking the can down the road VM Import/Export & Server Migration Service Replatforming - Lift and Shift with Optimization like rehosting but with Optimization using RDS instead of self-managed DBs using ELB instead of load balancers using S3 as backup or media storage no real negatives & no world-changing benefits Admin Overhead Reductions, Performance Benefits, more effective backups or Improved HA/FT - high availability & fault tolreant Repurchasing - Move to something new.. SAAS when there is no reason to self-manage USE a XaaS Product Example MS Exchange => Microsoft 365 CRM => Salesforce Payroll => Xero Refactoring/ Re-architecting - Take advantage of cloud reviewing the architecture of an application adopting cloud-native architectures & products Service-Orientated or Microservices APIs, Event-Driven or Serverless Initially very expensive & time consuming best long-term benefits often cheaper much more scalable better HA/FT costs aligned with app usage Retire - Do we even need this ? NO? Dump it Systems are often running for no reason Auditing their usage is often more work than leaving running If you dont need the application --> switch if off Retain - Not worth time/money or is too scary to migrate DO NOTHING Old Application - not worth the move Complex application - leave till later Super critical application - risky Complete the migration - swing back to focus on the left-overs","title":"6R migration"},{"location":"migration/migration/#6-rs-of-cloud-migration","text":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 R's of Cloud Migration"},{"location":"migration/migration/#rehosting-lift-and-shift","text":"Migrate as it is (App Virtual Machine - EC2, DB => EC2) +Reduced Admin Overhead (IAAS) +Potentially easier to optmise when in AWS Cost savings -- burst instances not taking full advantage of cloud kicking the can down the road VM Import/Export & Server Migration Service","title":"Rehosting - Lift and Shift"},{"location":"migration/migration/#replatforming-lift-and-shift-with-optimization","text":"like rehosting but with Optimization using RDS instead of self-managed DBs using ELB instead of load balancers using S3 as backup or media storage no real negatives & no world-changing benefits Admin Overhead Reductions, Performance Benefits, more effective backups or Improved HA/FT - high availability & fault tolreant","title":"Replatforming - Lift and Shift with Optimization"},{"location":"migration/migration/#repurchasing-move-to-something-new-saas","text":"when there is no reason to self-manage USE a XaaS Product Example MS Exchange => Microsoft 365 CRM => Salesforce Payroll => Xero","title":"Repurchasing - Move to something new.. SAAS"},{"location":"migration/migration/#refactoring-re-architecting-take-advantage-of-cloud","text":"reviewing the architecture of an application adopting cloud-native architectures & products Service-Orientated or Microservices APIs, Event-Driven or Serverless Initially very expensive & time consuming best long-term benefits often cheaper much more scalable better HA/FT costs aligned with app usage","title":"Refactoring/ Re-architecting - Take advantage of cloud"},{"location":"migration/migration/#retire-do-we-even-need-this-no-dump-it","text":"Systems are often running for no reason Auditing their usage is often more work than leaving running If you dont need the application --> switch if off","title":"Retire - Do we even need this ? NO? Dump it"},{"location":"migration/migration/#retain-not-worth-timemoney-or-is-too-scary-to-migrate","text":"DO NOTHING Old Application - not worth the move Complex application - leave till later Super critical application - risky Complete the migration - swing back to focus on the left-overs","title":"Retain - Not worth time/money or is too scary to migrate"},{"location":"migration/snowball/","text":"AWS Snowball Move large amounts of data IN and OUT of AWS expensive to transfer via netowrk too much data to be transmitted via network Physical storage => suitcase or truck Snowball Ordered from AWS - log a job - device delevered (not instant) Data Encryption uses KMS - at rest 50TB or 80TB 1 GBps (RJ45 1GBas-TX) or 10 GBps (LR/SR) Network 10 TB <--> 10PB economical range ( multiple devices ) Multiple devices to multiple locations ONLY STORAGE DEVICE Snowball Edge Both compute and storage Larger Capacity vs snowball 10 GBps, 10/25 (SFP), 45/50/100 (QSFP+) Storage Optimized - 80TB, 24 vCPU, 32 GiB RAM withEC2 - 1TB SSD Compute Optimized - 100TB + NVME, 52vCPU and 208 GiB RAM Compute Optimized - As above ... with a GPU Ideal for remote sites or where data processing on ingestion is needed Snowmobile Portable DC wihtin a shipping container on a truck Special Order Ideal for single location when 10PB+ is required Up to 100PB per snowmobile Not economical for multi-site (unless huge) or sub 10PB","title":"AWS Snowball"},{"location":"migration/snowball/#aws-snowball","text":"Move large amounts of data IN and OUT of AWS expensive to transfer via netowrk too much data to be transmitted via network Physical storage => suitcase or truck","title":"AWS Snowball"},{"location":"migration/snowball/#snowball","text":"Ordered from AWS - log a job - device delevered (not instant) Data Encryption uses KMS - at rest 50TB or 80TB 1 GBps (RJ45 1GBas-TX) or 10 GBps (LR/SR) Network 10 TB <--> 10PB economical range ( multiple devices ) Multiple devices to multiple locations ONLY STORAGE DEVICE","title":"Snowball"},{"location":"migration/snowball/#snowball-edge","text":"Both compute and storage Larger Capacity vs snowball 10 GBps, 10/25 (SFP), 45/50/100 (QSFP+) Storage Optimized - 80TB, 24 vCPU, 32 GiB RAM withEC2 - 1TB SSD Compute Optimized - 100TB + NVME, 52vCPU and 208 GiB RAM Compute Optimized - As above ... with a GPU Ideal for remote sites or where data processing on ingestion is needed","title":"Snowball Edge"},{"location":"migration/snowball/#snowmobile","text":"Portable DC wihtin a shipping container on a truck Special Order Ideal for single location when 10PB+ is required Up to 100PB per snowmobile Not economical for multi-site (unless huge) or sub 10PB","title":"Snowmobile"},{"location":"migration/storage-gateway/","text":"Storage Gateway FAQS - VM or the hardware appliance in on-premise - Supports protocols such as iSCSI, SMB, and NFS - Use Cases 1. move backups and archives to the cloud 2. reduce on-premises storage with cloud-backed file shares 3. provide on-premises applications low latency access to data stored in AWS 4. data lake access for pre and post processing workflows. Volume Gateway Volume Gateway provides an iSCSI target , which enables you to create block storage volumes and mount them as iSCSI devices from your on-premises or EC2 application servers. The Volume Gateway runs in either a cached or stored mode. In the cached mode , your primary data is written to S3, while retaining your frequently accessed data locally in a cache for low-latency access. In the stored mode , your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS. Tape Gateway - VTL(virtual tape library) Mode File Gateway Bridges on-premises file storage and S3 Mount Points (shares) available via NFS and SMB Map directly onto an S3 bucket Files stored into a mount point , are visible as objects in a S3 bucket READ and WRITE Caching ensure LAN-like performance The Amazon S3 File Gateway enables you to store and retrieve objects in Amazon S3 using file protocols such as Network File System (NFS) and Server Message Block (SMB). Objects written through S3 File Gateway can be directly accessed in S3. The Amazon FSx File Gateway enables you to store and retrieve files in Amazon FSx for Windows File Server using the SMB protocol. Files written through Amazon FSx File Gateway are directly accessible in Amazon FSx for Windows File Server.","title":"Storage Gateway"},{"location":"migration/storage-gateway/#storage-gateway","text":"FAQS - VM or the hardware appliance in on-premise - Supports protocols such as iSCSI, SMB, and NFS - Use Cases 1. move backups and archives to the cloud 2. reduce on-premises storage with cloud-backed file shares 3. provide on-premises applications low latency access to data stored in AWS 4. data lake access for pre and post processing workflows.","title":"Storage Gateway"},{"location":"migration/storage-gateway/#volume-gateway","text":"Volume Gateway provides an iSCSI target , which enables you to create block storage volumes and mount them as iSCSI devices from your on-premises or EC2 application servers. The Volume Gateway runs in either a cached or stored mode. In the cached mode , your primary data is written to S3, while retaining your frequently accessed data locally in a cache for low-latency access. In the stored mode , your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS.","title":"Volume Gateway"},{"location":"migration/storage-gateway/#tape-gateway-vtlvirtual-tape-library-mode","text":"","title":"Tape Gateway - VTL(virtual tape library) Mode"},{"location":"migration/storage-gateway/#file-gateway","text":"Bridges on-premises file storage and S3 Mount Points (shares) available via NFS and SMB Map directly onto an S3 bucket Files stored into a mount point , are visible as objects in a S3 bucket READ and WRITE Caching ensure LAN-like performance The Amazon S3 File Gateway enables you to store and retrieve objects in Amazon S3 using file protocols such as Network File System (NFS) and Server Message Block (SMB). Objects written through S3 File Gateway can be directly accessed in S3. The Amazon FSx File Gateway enables you to store and retrieve files in Amazon FSx for Windows File Server using the SMB protocol. Files written through Amazon FSx File Gateway are directly accessible in Amazon FSx for Windows File Server.","title":"File Gateway"},{"location":"miscellaneous/device-farm/","text":"Overview Provides managed Web and Mobile Application testing Test on a fleet of real browsers and devices Real devices Phones, tablets, different languages, sizes and operatings systems Use built in or supported automated testing framework Receive reports detailing testing output","title":"AWS Device Farm"},{"location":"miscellaneous/device-farm/#overview","text":"Provides managed Web and Mobile Application testing Test on a fleet of real browsers and devices Real devices Phones, tablets, different languages, sizes and operatings systems Use built in or supported automated testing framework Receive reports detailing testing output","title":"Overview"},{"location":"miscellaneous/glue/","text":"Overview Serverless ETL (Extract, Transform & Load) service different than datapipeline (which can do ETL) and uses servers ( EMR ) Moves and Transforms data between source and destination use glue job to enrich data in serverless way Crawls data sources and generates the AWS Glue Data Catalog Data Source:Store - S3, RDS, JDBC compatible & DynamoDB Data Source:Streams - Kinesis Data Stream & Apache Kafka Data Targets - S3, RDS, JDBC Databases AWS Glue - Data Catalog Persistent metadata about data sources within a region One catalog per region per account Avoids data silos.. Amazon Athena, Redshift Spectrum, EMR & AWS Lake Formation all use Data Catalog configure crawlers for data sources Either AWS Glue or datapipeline","title":"AWS Glue"},{"location":"miscellaneous/glue/#overview","text":"Serverless ETL (Extract, Transform & Load) service different than datapipeline (which can do ETL) and uses servers ( EMR ) Moves and Transforms data between source and destination use glue job to enrich data in serverless way Crawls data sources and generates the AWS Glue Data Catalog Data Source:Store - S3, RDS, JDBC compatible & DynamoDB Data Source:Streams - Kinesis Data Stream & Apache Kafka Data Targets - S3, RDS, JDBC Databases","title":"Overview"},{"location":"miscellaneous/glue/#aws-glue-data-catalog","text":"Persistent metadata about data sources within a region One catalog per region per account Avoids data silos.. Amazon Athena, Redshift Spectrum, EMR & AWS Lake Formation all use Data Catalog configure crawlers for data sources Either AWS Glue or datapipeline","title":"AWS Glue - Data Catalog"},{"location":"miscellaneous/kinesis-video-streams/","text":"Overview Ingest ingest live video from producers Devices like security cameras, smartphones, cars, drones time-serialised audio, thermal and RADAR data Once data is ingested to AWS, consumers can access the data frame by frame .. or as needed Can persist and encrypt (both in rest and transit) cannot ingest data directly.. only via API Integrates with other AWS services like Rekognition and Connect","title":"Kines Video Streams"},{"location":"miscellaneous/kinesis-video-streams/#overview","text":"Ingest ingest live video from producers Devices like security cameras, smartphones, cars, drones time-serialised audio, thermal and RADAR data Once data is ingested to AWS, consumers can access the data frame by frame .. or as needed Can persist and encrypt (both in rest and transit) cannot ingest data directly.. only via API Integrates with other AWS services like Rekognition and Connect","title":"Overview"},{"location":"miscellaneous/lex-connect/","text":"Amazon Lex Text or Voice conversational interfaces Powers the Alexa service Automatic speech recognition (ASR) - speech to text Natural language Understanding (NLU) - Intent Build understanding of voice and text into your application Benefits Scales with load Integrates with AWS services like lambda Quick to Deploy Pay as you go model Apps - chatbots, Voice Assistants, Q&A Bots, Info/Enterprise Bots Amazon Connect Contact Center ... as a service Omnichannel - voice and chat, incoming & outgoing Integrates with PSTN networks for traditional voice Agents can connect using the internet from anywhere Integrates with other AWS services like Lambda/LEX for additional intelligence and features Quick to provision, pay as you go use pricing and scalable","title":"AWS Lex & Connect"},{"location":"miscellaneous/lex-connect/#amazon-lex","text":"Text or Voice conversational interfaces Powers the Alexa service Automatic speech recognition (ASR) - speech to text Natural language Understanding (NLU) - Intent Build understanding of voice and text into your application Benefits Scales with load Integrates with AWS services like lambda Quick to Deploy Pay as you go model Apps - chatbots, Voice Assistants, Q&A Bots, Info/Enterprise Bots","title":"Amazon Lex"},{"location":"miscellaneous/lex-connect/#amazon-connect","text":"Contact Center ... as a service Omnichannel - voice and chat, incoming & outgoing Integrates with PSTN networks for traditional voice Agents can connect using the internet from anywhere Integrates with other AWS services like Lambda/LEX for additional intelligence and features Quick to provision, pay as you go use pricing and scalable","title":"Amazon Connect"},{"location":"miscellaneous/rekognition/","text":"Overview Deep Learning Image and Video analysis Identify objects, people, text, ativities, content moderation Face detection, Face analysis, face comparision, pathing Pay per Image or Pay per minute Integrates with application via API Event Driven - invoke when image is uplaoded to S3 Can analyse live video streams - kinesis video stream","title":"AWS Rekognition"},{"location":"miscellaneous/rekognition/#overview","text":"Deep Learning Image and Video analysis Identify objects, people, text, ativities, content moderation Face detection, Face analysis, face comparision, pathing Pay per Image or Pay per minute Integrates with application via API Event Driven - invoke when image is uplaoded to S3 Can analyse live video streams - kinesis video stream","title":"Overview"},{"location":"monitoring/cloudtrail/","text":"CloudTrail is a product which logs API calls and account events. CloudTrail Essentails Log API calls/activities as a CloudTrail Event 90 days stored by default in Event History enabled by default - no cost from AWS Custmize by creatign 1 or more trails Management events - management operations (control plane) - ec2 launch, terminate Data Events - resouces event (object upload in s3) - extra cost - not by default By default , only management events are looged It can be configured to store data in S3 or Cloudwatch logs Global services logged their events in us-east-1 Trail can be one region or all regions NOT REALTIME - there is a delay","title":"CloudTrail"},{"location":"monitoring/cloudtrail/#cloudtrail-essentails","text":"Log API calls/activities as a CloudTrail Event 90 days stored by default in Event History enabled by default - no cost from AWS Custmize by creatign 1 or more trails Management events - management operations (control plane) - ec2 launch, terminate Data Events - resouces event (object upload in s3) - extra cost - not by default By default , only management events are looged It can be configured to store data in S3 or Cloudwatch logs Global services logged their events in us-east-1 Trail can be one region or all regions NOT REALTIME - there is a delay","title":"CloudTrail Essentails"},{"location":"monitoring/cloudwatch/","text":"Cloudwatch Metrics Ingestion, Storage and Management of Metrics Public service AWS service integration Agent for internal metrics On-premise and application integration via API/Agent Cloudwatch ALARM can be created off metrics and alram notification can be sent to multiple AWS resources like SNS, Autoscaling policy and EventBridge threshold - OK or ALARM DATA Namespace = metrics name ( CPUUtilization ) => dimensions ( name=InstanceId, value=i-instanceid ) Aggregate data using dimensions Datapoint = timestamp with value (optional) Resolution = Standard(60s granularity) - 1s granularity for additional charge Statistic - Min, Max, Sum, Average Percentile - e.g. p95 & p97.5 Cloudwatch Logs CloudWatch logs is a product which can store, manage and provide access to logging data for on-premises and AWS environments including systems and applications Public Service - store, monitor and access log data AWS, On-Premises, IOT or any application CWAgent - system or custom application logging VPC Flow Logs Cloudtrail .. Account events and AWS API Calls Elastic Beanstalk, ECS container logs, API GW and Lambda execution logs Route53 - Log DNS Requests Global service logs to us-east-1 Log Groups by default retain indefinitely encryption permissions via policy metric filter in log events ==> generates metric metric can be used to create and trigger alarm S3 Export - Create-Export-Taks (not real time) Log Stream - subgroupping of log group for instances or lambda execution LogEvent - Timestamp Raw Message Subscriptions - Realtime per log group - Subscription Filter -> destination ARN - Kinesis Data Firehose - near realtime to S3 - Elastic Search - real time - Lambda Function - relatime - Kinesis Data Stream - realtime Clouwatch logs Aggregation Multiple log groups with subscription filters ==> Kinesis data Stream ==> Kinesis Data Firehose ==> S3","title":"Cloudwatch"},{"location":"monitoring/cloudwatch/#cloudwatch-metrics","text":"Ingestion, Storage and Management of Metrics Public service AWS service integration Agent for internal metrics On-premise and application integration via API/Agent Cloudwatch ALARM can be created off metrics and alram notification can be sent to multiple AWS resources like SNS, Autoscaling policy and EventBridge threshold - OK or ALARM DATA Namespace = metrics name ( CPUUtilization ) => dimensions ( name=InstanceId, value=i-instanceid ) Aggregate data using dimensions Datapoint = timestamp with value (optional) Resolution = Standard(60s granularity) - 1s granularity for additional charge Statistic - Min, Max, Sum, Average Percentile - e.g. p95 & p97.5","title":"Cloudwatch Metrics"},{"location":"monitoring/cloudwatch/#cloudwatch-logs","text":"CloudWatch logs is a product which can store, manage and provide access to logging data for on-premises and AWS environments including systems and applications Public Service - store, monitor and access log data AWS, On-Premises, IOT or any application CWAgent - system or custom application logging VPC Flow Logs Cloudtrail .. Account events and AWS API Calls Elastic Beanstalk, ECS container logs, API GW and Lambda execution logs Route53 - Log DNS Requests Global service logs to us-east-1 Log Groups by default retain indefinitely encryption permissions via policy metric filter in log events ==> generates metric metric can be used to create and trigger alarm S3 Export - Create-Export-Taks (not real time) Log Stream - subgroupping of log group for instances or lambda execution LogEvent - Timestamp Raw Message Subscriptions - Realtime per log group - Subscription Filter -> destination ARN - Kinesis Data Firehose - near realtime to S3 - Elastic Search - real time - Lambda Function - relatime - Kinesis Data Stream - realtime Clouwatch logs Aggregation Multiple log groups with subscription filters ==> Kinesis data Stream ==> Kinesis Data Firehose ==> S3","title":"Cloudwatch Logs"},{"location":"monitoring/cost-allocation-tags/","text":"Overview","title":"Cost Allocation Tags"},{"location":"monitoring/cost-allocation-tags/#overview","text":"","title":"Overview"},{"location":"monitoring/trusted-advisor/","text":"Account level - no agents to install, it just works Basic Features Cos Optimization Performance Security Fault Tolerance Service Limits 7 core checks iwht basic and devloper support plans Anything beyond requires Business or Enterprise 7 Core Checks S3 Buckets - Open Permissions Security Groups - 0.0.0.0/0 for ports IAM Use - at least one IAM user MFA on Root Account EBS Public Snapshots RDS Public Snapshots 50 Service limit checks Business and Enterprise Support - addition to core checks 115 further checks Access via the AWS support API Allow programatic capability CloudWatch Integration","title":"Trusted Advisor"},{"location":"monitoring/x-ray/","text":"Overview Tracing Header - trace ID is generated and tracked through out the distributed application for a given request Segments - Data blocks --> ip/host, response, work done(times), issues Subsegments - more granular vesion of segments including calls to other services as a part of the segment (endpoints etc) Service Graph - JSON Document detailing services and resources which make up your application Service Map - Visual version of the service graph showing traces Integration","title":"AWS X-RAY"},{"location":"monitoring/x-ray/#overview","text":"Tracing Header - trace ID is generated and tracked through out the distributed application for a given request Segments - Data blocks --> ip/host, response, work done(times), issues Subsegments - more granular vesion of segments including calls to other services as a part of the segment (endpoints etc) Service Graph - JSON Document detailing services and resources which make up your application Service Map - Visual version of the service graph showing traces","title":"Overview"},{"location":"monitoring/x-ray/#integration","text":"","title":"Integration"},{"location":"networking-and-hybrid/aws-client-vpn/","text":"Overview Managed implementation of OpenVPN supoorts any client with OpenVPN software Client connects to a Client VPN Endpoint associated to ONE VPC 1 + Target Networks for high availaibility .. billed based on network associations Client VPN Endpoint COST - per interface deployed Creates interface ( Client VPN ENI ) in subnet (one subnet per AZ) multiple subnet target association for HA Authentication Certificates (imported to ACM) Federated Identity provider AWS directory service (Simple AD for demo) Logging - integates with cloudwatch logs Client VPN Route Table - pushed to client machine Split Tunnel - Instead of replacing client route table, routes are added to the existing route tables - split tunnel is not default and should be enabled while creating client VPN endpoint Client - AWS VPN Client - with .ovpn file from VPN endpoint","title":"AWS Client VPN"},{"location":"networking-and-hybrid/aws-client-vpn/#overview","text":"Managed implementation of OpenVPN supoorts any client with OpenVPN software Client connects to a Client VPN Endpoint associated to ONE VPC 1 + Target Networks for high availaibility .. billed based on network associations","title":"Overview"},{"location":"networking-and-hybrid/aws-client-vpn/#client-vpn-endpoint","text":"COST - per interface deployed Creates interface ( Client VPN ENI ) in subnet (one subnet per AZ) multiple subnet target association for HA Authentication Certificates (imported to ACM) Federated Identity provider AWS directory service (Simple AD for demo) Logging - integates with cloudwatch logs Client VPN Route Table - pushed to client machine Split Tunnel - Instead of replacing client route table, routes are added to the existing route tables - split tunnel is not default and should be enabled while creating client VPN endpoint Client - AWS VPN Client - with .ovpn file from VPN endpoint","title":"Client VPN Endpoint"},{"location":"networking-and-hybrid/border-gateway-protocol/","text":"Overview Autonomous System ( AS ) - self managing network BGP comprises of ASs and viewed as black box Autonomous System Number( ASN ) are unique and allocated by IANA ( 0-65535 ), 64512-65534 are private Distributed and Reliable and operates over tcp/179 Peering between two AS is not automatic peering is manually configured iBGP = Internal BGP - Routing within an AS eBGP = ExternalBGP - Routign between AS's BGP path traversal BGP is a path-vector protocol exchanges the best path to a destination between peers the path is called ASPATH does not consider link speed or condition while calculating and advertising shortest path to its peers Prepending same ASN no. to the ASPATH can be used to artifically make a path longer making the other path preferred (if needed).","title":"Border Gateway Protocol"},{"location":"networking-and-hybrid/border-gateway-protocol/#overview","text":"Autonomous System ( AS ) - self managing network BGP comprises of ASs and viewed as black box Autonomous System Number( ASN ) are unique and allocated by IANA ( 0-65535 ), 64512-65534 are private Distributed and Reliable and operates over tcp/179 Peering between two AS is not automatic peering is manually configured iBGP = Internal BGP - Routing within an AS eBGP = ExternalBGP - Routign between AS's","title":"Overview"},{"location":"networking-and-hybrid/border-gateway-protocol/#bgp-path-traversal","text":"BGP is a path-vector protocol exchanges the best path to a destination between peers the path is called ASPATH does not consider link speed or condition while calculating and advertising shortest path to its peers Prepending same ASN no. to the ASPATH can be used to artifically make a path longer making the other path preferred (if needed).","title":"BGP path traversal"},{"location":"networking-and-hybrid/dhcp/","text":"DHCP in a VPC The Dynamic Host Configuration Protocol (DHCP) provides a standard for passing configuration information to hosts on a TCP/IP network. The options field of a DHCP message contains configuration parameters, including the domain name , domain name server , and the netbios-node-type . When you create a VPC , AWS automatically create a set of DHCP options and associate them with the VPC . You can configure your own DHCP options set for your VPC. Once created, DHCP option sets cannot be modified IMMMUTABLE DHCP option sets can be associated with 0 or more VPCs One VPC can have only 1 max Option Set Associating a new option set is immediate - but changes require a DHCP Renwe which takes time Default Gateway - subnet + 1 DNS Server - VPC + 2 (by default R53 Resolver)","title":"DHCP"},{"location":"networking-and-hybrid/dhcp/#dhcp-in-a-vpc","text":"The Dynamic Host Configuration Protocol (DHCP) provides a standard for passing configuration information to hosts on a TCP/IP network. The options field of a DHCP message contains configuration parameters, including the domain name , domain name server , and the netbios-node-type . When you create a VPC , AWS automatically create a set of DHCP options and associate them with the VPC . You can configure your own DHCP options set for your VPC. Once created, DHCP option sets cannot be modified IMMMUTABLE DHCP option sets can be associated with 0 or more VPCs One VPC can have only 1 max Option Set Associating a new option set is immediate - but changes require a DHCP Renwe which takes time Default Gateway - subnet + 1 DNS Server - VPC + 2 (by default R53 Resolver)","title":"DHCP in a VPC"},{"location":"networking-and-hybrid/global-accelerator/","text":"Overview AWS Global Accelerator is designed to improve global network performance by offering entry point onto the global AWS transit network as close to customers as possible using ANycast IP addresses Anycast IP's allow a single IP to be in multiple locations . Routing moves traffic to closet location 2 anycast IP Addresses All location of global accelerator edge locations wil use the pair of anycast IP addresses Traffic initially uses public internet before entering a Global Accelerator edge location from the edge, data transits globally across the AWS global backbone network increases the performance substantially Global Accelerator vs Cloudfront Global Accelerator Cloudfront Doesnot support caching Cache the http content using edge location Move AWS network closer to the customer Move content closer to the customer by caching Network product which supports TCP/UDP (deos not understand http protocol) Only supports HTTP/HTTPS content caching","title":"Global Accelerator"},{"location":"networking-and-hybrid/global-accelerator/#overview","text":"AWS Global Accelerator is designed to improve global network performance by offering entry point onto the global AWS transit network as close to customers as possible using ANycast IP addresses Anycast IP's allow a single IP to be in multiple locations . Routing moves traffic to closet location 2 anycast IP Addresses All location of global accelerator edge locations wil use the pair of anycast IP addresses Traffic initially uses public internet before entering a Global Accelerator edge location from the edge, data transits globally across the AWS global backbone network increases the performance substantially","title":"Overview"},{"location":"networking-and-hybrid/global-accelerator/#global-accelerator-vs-cloudfront","text":"Global Accelerator Cloudfront Doesnot support caching Cache the http content using edge location Move AWS network closer to the customer Move content closer to the customer by caching Network product which supports TCP/UDP (deos not understand http protocol) Only supports HTTP/HTTPS content caching","title":"Global Accelerator vs Cloudfront"},{"location":"networking-and-hybrid/ipsec-vpn/","text":"Fundamentals IKE=Internet Key Exchange SA=Securit Association DH=Diffie\u2013Hellman key exchange IPSEC is a group of protocols.. It sets up secure tunnels across insecure networks between two peers (local and remote) Data inside tunnels is encrypted - secure connection over an insecure network Provieds Authentication between peers and data is encrypted in transit \"interesting traffic\" is the traffic that is allowed in the encryption domain Symmetric Encryption - fast but exchanging keys securely is challenging Asymmetric Encryption - slow but exchanging public keys is easy IKE Phase 1 - slow & heavy authenticate - pre-shared key(password)/cerficate using asymmetric encryption to agree on and create a shared symmetric key IKE SA created (phase 1 tunnel) IKE Phase 2 - fast & agile uses the key agreed in phase 1 agree encryption method, and keys used for bulk data transfer Create IPSEC SA .. phase2 tunnel (architecturally running over phase1) Policy-based VPNs ..rule sets match traffic => a pair of SAs ..different rules/security settings Route-based VPNs ..target matching (prefix) ..matches a single pair of SAs","title":"IPSec VPN"},{"location":"networking-and-hybrid/ipsec-vpn/#fundamentals","text":"IKE=Internet Key Exchange SA=Securit Association DH=Diffie\u2013Hellman key exchange IPSEC is a group of protocols.. It sets up secure tunnels across insecure networks between two peers (local and remote) Data inside tunnels is encrypted - secure connection over an insecure network Provieds Authentication between peers and data is encrypted in transit \"interesting traffic\" is the traffic that is allowed in the encryption domain","title":"Fundamentals"},{"location":"networking-and-hybrid/ipsec-vpn/#_1","text":"Symmetric Encryption - fast but exchanging keys securely is challenging Asymmetric Encryption - slow but exchanging public keys is easy IKE Phase 1 - slow & heavy authenticate - pre-shared key(password)/cerficate using asymmetric encryption to agree on and create a shared symmetric key IKE SA created (phase 1 tunnel) IKE Phase 2 - fast & agile uses the key agreed in phase 1 agree encryption method, and keys used for bulk data transfer Create IPSEC SA .. phase2 tunnel (architecturally running over phase1) Policy-based VPNs ..rule sets match traffic => a pair of SAs ..different rules/security settings Route-based VPNs ..target matching (prefix) ..matches a single pair of SAs","title":""},{"location":"networking-and-hybrid/ipsec-vpn/#_2","text":"","title":""},{"location":"networking-and-hybrid/ipv6-ipv4/","text":"IPv4 Private IPv4 (RFC1918) are translated into Public IPv4 via NAT NAT=Network Address Translation Services ( EC2 instances ) never have public IPv4 Addressing configured on them within a VPC .. handled by the IGW IPV4 Address Space - Total: 4,294,967,296 |CLASS A| CLASS B| CLASS C|CLASS D|CLASS E| |:---:|:---:|:---:|:---:|:---:| |||||| IPv6 All IPv6 addresses in use within AWS are publicly routable NAT is not used for IPv6 Egress Only Internet Gateway for outgoing traffic only Enable on the VPC Either AWS provides the range /56 for the VPC and subnets pick the CIDR rannge within VPC CIDR range OR bring your own IPv6 range Routing for IPv4 and IPv6 are handled seperately |Destination|Target| |:---:|:---:| |10.16.0.0/16|local| | 2600::ff18:8e1:2200::/56 | local | |0.0.0.0/0|IGW| | ::/0 | EG-IGW | Considerations IPv6 can be added while creating a VPC/Sunets Or addressing can be migrated to IPv6 Add VPC Range , Add subnet range(s) Add routes & appropriate gateway(s) Configure IPv6 on Services Be aware of which services support IPv6 vs NOT like AWS PrivateLink","title":"Ipv6 ipv4"},{"location":"networking-and-hybrid/ipv6-ipv4/#ipv4","text":"Private IPv4 (RFC1918) are translated into Public IPv4 via NAT NAT=Network Address Translation Services ( EC2 instances ) never have public IPv4 Addressing configured on them within a VPC .. handled by the IGW IPV4 Address Space - Total: 4,294,967,296 |CLASS A| CLASS B| CLASS C|CLASS D|CLASS E| |:---:|:---:|:---:|:---:|:---:| ||||||","title":"IPv4"},{"location":"networking-and-hybrid/ipv6-ipv4/#ipv6","text":"All IPv6 addresses in use within AWS are publicly routable NAT is not used for IPv6 Egress Only Internet Gateway for outgoing traffic only Enable on the VPC Either AWS provides the range /56 for the VPC and subnets pick the CIDR rannge within VPC CIDR range OR bring your own IPv6 range Routing for IPv4 and IPv6 are handled seperately |Destination|Target| |:---:|:---:| |10.16.0.0/16|local| | 2600::ff18:8e1:2200::/56 | local | |0.0.0.0/0|IGW| | ::/0 | EG-IGW | Considerations IPv6 can be added while creating a VPC/Sunets Or addressing can be migrated to IPv6 Add VPC Range , Add subnet range(s) Add routes & appropriate gateway(s) Configure IPv6 on Services Be aware of which services support IPv6 vs NOT like AWS PrivateLink","title":"IPv6"},{"location":"networking-and-hybrid/private-link/","text":"AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network. Key Points HA via multiple endpoints in multiple AZ Only supports IPv4 and TCP( IPv6 is not supported ) Private DNS is supported (verified domains) Direct Connect, Site-to-Site VPN and VPC Peering are supported .","title":"AWS PrivateLink"},{"location":"networking-and-hybrid/private-link/#key-points","text":"HA via multiple endpoints in multiple AZ Only supports IPv4 and TCP( IPv6 is not supported ) Private DNS is supported (verified domains) Direct Connect, Site-to-Site VPN and VPC Peering are supported .","title":"Key Points"},{"location":"networking-and-hybrid/public-private-services/","text":"Diffferentiors NETWORKING PERSPECTIVE but there is also PERMISSION prespective for authorization Public Internet AWS Public Zone AWS private Zone Internet services like email AWS Public servcies like s3 Resources located inside private VPCs like EC2 instances","title":"Public vs Private Services"},{"location":"networking-and-hybrid/public-private-services/#diffferentiors","text":"NETWORKING PERSPECTIVE but there is also PERMISSION prespective for authorization Public Internet AWS Public Zone AWS private Zone Internet services like email AWS Public servcies like s3 Resources located inside private VPCs like EC2 instances","title":"Diffferentiors"},{"location":"networking-and-hybrid/site-to-site-accelerated/","text":"Accelerated Site-to-Stie VPN Accelerated VPN provides performance enhancements by routing traffic over a more direct and efficient path between CGW and AWS, avoiding the public internet as much as possible. Runing Direct Connect could be expensive for businesses Accleration can be enabled when creating a TGW VPN attachment. Not compatible with VPNs using a VGW fixed accelarator cost + a transfer fee AWS Network has been extended closer to your premises so the benefits appear earlier the traffic will only be in public internet when connecting between CGW and edge locations. DEMO Router1Private 192.168.12.154 Private IP of Router1 - Router1Public 3.85.143.209 Public IP of Router1 - Router2Private 192.168.12.52 Private IP of Router2 - Router2Public 54.89.189.79 Public IP of Router2 - Create 2 CGW for 2 routers on AWS using their public address. Create 2 TGW attachement for VPN using the CGW ids TGW-VPN attachment creates 2 tunnels between TGW and CGW in AWS they are two endpoints created in 2 different AZs while creeating TGW attachment enable Acceleration improves performance of VPN tunnels via AWS Global Accelerator and the AWS global network SiteToSite VPN from TGW <--> will be in the pending state","title":"Accelerated Site-to-Site VPN"},{"location":"networking-and-hybrid/site-to-site-accelerated/#accelerated-site-to-stie-vpn","text":"Accelerated VPN provides performance enhancements by routing traffic over a more direct and efficient path between CGW and AWS, avoiding the public internet as much as possible. Runing Direct Connect could be expensive for businesses Accleration can be enabled when creating a TGW VPN attachment. Not compatible with VPNs using a VGW fixed accelarator cost + a transfer fee AWS Network has been extended closer to your premises so the benefits appear earlier the traffic will only be in public internet when connecting between CGW and edge locations.","title":"Accelerated Site-to-Stie VPN"},{"location":"networking-and-hybrid/site-to-site-accelerated/#demo","text":"Router1Private 192.168.12.154 Private IP of Router1 - Router1Public 3.85.143.209 Public IP of Router1 - Router2Private 192.168.12.52 Private IP of Router2 - Router2Public 54.89.189.79 Public IP of Router2 - Create 2 CGW for 2 routers on AWS using their public address. Create 2 TGW attachement for VPN using the CGW ids TGW-VPN attachment creates 2 tunnels between TGW and CGW in AWS they are two endpoints created in 2 different AZs while creeating TGW attachment enable Acceleration improves performance of VPN tunnels via AWS Global Accelerator and the AWS global network SiteToSite VPN from TGW <--> will be in the pending state","title":"DEMO"},{"location":"networking-and-hybrid/site-to-site-vpn/","text":"AWS Site-to-Site VPN is a hardware VPN solution which creates a highly available IPSEC VPN between an AWS VPN and external network such as on-premises traditional networks Overview A logical connection between a VPC and on-premise network encrypted using IPSec, running over the public internet Full HA - if you design and implement it correctly Compare to Direct Connect( DX ) setup, it is quick to provision - less than an hour Components Virtual Private Gateway ( VGW ) associated with a VPC and added as route in route table. HA by design with multiple physical Endpoints the Endpoints span over multiple AZs Customer Gateway ( CGW ) represents both logical (AWS) and physical (on-premise) created in AWS VPN connection between the VGW and CGW static or dynamic Static vs Dynamic VPN (BGP) Static Dynamic Easy to setup Relatively tedious than static to implement No load balancing and multi connection failover Multiple VPN connections provide HA and traffic distribution Routes for remote side added to route tables as static routes Addition to static routes , route propagation can be enabled to automatically add routes to RT's using BGP protocol BGP is configured on both customer annd AWS using ASN and networks are exchanged via BGP VPN Considerations Speed Limitations ~ 1.25Gbps Cap on VGW ~ 1.25Gbps Latency Considerations - inconsistent, public internet Cost - AWS hourly cost, GB out cost , data cap for internet provider (on premises) Speed of setup - in hours .. all software configuration Can be used as a backup for Direct Connect( DX ) Can be used with Direct Connect( DX ) start with VPN till DX is approved","title":"Site-to-Site VPN"},{"location":"networking-and-hybrid/site-to-site-vpn/#overview","text":"A logical connection between a VPC and on-premise network encrypted using IPSec, running over the public internet Full HA - if you design and implement it correctly Compare to Direct Connect( DX ) setup, it is quick to provision - less than an hour","title":"Overview"},{"location":"networking-and-hybrid/site-to-site-vpn/#components","text":"Virtual Private Gateway ( VGW ) associated with a VPC and added as route in route table. HA by design with multiple physical Endpoints the Endpoints span over multiple AZs Customer Gateway ( CGW ) represents both logical (AWS) and physical (on-premise) created in AWS VPN connection between the VGW and CGW static or dynamic","title":"Components"},{"location":"networking-and-hybrid/site-to-site-vpn/#static-vs-dynamic-vpn-bgp","text":"Static Dynamic Easy to setup Relatively tedious than static to implement No load balancing and multi connection failover Multiple VPN connections provide HA and traffic distribution Routes for remote side added to route tables as static routes Addition to static routes , route propagation can be enabled to automatically add routes to RT's using BGP protocol BGP is configured on both customer annd AWS using ASN and networks are exchanged via BGP","title":"Static vs Dynamic VPN (BGP)"},{"location":"networking-and-hybrid/site-to-site-vpn/#vpn-considerations","text":"Speed Limitations ~ 1.25Gbps Cap on VGW ~ 1.25Gbps Latency Considerations - inconsistent, public internet Cost - AWS hourly cost, GB out cost , data cap for internet provider (on premises) Speed of setup - in hours .. all software configuration Can be used as a backup for Direct Connect( DX ) Can be used with Direct Connect( DX ) start with VPN till DX is approved","title":"VPN Considerations"},{"location":"networking-and-hybrid/stateless-vs-stateful/","text":"Stateful vs Stateless Firewalls Stateless Stateful Explicit inbound and outbound for any traffic In-built intelligence based on the request Inbound and outbound rule for both client and server Outbound rules are automatically determined and allowed like ephemeral port Outbound and inbound for server for software update Inbounds rules are automatically determined from the request like ephemeral port NACL A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. STATELESS - request and response are viewed as different entity NACL filter traffic crossing the subnet bondary INBOUND or OUTBOUND It does not affect traffic within the subnet Each NACL comprises of 2 set of rules - Inbound Rules and Outbound Rules Each rule within NACL can explicitly ALLOW or DENY traffic based on DST IPs/CIDR or DST Port or Protocols - no logical resources Rules within NACL are processed in order. Lowest rule number gets priority and * can be used for implicit deny if no match is found for the destination traffic. Can be used with Security Group to add explicit Deny Single NACL can be associated with multiple subnets Default NACL VPC is created with default NACL Inbound and Outbound Rules have the implicit deny(*) and ALLOW ALL rule results in all traffic is allowed and the NACL has no effect Custom NACL Custom NACLs can be creaeted for a specific VPC not associated with any SUBNETS initially Default inbound and oubound rules have implicit(*) DENY results in all traffic DENIED Security Groups STATEFUL - detect response traffic automatically Allowed (IN or OUT) request = allowed response NO Explicit DENY .. only ALLOW or implicit DENY can't block specific bad actors Supports IP/CIDR + logical resources including other security groups AND itself Attached to ENI's not instances Comprises of Inbound and Outbound rules Self referencing SG rules - nodes talking to each other in cluster mode","title":"Stateful vs Stateless"},{"location":"networking-and-hybrid/stateless-vs-stateful/#stateful-vs-stateless-firewalls","text":"Stateless Stateful Explicit inbound and outbound for any traffic In-built intelligence based on the request Inbound and outbound rule for both client and server Outbound rules are automatically determined and allowed like ephemeral port Outbound and inbound for server for software update Inbounds rules are automatically determined from the request like ephemeral port","title":"Stateful vs Stateless Firewalls"},{"location":"networking-and-hybrid/stateless-vs-stateful/#nacl","text":"A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. STATELESS - request and response are viewed as different entity NACL filter traffic crossing the subnet bondary INBOUND or OUTBOUND It does not affect traffic within the subnet Each NACL comprises of 2 set of rules - Inbound Rules and Outbound Rules Each rule within NACL can explicitly ALLOW or DENY traffic based on DST IPs/CIDR or DST Port or Protocols - no logical resources Rules within NACL are processed in order. Lowest rule number gets priority and * can be used for implicit deny if no match is found for the destination traffic. Can be used with Security Group to add explicit Deny Single NACL can be associated with multiple subnets","title":"NACL"},{"location":"networking-and-hybrid/stateless-vs-stateful/#default-nacl","text":"VPC is created with default NACL Inbound and Outbound Rules have the implicit deny(*) and ALLOW ALL rule results in all traffic is allowed and the NACL has no effect","title":"Default NACL"},{"location":"networking-and-hybrid/stateless-vs-stateful/#custom-nacl","text":"Custom NACLs can be creaeted for a specific VPC not associated with any SUBNETS initially Default inbound and oubound rules have implicit(*) DENY results in all traffic DENIED","title":"Custom NACL"},{"location":"networking-and-hybrid/stateless-vs-stateful/#security-groups","text":"STATEFUL - detect response traffic automatically Allowed (IN or OUT) request = allowed response NO Explicit DENY .. only ALLOW or implicit DENY can't block specific bad actors Supports IP/CIDR + logical resources including other security groups AND itself Attached to ENI's not instances Comprises of Inbound and Outbound rules Self referencing SG rules - nodes talking to each other in cluster mode","title":"Security Groups"},{"location":"networking-and-hybrid/transit-gateway/","text":"The AWS Transit gateway is a network gateway which can be used to significantly simplify networking between VPC's, VPN and Direct Connect. Overview Network Transit Hub to connect VPCs to on premises networks Significantly reduces network complexity Single network object - HA and Scalable Connects to other network using attachments VPC, Site-to-Site VPN & Direct Connect Gateway Without TGW VPC peering is not transitive and creates a complex MESH with a lot of Admin Overhead With TGW Supports transitive routning Can be used to create global networks Share between accounts using AWS RAM Peer with different regions .. same or cross account Less complexity vs w/o TGW Deep dive HUB and SPOKE architeture 1 DX Gateway can be attached to 3 TGW Transit VIF from DX to DX gateway One default route table for TGW by default, the attachments propagate (add) routes to the default RT by default, the default RT is used by all the attachments for routing decisions by default, all attachments can route to all attachments TGW Peering TGW can peer with other TGW - upto 50 peering atachemtns per TGW Inter-region or inter-accounts peering attachment no route like/propagation over peering attachments - use static rotues between peers RECOMMENDATION - Use unique ASNs (TGW) for future route propagation features Public DNS => Private IP resolution is not supporte over peers Cross-Region data is encrypted over AWS global network TGW - Isolated Routing Use asscoiation and propogation to accomplish isolated routing Attachments can only be associated with 1RT associated RT is used for routing decision for that attachement RT's can be associated with many attachments Attachments can propagate to many RT's even those RTs which they are not associated with","title":"Transit Gateway"},{"location":"networking-and-hybrid/transit-gateway/#overview","text":"Network Transit Hub to connect VPCs to on premises networks Significantly reduces network complexity Single network object - HA and Scalable Connects to other network using attachments VPC, Site-to-Site VPN & Direct Connect Gateway","title":"Overview"},{"location":"networking-and-hybrid/transit-gateway/#without-tgw","text":"VPC peering is not transitive and creates a complex MESH with a lot of Admin Overhead","title":"Without TGW"},{"location":"networking-and-hybrid/transit-gateway/#with-tgw","text":"Supports transitive routning Can be used to create global networks Share between accounts using AWS RAM Peer with different regions .. same or cross account Less complexity vs w/o TGW","title":"With TGW"},{"location":"networking-and-hybrid/transit-gateway/#deep-dive","text":"HUB and SPOKE architeture 1 DX Gateway can be attached to 3 TGW Transit VIF from DX to DX gateway One default route table for TGW by default, the attachments propagate (add) routes to the default RT by default, the default RT is used by all the attachments for routing decisions by default, all attachments can route to all attachments","title":"Deep dive"},{"location":"networking-and-hybrid/transit-gateway/#tgw-peering","text":"TGW can peer with other TGW - upto 50 peering atachemtns per TGW Inter-region or inter-accounts peering attachment no route like/propagation over peering attachments - use static rotues between peers RECOMMENDATION - Use unique ASNs (TGW) for future route propagation features Public DNS => Private IP resolution is not supporte over peers Cross-Region data is encrypted over AWS global network","title":"TGW Peering"},{"location":"networking-and-hybrid/transit-gateway/#tgw-isolated-routing","text":"Use asscoiation and propogation to accomplish isolated routing Attachments can only be associated with 1RT associated RT is used for routing decision for that attachement RT's can be associated with many attachments Attachments can propagate to many RT's even those RTs which they are not associated with","title":"TGW - Isolated Routing"},{"location":"networking-and-hybrid/vpc-dns-endpoints/","text":"Overview DNS via the VPC .2 | +2 address .2 is reserved in every subnet Now called the Route53 Resolver VPC resources uses to access R53 Public and Associated Private Zones Only accessible from within a VPC Hybrid network integration is problematic - IN and OUT Before Route 53 Endpoints Hybrid DNS integration DNS Forwarder via DHCP EC2 based DNS Forwarder in AWS to act as intermediary for AWS to On-premises and vice-versa. Route53 Endpoints VPC interfaces (ENIs) - Accessible over VPN or DX Inbound and Outbound Inbound = on-premises can forward to the R53 Resolver Outbound = Conditional Forwarders, R53 to On-premises Rules control what requests are forwarded your.domain.org => On-premises DNS Nameservers Inbound Enpoints - inbound to AWS Update DNS configuration for on-premise DNS servers to forward queries from on-premise to AWS zone \"aws.animals4life.org\" { type forward ; forward only ; forwarders { 10 .16.105,145 ; 10 .16.35.16 ; } ; } ; Update app server config to update DNS servers Outbound Endpoints - outbound from AWS Create forward rules for domain to forward to on-prem DNS server IPs using outbound endpoint for a VPC.","title":"VPC IPv6"},{"location":"networking-and-hybrid/vpc-dns-endpoints/#overview","text":"DNS via the VPC .2 | +2 address .2 is reserved in every subnet Now called the Route53 Resolver VPC resources uses to access R53 Public and Associated Private Zones Only accessible from within a VPC Hybrid network integration is problematic - IN and OUT","title":"Overview"},{"location":"networking-and-hybrid/vpc-dns-endpoints/#before-route-53-endpoints","text":"Hybrid DNS integration DNS Forwarder via DHCP EC2 based DNS Forwarder in AWS to act as intermediary for AWS to On-premises and vice-versa.","title":"Before Route 53 Endpoints"},{"location":"networking-and-hybrid/vpc-dns-endpoints/#route53-endpoints","text":"VPC interfaces (ENIs) - Accessible over VPN or DX Inbound and Outbound Inbound = on-premises can forward to the R53 Resolver Outbound = Conditional Forwarders, R53 to On-premises Rules control what requests are forwarded your.domain.org => On-premises DNS Nameservers Inbound Enpoints - inbound to AWS Update DNS configuration for on-premise DNS servers to forward queries from on-premise to AWS zone \"aws.animals4life.org\" { type forward ; forward only ; forwarders { 10 .16.105,145 ; 10 .16.35.16 ; } ; } ; Update app server config to update DNS servers Outbound Endpoints - outbound from AWS Create forward rules for domain to forward to on-prem DNS server IPs using outbound endpoint for a VPC.","title":"Route53 Endpoints"},{"location":"networking-and-hybrid/vpc-endpoints/","text":"Gateway endpoints Provide private access to S3 and DynamoDB Per service per region Prefix List added to route table => Gateway Endpoint Highly Available ( HA ) accross all AZs in a region by default Endpoint policy is used to control what it can access Regional Service - cannot access cross-region services Prevent leaky Buckets - S3 buckets can be set to private only by allowing access ONLY from a gateway endpoint Gateway Endpoints are NOT accessible outside the VPC Interface endpoints Provide private access to AWS Public Services Everything except S3 and DDB but S3 is supported now By default NOT HA , added to specific subnets - an ENI Enable HA by adding one endpoint to one subnet per AZ used in the VPC Network access controlled via Security grroups Endpoint policies - restrict what can be done with the endpoint TCP and IPv4 only Uses PrivateLink Endpoint provides a NEW service endpoint DNS e.g. vpce-123-xyz.sns.us-east-1.vpce.amazonaws.com Endpoint Regional DNS Endpoint Zonal DNS Applicatons can optionally use these. or.. Private DNS overrides the default DNS for services - enabled by default Gateway vs Interface Gateway Interface Only supports S3 and Dynamo Support most of the AWS services except DynamoDB. (supported s3 recently) By default, Highly Available Endpoints need to be deployed to all the subnets used in AZs. (ENI) Only supports TCP and IPv4 Prefix list added to the route table Private DNS overrides the default DNS for services Access is controlled with endpoint policy Additional to endpoint policy, security groups can be used to control network access VPC Endpoint Policies & Bucket Policies By default, vpce allow access to an entire service in a region { \"Statement\" :[ { \"Action\" : \"*\" , \"Effect\" : \"Allow\" , \"Resorce\" : \"*\" , \"Principal\" : \"*\" } ] } Some services support endpoint policies Limits access via that endpoint only Commonly used to limit what private VPCs can access Endpoint policies combined with bucket policies can fine tune the access for the bucket","title":"VPC Endpoints"},{"location":"networking-and-hybrid/vpc-endpoints/#gateway-endpoints","text":"Provide private access to S3 and DynamoDB Per service per region Prefix List added to route table => Gateway Endpoint Highly Available ( HA ) accross all AZs in a region by default Endpoint policy is used to control what it can access Regional Service - cannot access cross-region services Prevent leaky Buckets - S3 buckets can be set to private only by allowing access ONLY from a gateway endpoint Gateway Endpoints are NOT accessible outside the VPC","title":"Gateway endpoints"},{"location":"networking-and-hybrid/vpc-endpoints/#interface-endpoints","text":"Provide private access to AWS Public Services Everything except S3 and DDB but S3 is supported now By default NOT HA , added to specific subnets - an ENI Enable HA by adding one endpoint to one subnet per AZ used in the VPC Network access controlled via Security grroups Endpoint policies - restrict what can be done with the endpoint TCP and IPv4 only Uses PrivateLink Endpoint provides a NEW service endpoint DNS e.g. vpce-123-xyz.sns.us-east-1.vpce.amazonaws.com Endpoint Regional DNS Endpoint Zonal DNS Applicatons can optionally use these. or.. Private DNS overrides the default DNS for services - enabled by default","title":"Interface endpoints"},{"location":"networking-and-hybrid/vpc-endpoints/#gateway-vs-interface","text":"Gateway Interface Only supports S3 and Dynamo Support most of the AWS services except DynamoDB. (supported s3 recently) By default, Highly Available Endpoints need to be deployed to all the subnets used in AZs. (ENI) Only supports TCP and IPv4 Prefix list added to the route table Private DNS overrides the default DNS for services Access is controlled with endpoint policy Additional to endpoint policy, security groups can be used to control network access","title":"Gateway vs Interface"},{"location":"networking-and-hybrid/vpc-endpoints/#vpc-endpoint-policies-bucket-policies","text":"By default, vpce allow access to an entire service in a region { \"Statement\" :[ { \"Action\" : \"*\" , \"Effect\" : \"Allow\" , \"Resorce\" : \"*\" , \"Principal\" : \"*\" } ] } Some services support endpoint policies Limits access via that endpoint only Commonly used to limit what private VPCs can access Endpoint policies combined with bucket policies can fine tune the access for the bucket","title":"VPC Endpoint Policies &amp; Bucket Policies"},{"location":"networking-and-hybrid/vpc-router/","text":"Characteristics Virtual Router within a VPC Highly Available - across all AZs in that region Scalable - no performance management required Routes traffic between subnets also routes EXTERNAL networks INTO the VPC also routes VPC into EXTERNAL networks Router has interface in every subnet - subnet+1 address ( default GW via DHCP option set ) Controlled using route tables Route tables Route tables for your VPC Every VPC is created with a Main route table (RT) also defaults for every subnet in the VPC Custom route table can be created and associated with subnets in the VPC which removes the Main RT from the subnet Subnes are associated with One RT (Main or Custom) Main RT should be left as it is and Custom Route Tables should be created for any customization to ensure default association of subnet does not get default RT Route tables contain routes most specific routes first static routes are added and update manually if route propagation is enabled, routes are automatically added called propagated routes Limitation = 50 static routes and 100 dynamic/propagated routes Priority Route Type/Condition 1 Longest Prefix 2 Static Routes 3 Propagated Routes DX VPN Static VPN BGP AS_PATH RT's can be associated with gateways- IGW or VGW IPv4 and IPv6 are handled seperatly within a RT Routes send traffic based on destination to a target","title":"VPC Router"},{"location":"networking-and-hybrid/vpc-router/#characteristics","text":"Virtual Router within a VPC Highly Available - across all AZs in that region Scalable - no performance management required Routes traffic between subnets also routes EXTERNAL networks INTO the VPC also routes VPC into EXTERNAL networks Router has interface in every subnet - subnet+1 address ( default GW via DHCP option set ) Controlled using route tables","title":"Characteristics"},{"location":"networking-and-hybrid/vpc-router/#route-tables","text":"Route tables for your VPC Every VPC is created with a Main route table (RT) also defaults for every subnet in the VPC Custom route table can be created and associated with subnets in the VPC which removes the Main RT from the subnet Subnes are associated with One RT (Main or Custom) Main RT should be left as it is and Custom Route Tables should be created for any customization to ensure default association of subnet does not get default RT Route tables contain routes most specific routes first static routes are added and update manually if route propagation is enabled, routes are automatically added called propagated routes Limitation = 50 static routes and 100 dynamic/propagated routes Priority Route Type/Condition 1 Longest Prefix 2 Static Routes 3 Propagated Routes DX VPN Static VPN BGP AS_PATH RT's can be associated with gateways- IGW or VGW IPv4 and IPv6 are handled seperatly within a RT Routes send traffic based on destination to a target","title":"Route tables"},{"location":"networking-and-hybrid/vpc-routing/","text":"Basics Route tables for your VPC CIDR-Overlap Peering between overlapping CIDRs isn't supported Create routes specific to subnets in VPCs with same CIDR Single route table solution Add a more specific route for VPC-Y with CIDR from VPC-Y Different route tables solution one route table for subnet-X in VPC-I routing to VPC-X another route table to subnet-Y in VPC-I routing to VPC-Y A solution but not a HA solution Ingress routing Gateway route tables can be used to direct a gateway (e.g. IGW) to take actions in inbound traffic - such as forwarding it to a security appliance fine-grain control over the routing path of traffic entering your VPC.","title":"VPC Routing"},{"location":"networking-and-hybrid/vpc-routing/#basics","text":"Route tables for your VPC","title":"Basics"},{"location":"networking-and-hybrid/vpc-routing/#cidr-overlap","text":"Peering between overlapping CIDRs isn't supported Create routes specific to subnets in VPCs with same CIDR Single route table solution Add a more specific route for VPC-Y with CIDR from VPC-Y Different route tables solution one route table for subnet-X in VPC-I routing to VPC-X another route table to subnet-Y in VPC-I routing to VPC-Y A solution but not a HA solution","title":"CIDR-Overlap"},{"location":"networking-and-hybrid/vpc-routing/#ingress-routing","text":"Gateway route tables can be used to direct a gateway (e.g. IGW) to take actions in inbound traffic - such as forwarding it to a security appliance fine-grain control over the routing path of traffic entering your VPC.","title":"Ingress routing"},{"location":"networking-and-hybrid/vpc-structure/","text":"How many AZs Cost effective high availability - Spread across 2 vs 3 AZs Cost effective vs. minimal no. of AZs Subnets & Tiers","title":"VPC Structure"},{"location":"networking-and-hybrid/vpc-structure/#how-many-azs","text":"Cost effective high availability - Spread across 2 vs 3 AZs Cost effective vs. minimal no. of AZs","title":"How many AZs"},{"location":"networking-and-hybrid/vpc-structure/#subnets-tiers","text":"","title":"Subnets &amp; Tiers"},{"location":"networking-and-hybrid/direct-connect/direct-connect/","text":"DX Concepts A physical connection (1, 10 or 100 Gbps) Business Permises => DX Location => AWS Region Port Allocation at a DX Location hourly cost outbound data transfer Provisioning time - AWS take time to allocate the port Once the port is allocated , customer will need to arrange connection into the port could take weeks or months to seting up connection intially - like physically laying the cables bettween business permises and DX Location physical cable does not provide resilience DX provides low & consitent latency + high speeds Can access AWS Private Services ( VPCs ) and AWS Public Services - NO INTERNET Within the DX location, a Cross Connect is established between AWS DX Router and Customer DX Router . AWS DX Rotuer and Customer DX Router resides in AWS Direct Connect Cage and Customer/Comms Cage respectively. Configure Virtual Interfaces over physical cables - VIFs DX Physical Connection Architecture DX Connection = Physical Port (1, 10, 100 Gbps) Can only use Single-mode fibre and NO copper Speed Transceiver 1Gbps 1000BBASE-LX (1310 nm) Transceiver 10Gbps 10GBBASE-LR (1310 nm) Transceiver 100Gbps 100GBASE-LR4 Auto-negotiation shoule be DISABLED configure port speed and full-duplex manually set Ensure the router in DX location supports BGP and BGP MD5 Authentication optional: MACsec and Bidirectional Forwarding Detection (BFD) VPC endpoints cannot be accesed through Private VIP (don't need them) - accessible through public VIP DX Security (MACSec) MAC Security (MACsec) is an IEEE standard that provides data confidentiality, data integrity, and data origin authenticity. Frame encryption - layer 2 - IEE 802.1AE-201188 Hop by Hop encryption between two switches/routers (adjacency) Confidentaility - strong encryption Data Integrity - data cannot be modified in transit Data oriign authenticity Replay Protection Does not replace IPSec over DX - not end-2-end Designed to allow for super high spped .. terabit networks MACSec - 101 Each MACSec participant creates secure channel - unidirectional (1 IN an 1 OUT) Secure Channel Identifier ( SCI ) Secure Associations - sessions on SC, generally 1 exists at a time series of transient sessions 1 exists at a time except when they are being replaced MACsec encapsulation - 16 bytes MACsec tag & 16 bytes Integrity Check Value ( ICV ) MACsec Key Agreement - discovery, authentication & key generation Cipher Suite - how data is encrypted .. packets per key, rotation ... Get started with MACsec on dedicated connetions DX Connection process A DX connection begins in a DX location contains contains AWS equipment contains customer/provider equipment DX location is not owned by AWS - renting Cages are rented by AWS and Customers Only the DC staff can connect things together. when authorisation is provided by all the parties LOA-CFA - Letter of Authorization and Connecting Facility Assignment DX Virtual Interfaces BGP Session + VLAN Private and Public VIFs are based on VLANs and BGP sessions DX connections are a layer 2 connection - Data Link We need to connect to multiple layers of Layer 3(IP) networks (VPCs & public zone) over the DX connection Virtual Interfaces (VIFs) allow us to run multiple L3 networks over the layer 2 direct connect (DX) Interfaces - BGP Peering Session + VLAN VLAN isolates different layer 3 network BGP exchanges routes & authenticates Types of VIFS > for hosted connection, there can only 1 VIF VIF Desc Limit (Dedicated) Public VIF Use to connect to public zone services which don't run withing VPC private+public =50 Private VIF Use to connect to private services within your VPC private+public = 50 Transit VIF Allow integration between TGW and DX 1 BGP is between the Customer DX router and AWS DX router can be extended to customer premises Private VIF Access 1 VPC resources using private IPs (workaround for multiple VPCs - TGW, DXG *) accessing resources like EC2 with public IPs won't work Attach to VGWFs - 1* VPC only has to be in the same region as the DX location your connection terminates in 1 private VIF === 1 VGW === 1 VPC No encryption on private VIFs .. apps can layer on encyrption (e.g HTTPS ) Can use MTU of 1500 and 9001 (Jumbo Frames) - ensure all the links in the communication chain supports it Using VGW = Route Propagation enabled by default! but will have to terminate the private VIF at the VGW VGW has AWS ASN - or you can configure one IPv4 or IPv6 (Seperate BGP pereing connetion) You configure YOUR ASN on the VIF. either public ASN or private ASN (64512 to 65535) Creating Private VIF Pick the connection the VIF will run over Choose VGW (default) or Direct Connect Gateway* Interface Owner - this AWS account or another AWS account Choose a VLAN id - 802.1Q - needs to match customer config Requires BGP ASN of on-premises (public or private). If private, use 64512 to 65535 choose peer IPs or auto-generated by AWS AWS will advertise the VPC CIDR and the BGP Peer IPs (/30's) you can advertise default or specific corp prefixes ( max 100 ) beyond 100, the interface will go ideal and won't work Once the VIF is created , you can download the configuration file and configure the customer equipment Public VIFs Public VIFs provide access to AWS public zone services such as SQS, SNS, S3 as well as Public and Elastic IPs Access Public Zone services .. elastic IPs, public services (e.g SNS, SQS, S3 etc) No direct access to private (VPC) services Can access all public zone regions - accross AWS global network all regions AWS advertise all AWS public IP ranges to you .. you advertise any public IPs you own over BGP - have to work with AWS .. bi-directional BGP communities - filter what routes you recieve based on geographic region Advertised perfixes are not trasitive .. your prefixes don't leave AWS ( no other customers ) Creating Public VIFs Easy to set up assuming there is already an operational DX connection Public VIF + VPN Neither public or private VIFS offer any form of encryption Encrypted & Authenticated tunnel Running over DX (low latency & consistency latency) Uses a Public VIF + VGW/TGW public endpoints - consider what is it you trying to access VPN is transit agnostice - ( DX/Public Internet ) VPN is end-2-end encryption ( MACsec is single hop based ) CGW <====> TGW/VGW VPN has wider vendor support VPN has more CRYPTOGRAPHIC overhead (limits speeds) vs MACSec VPN can be used while DX is being provisioned and/or as a DX Backup Direct Connect Gateway DX Gateway can be used to extend private VIF functionality to allow connections to global VPCs (up to 500) from a single DX connection. Global newtwork device - accessible in all regions Associate private VIF to DX Gateway -- Private VIF => DX Gateway( any region) Associate with VGW's attached to VPCs globally in AWS Communication between VPCs and On-premises is allowed. Communication between VPC with other VPCs via DX Gateway is not allowed 1 Private VIF = 1 DX Gateway & 10 VGW per DX Gateway 1 DX can have 50 private VIFs == 50 DX Gateways == 500 VPCs Cross-Account Direct Connect Gateway DX Gateway, Transit VIFs and TGW A DX Gateway can be associated with VPCs & Private VIFs .. OR .. TGW & Trasnit VIF ... NOT BOTH 1 Transit VIP/ DX 1 Transit VIP can supports upto 3 TGWs DXGW does not route between attachments - Use TGW peering TGW can connect to MAX of 20 DXGW Each TGW supports 5000 attachments Direct Connect Resilience Multiple points of failure in default setup of Direct Connect Set up multiple routers in DX location (at least 2 of each AWS router and customer router) - some resiliency point of failure is still DX location and customer premise Multiple (at least 2) DX locations with multiple (at least 2) customer premises - for better resiliency Combine 2 and 3 for the maximum resiliency Direct Connect Link Aggregation Groups NOT A RESILIENCY FEATURE Multiple physical connections act as one - speed multiplier Uses active/active architecture all members of the LAG are used at the same time 2 100Gb ports or maximum of 4 connections < 100Gb per LAG All connections need to be the same speed and terminate at the same DX location Attrbute minimumLinks determine the active status of LAG . if the no. of connections goes below this number, the LAG is viewed as failure .","title":"Direct Connect"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#dx-concepts","text":"A physical connection (1, 10 or 100 Gbps) Business Permises => DX Location => AWS Region Port Allocation at a DX Location hourly cost outbound data transfer Provisioning time - AWS take time to allocate the port Once the port is allocated , customer will need to arrange connection into the port could take weeks or months to seting up connection intially - like physically laying the cables bettween business permises and DX Location physical cable does not provide resilience DX provides low & consitent latency + high speeds Can access AWS Private Services ( VPCs ) and AWS Public Services - NO INTERNET Within the DX location, a Cross Connect is established between AWS DX Router and Customer DX Router . AWS DX Rotuer and Customer DX Router resides in AWS Direct Connect Cage and Customer/Comms Cage respectively. Configure Virtual Interfaces over physical cables - VIFs","title":"DX Concepts"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#dx-physical-connection-architecture","text":"DX Connection = Physical Port (1, 10, 100 Gbps) Can only use Single-mode fibre and NO copper Speed Transceiver 1Gbps 1000BBASE-LX (1310 nm) Transceiver 10Gbps 10GBBASE-LR (1310 nm) Transceiver 100Gbps 100GBASE-LR4 Auto-negotiation shoule be DISABLED configure port speed and full-duplex manually set Ensure the router in DX location supports BGP and BGP MD5 Authentication optional: MACsec and Bidirectional Forwarding Detection (BFD) VPC endpoints cannot be accesed through Private VIP (don't need them) - accessible through public VIP","title":"DX Physical Connection Architecture"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#dx-security-macsec","text":"MAC Security (MACsec) is an IEEE standard that provides data confidentiality, data integrity, and data origin authenticity. Frame encryption - layer 2 - IEE 802.1AE-201188 Hop by Hop encryption between two switches/routers (adjacency) Confidentaility - strong encryption Data Integrity - data cannot be modified in transit Data oriign authenticity Replay Protection Does not replace IPSec over DX - not end-2-end Designed to allow for super high spped .. terabit networks","title":"DX Security (MACSec)"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#macsec-101","text":"Each MACSec participant creates secure channel - unidirectional (1 IN an 1 OUT) Secure Channel Identifier ( SCI ) Secure Associations - sessions on SC, generally 1 exists at a time series of transient sessions 1 exists at a time except when they are being replaced MACsec encapsulation - 16 bytes MACsec tag & 16 bytes Integrity Check Value ( ICV ) MACsec Key Agreement - discovery, authentication & key generation Cipher Suite - how data is encrypted .. packets per key, rotation ... Get started with MACsec on dedicated connetions","title":"MACSec - 101"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#dx-connection-process","text":"A DX connection begins in a DX location contains contains AWS equipment contains customer/provider equipment DX location is not owned by AWS - renting Cages are rented by AWS and Customers Only the DC staff can connect things together. when authorisation is provided by all the parties LOA-CFA - Letter of Authorization and Connecting Facility Assignment","title":"DX Connection process"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#dx-virtual-interfaces","text":"","title":"DX Virtual Interfaces"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#bgp-session-vlan","text":"Private and Public VIFs are based on VLANs and BGP sessions DX connections are a layer 2 connection - Data Link We need to connect to multiple layers of Layer 3(IP) networks (VPCs & public zone) over the DX connection Virtual Interfaces (VIFs) allow us to run multiple L3 networks over the layer 2 direct connect (DX) Interfaces - BGP Peering Session + VLAN VLAN isolates different layer 3 network BGP exchanges routes & authenticates Types of VIFS > for hosted connection, there can only 1 VIF VIF Desc Limit (Dedicated) Public VIF Use to connect to public zone services which don't run withing VPC private+public =50 Private VIF Use to connect to private services within your VPC private+public = 50 Transit VIF Allow integration between TGW and DX 1 BGP is between the Customer DX router and AWS DX router can be extended to customer premises","title":"BGP Session + VLAN"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#private-vif","text":"Access 1 VPC resources using private IPs (workaround for multiple VPCs - TGW, DXG *) accessing resources like EC2 with public IPs won't work Attach to VGWFs - 1* VPC only has to be in the same region as the DX location your connection terminates in 1 private VIF === 1 VGW === 1 VPC No encryption on private VIFs .. apps can layer on encyrption (e.g HTTPS ) Can use MTU of 1500 and 9001 (Jumbo Frames) - ensure all the links in the communication chain supports it Using VGW = Route Propagation enabled by default! but will have to terminate the private VIF at the VGW VGW has AWS ASN - or you can configure one IPv4 or IPv6 (Seperate BGP pereing connetion) You configure YOUR ASN on the VIF. either public ASN or private ASN (64512 to 65535) Creating Private VIF Pick the connection the VIF will run over Choose VGW (default) or Direct Connect Gateway* Interface Owner - this AWS account or another AWS account Choose a VLAN id - 802.1Q - needs to match customer config Requires BGP ASN of on-premises (public or private). If private, use 64512 to 65535 choose peer IPs or auto-generated by AWS AWS will advertise the VPC CIDR and the BGP Peer IPs (/30's) you can advertise default or specific corp prefixes ( max 100 ) beyond 100, the interface will go ideal and won't work Once the VIF is created , you can download the configuration file and configure the customer equipment","title":"Private VIF"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#public-vifs","text":"Public VIFs provide access to AWS public zone services such as SQS, SNS, S3 as well as Public and Elastic IPs Access Public Zone services .. elastic IPs, public services (e.g SNS, SQS, S3 etc) No direct access to private (VPC) services Can access all public zone regions - accross AWS global network all regions AWS advertise all AWS public IP ranges to you .. you advertise any public IPs you own over BGP - have to work with AWS .. bi-directional BGP communities - filter what routes you recieve based on geographic region Advertised perfixes are not trasitive .. your prefixes don't leave AWS ( no other customers ) Creating Public VIFs Easy to set up assuming there is already an operational DX connection Public VIF + VPN Neither public or private VIFS offer any form of encryption Encrypted & Authenticated tunnel Running over DX (low latency & consistency latency) Uses a Public VIF + VGW/TGW public endpoints - consider what is it you trying to access VPN is transit agnostice - ( DX/Public Internet ) VPN is end-2-end encryption ( MACsec is single hop based ) CGW <====> TGW/VGW VPN has wider vendor support VPN has more CRYPTOGRAPHIC overhead (limits speeds) vs MACSec VPN can be used while DX is being provisioned and/or as a DX Backup","title":"Public VIFs"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#direct-connect-gateway","text":"DX Gateway can be used to extend private VIF functionality to allow connections to global VPCs (up to 500) from a single DX connection. Global newtwork device - accessible in all regions Associate private VIF to DX Gateway -- Private VIF => DX Gateway( any region) Associate with VGW's attached to VPCs globally in AWS Communication between VPCs and On-premises is allowed. Communication between VPC with other VPCs via DX Gateway is not allowed 1 Private VIF = 1 DX Gateway & 10 VGW per DX Gateway 1 DX can have 50 private VIFs == 50 DX Gateways == 500 VPCs Cross-Account Direct Connect Gateway","title":"Direct Connect Gateway"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#dx-gateway-transit-vifs-and-tgw","text":"A DX Gateway can be associated with VPCs & Private VIFs .. OR .. TGW & Trasnit VIF ... NOT BOTH 1 Transit VIP/ DX 1 Transit VIP can supports upto 3 TGWs DXGW does not route between attachments - Use TGW peering TGW can connect to MAX of 20 DXGW Each TGW supports 5000 attachments","title":"DX Gateway, Transit VIFs and TGW"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#direct-connect-resilience","text":"Multiple points of failure in default setup of Direct Connect Set up multiple routers in DX location (at least 2 of each AWS router and customer router) - some resiliency point of failure is still DX location and customer premise Multiple (at least 2) DX locations with multiple (at least 2) customer premises - for better resiliency Combine 2 and 3 for the maximum resiliency","title":"Direct Connect Resilience"},{"location":"networking-and-hybrid/direct-connect/direct-connect/#direct-connect-link-aggregation-groups","text":"NOT A RESILIENCY FEATURE Multiple physical connections act as one - speed multiplier Uses active/active architecture all members of the LAG are used at the same time 2 100Gb ports or maximum of 4 connections < 100Gb per LAG All connections need to be the same speed and terminate at the same DX location Attrbute minimumLinks determine the active status of LAG . if the no. of connections goes below this number, the LAG is viewed as failure .","title":"Direct Connect Link Aggregation Groups"},{"location":"networking-and-hybrid/route53/dns/","text":"Domain Name System - distributed discovery service Information is distributed and delegated - recursive query DNS Client Resolver Zone Zonefile Nameserver laptop, phone software on you device or a server which queries DNS on your behalf A part of the DNS database physical database for a zone where zonefiles are hosted Root Hints => provided by OS vendor - config points at the root server IPs and addresses Root Server => hosts the DNS root zone Root Zone => points at TLD authoritative servers gTLD => generic top level domain (.com .org) ccTLD => country-code top level domain (.uk, .eu etc) IANA : https://www.iana.org Root hints : https://www.internic.net/domain/named.root Root Servers : https://www.iana.org/domains/root/servers Root Zone Database : https://www.iana.org/domains/root/db Root Zone File : https://www.internic.net/domain/root.zone Delegation Record for .com : https://www.iana.org/domains/root/db/com.html","title":"DNS"},{"location":"networking-and-hybrid/route53/fundamentals/","text":"Basics Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It provides following features Register Domains Hosted Zones - managed nameserers Zone files in AWS Hosted on four managed name servers Public Hosted Zone Private Hosted Zone - linked to VPCs RecordSets - records within hosted zone DNS Record Types Nameserver (NS) A and AAAA Records A record maps to ipv4 AAAA record maps to ipv6 CNAME Records cannot point directly to other names can only point to the name MX Records important for email priority and value (mail || mail.other.domain) MX query TXT Records additional text data to the record Query TXT for the validation TTL - Time To Live Resolver cached for TTL no. of seconds also non-authoriative answer","title":"Fundamentals"},{"location":"networking-and-hybrid/route53/fundamentals/#basics","text":"Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It provides following features Register Domains Hosted Zones - managed nameserers Zone files in AWS Hosted on four managed name servers Public Hosted Zone Private Hosted Zone - linked to VPCs RecordSets - records within hosted zone","title":"Basics"},{"location":"networking-and-hybrid/route53/fundamentals/#dns-record-types","text":"Nameserver (NS) A and AAAA Records A record maps to ipv4 AAAA record maps to ipv6 CNAME Records cannot point directly to other names can only point to the name MX Records important for email priority and value (mail || mail.other.domain) MX query TXT Records additional text data to the record Query TXT for the validation","title":"DNS Record Types"},{"location":"networking-and-hybrid/route53/fundamentals/#ttl-time-to-live","text":"Resolver cached for TTL no. of seconds also non-authoriative answer","title":"TTL - Time To Live"},{"location":"networking-and-hybrid/route53/hosted-zones/","text":"Overview A R53 Hosted Zone is a DNS DB for a domain e.g. myorg.org Globally resilient (multiple DNS servers) Created with domain registration via R53 (also can be created seperately) Host DNS Records (e.g A, AAAA, MX, NS, TXT...) Hosted Zones are what the DNS sytem refrences - Authoritative for a domain e.g. myorg.org Public Hosted Zone DNS Database ( zone file ) hosted by R53 ( Public Name Servers ) Accessible from the public internet as well as from VPCs using R53 Resolver Hosted on 4 R53 Name servers ( NS ) specific for the zone use NS records to point at these NS (connect to global DNS) Resource Records ( RR ) created within the Hosted Zone Externally registered domains can point at R53 Public Zone Private Hosted Zone A Public Hosted zone which is not public Associated with VPCs only accessible in those VPCs using different accounts is supported via CLI/API Split-view (overlapping public and private ) for PUBLIC and INTERNAL use with the same one name Split View Hosted Zones Records in the private hosted zone are not accessible from the public internet Hosted zone associated with VPCs are accessible within the VPCs R53 CNAME vs ALIAS A maps a NAME to an IP address like catagram.ip => 1.2.3.4 CNAME maps a NAME to another NAME like www.catagram.io => catagram.io CNAME is invalid for naked/apex Many AWS services uses a DNS name (ELBs) With just CNAME - catagram.io => ELB would be invalid Alias Records ALIAS records map a NAME to an AWS resoruce Can be used both for naked/apex and normal records For non apex/naked - functions like CNAME For AWS Services - default to picking ALIAS Should be the same Type as what the record is pointing at API Gatewy, Cloudfront, Elastic Beanstalk, ELB, Global Accelerator & s3","title":"Hosted Zones"},{"location":"networking-and-hybrid/route53/hosted-zones/#overview","text":"A R53 Hosted Zone is a DNS DB for a domain e.g. myorg.org Globally resilient (multiple DNS servers) Created with domain registration via R53 (also can be created seperately) Host DNS Records (e.g A, AAAA, MX, NS, TXT...) Hosted Zones are what the DNS sytem refrences - Authoritative for a domain e.g. myorg.org","title":"Overview"},{"location":"networking-and-hybrid/route53/hosted-zones/#public-hosted-zone","text":"DNS Database ( zone file ) hosted by R53 ( Public Name Servers ) Accessible from the public internet as well as from VPCs using R53 Resolver Hosted on 4 R53 Name servers ( NS ) specific for the zone use NS records to point at these NS (connect to global DNS) Resource Records ( RR ) created within the Hosted Zone Externally registered domains can point at R53 Public Zone","title":"Public Hosted Zone"},{"location":"networking-and-hybrid/route53/hosted-zones/#private-hosted-zone","text":"A Public Hosted zone which is not public Associated with VPCs only accessible in those VPCs using different accounts is supported via CLI/API Split-view (overlapping public and private ) for PUBLIC and INTERNAL use with the same one name","title":"Private Hosted Zone"},{"location":"networking-and-hybrid/route53/hosted-zones/#split-view-hosted-zones","text":"Records in the private hosted zone are not accessible from the public internet Hosted zone associated with VPCs are accessible within the VPCs","title":"Split View Hosted Zones"},{"location":"networking-and-hybrid/route53/hosted-zones/#r53-cname-vs-alias","text":"A maps a NAME to an IP address like catagram.ip => 1.2.3.4 CNAME maps a NAME to another NAME like www.catagram.io => catagram.io CNAME is invalid for naked/apex Many AWS services uses a DNS name (ELBs) With just CNAME - catagram.io => ELB would be invalid Alias Records ALIAS records map a NAME to an AWS resoruce Can be used both for naked/apex and normal records For non apex/naked - functions like CNAME For AWS Services - default to picking ALIAS Should be the same Type as what the record is pointing at API Gatewy, Cloudfront, Elastic Beanstalk, ELB, Global Accelerator & s3","title":"R53 CNAME vs ALIAS"},{"location":"networking-and-hybrid/route53/routing-policies/","text":"Route53 Health Checks Health Checks are created and configured seperately Records an use it for routing Health Checkers are located globally Interval - every 30s (reduce to 10s for extra cost) Supports multiple method of health checks Endpoint: Ip, TCP (2s), HTTP/HTTPS (2s + 2s body) string matching Cloudwatch alarms Other health checks - checks of checks Simple Routing Supports 1 record per NAME 1 record can have multiple values Single value is returned randomly DOES NOT SUPPORT HEALTH CHECKS Failover Routing Active/passive failover health check on primary record and failover to secondary record if health check is failing on primary record Multi-Value Routing Multiple records with same name each record is independent and can have an associated health check NOT THE SUBSTITUTION FOR LOAD BALANCER Upto 8 healthy records can be retured to client (> 8 records - randomized 8) unhealthy records are not returned Weighted Routing Simple load balancing with weights helpful for testing new software versions control the distribution for same record Latency-Based Routing Optimze performance and user experience AWS maintain database of latency to redirect traffic to record with least latency health check can be associated with routes to ensure healhty routes are returned Geolocation Routing R53 Geolocation Docs - Relevant ( location ) records - State - SubdivisionCode | SubdivisionName (only for US) - Country - CountryCode | CountryName - Continent - ContinentCode | ContinentName - Default Cane be used of regional restrictions , language specific content or load balancing across regional endpoints Geoproximity Routing Records can be tagged with an AWS Region or lat and long cordinates Routing is distance based but can be influence with bias + or - bias can be added to rules to increase a region size. Route 53 Interoperability Domain registrar or Domain Hosting","title":"Routing Policies"},{"location":"networking-and-hybrid/route53/routing-policies/#route53-health-checks","text":"Health Checks are created and configured seperately Records an use it for routing Health Checkers are located globally Interval - every 30s (reduce to 10s for extra cost) Supports multiple method of health checks Endpoint: Ip, TCP (2s), HTTP/HTTPS (2s + 2s body) string matching Cloudwatch alarms Other health checks - checks of checks","title":"Route53 Health Checks"},{"location":"networking-and-hybrid/route53/routing-policies/#simple-routing","text":"Supports 1 record per NAME 1 record can have multiple values Single value is returned randomly DOES NOT SUPPORT HEALTH CHECKS","title":"Simple Routing"},{"location":"networking-and-hybrid/route53/routing-policies/#failover-routing","text":"Active/passive failover health check on primary record and failover to secondary record if health check is failing on primary record","title":"Failover Routing"},{"location":"networking-and-hybrid/route53/routing-policies/#multi-value-routing","text":"Multiple records with same name each record is independent and can have an associated health check NOT THE SUBSTITUTION FOR LOAD BALANCER Upto 8 healthy records can be retured to client (> 8 records - randomized 8) unhealthy records are not returned","title":"Multi-Value Routing"},{"location":"networking-and-hybrid/route53/routing-policies/#weighted-routing","text":"Simple load balancing with weights helpful for testing new software versions control the distribution for same record","title":"Weighted Routing"},{"location":"networking-and-hybrid/route53/routing-policies/#latency-based-routing","text":"Optimze performance and user experience AWS maintain database of latency to redirect traffic to record with least latency health check can be associated with routes to ensure healhty routes are returned","title":"Latency-Based Routing"},{"location":"networking-and-hybrid/route53/routing-policies/#geolocation-routing","text":"R53 Geolocation Docs - Relevant ( location ) records - State - SubdivisionCode | SubdivisionName (only for US) - Country - CountryCode | CountryName - Continent - ContinentCode | ContinentName - Default Cane be used of regional restrictions , language specific content or load balancing across regional endpoints","title":"Geolocation Routing"},{"location":"networking-and-hybrid/route53/routing-policies/#geoproximity-routing","text":"Records can be tagged with an AWS Region or lat and long cordinates Routing is distance based but can be influence with bias + or - bias can be added to rules to increase a region size.","title":"Geoproximity Routing"},{"location":"networking-and-hybrid/route53/routing-policies/#route-53-interoperability","text":"Domain registrar or Domain Hosting","title":"Route 53 Interoperability"},{"location":"permission-and-accounts/aws-organizations/","text":"Overview Centrally manage multiple AWS accounts (could be 100s for large organizations) centrally manage and govern your environment as you grow and scale your AWS resources AWS Organizations is integrated with other AWS services can define central configurations , security mechanisms , audit requirements , and resource sharing across accounts in your organization One and only Management account and 0 or more Member accounts Organization root and units Organization Root is just a container which can contain both management and member accounts Organization Root also can contain other containers called Organizational Unit (OU) OU can also contains both management and member accounts as well as additional OUs Organization root and OU makes a heirarichal set up of accounts Benfits Centrally manage billing and costs - Consolidated Billing Management Account becomes payer account since all the bills are passed to the management account form members account Single bill for management account and all members account Reduces significant overhead for managing bills for larger enterprise Consolidation of reservation and volume discounts Control access and permissions Apply service control policies (SCPs) to users, accounts, or OUs to control access to AWS resources, services, and Regions within your organization Steps to create AWS organization for existing accounts creating new account within the organization does not require invitation Create organization using existing account the account now become Management Account (previously Master account ) From the Management Account invite other existing standard accounts existing standard accounts will need to approve the invite to join the organization once approved standard accounts become member accounts of the organization Role to switch bewtween acounts General account to switch to many other different accoutns Role with trust from accountID (General account) Role with adminstrative policy","title":"AWS Organizations"},{"location":"permission-and-accounts/aws-organizations/#overview","text":"Centrally manage multiple AWS accounts (could be 100s for large organizations) centrally manage and govern your environment as you grow and scale your AWS resources AWS Organizations is integrated with other AWS services can define central configurations , security mechanisms , audit requirements , and resource sharing across accounts in your organization One and only Management account and 0 or more Member accounts","title":"Overview"},{"location":"permission-and-accounts/aws-organizations/#organization-root-and-units","text":"Organization Root is just a container which can contain both management and member accounts Organization Root also can contain other containers called Organizational Unit (OU) OU can also contains both management and member accounts as well as additional OUs Organization root and OU makes a heirarichal set up of accounts","title":"Organization root and units"},{"location":"permission-and-accounts/aws-organizations/#benfits","text":"","title":"Benfits"},{"location":"permission-and-accounts/aws-organizations/#centrally-manage-billing-and-costs-consolidated-billing","text":"Management Account becomes payer account since all the bills are passed to the management account form members account Single bill for management account and all members account Reduces significant overhead for managing bills for larger enterprise Consolidation of reservation and volume discounts","title":"Centrally manage billing and costs - Consolidated Billing"},{"location":"permission-and-accounts/aws-organizations/#control-access-and-permissions","text":"Apply service control policies (SCPs) to users, accounts, or OUs to control access to AWS resources, services, and Regions within your organization","title":"Control access and permissions"},{"location":"permission-and-accounts/aws-organizations/#steps-to-create-aws-organization-for-existing-accounts","text":"creating new account within the organization does not require invitation Create organization using existing account the account now become Management Account (previously Master account ) From the Management Account invite other existing standard accounts existing standard accounts will need to approve the invite to join the organization once approved standard accounts become member accounts of the organization","title":"Steps to create AWS organization for existing accounts"},{"location":"permission-and-accounts/aws-organizations/#role-to-switch-bewtween-acounts","text":"General account to switch to many other different accoutns Role with trust from accountID (General account) Role with adminstrative policy","title":"Role to switch bewtween acounts"},{"location":"permission-and-accounts/permission-boundaries/","text":"Overview Boundaries can be applied to IAM user and roles IAM permission boundaries does not affect RESOURCE POLICIES (only IDENTITY POLICY ) Permission boundaries don't GRANT any access but define maximum permissions an identity can receive SCP only applies to identities in the account Use Cases Delegation Problems - IAM administrator permissions apply permission boundary policy for user allow adminstration to update user with the user boundary policy does not allow to modify it owns policy but also does not allow to modify user with no user boundary policy attached","title":"Permission Boundaries"},{"location":"permission-and-accounts/permission-boundaries/#overview","text":"Boundaries can be applied to IAM user and roles IAM permission boundaries does not affect RESOURCE POLICIES (only IDENTITY POLICY ) Permission boundaries don't GRANT any access but define maximum permissions an identity can receive SCP only applies to identities in the account","title":"Overview"},{"location":"permission-and-accounts/permission-boundaries/#use-cases","text":"Delegation Problems - IAM administrator permissions apply permission boundary policy for user allow adminstration to update user with the user boundary policy does not allow to modify it owns policy but also does not allow to modify user with no user boundary policy attached","title":"Use Cases"},{"location":"permission-and-accounts/permissions-evaluation/","text":"Overview Explicit Deny Check for Explicit Deny Procced with SCP if there is no explicit deny SCP Check for any SCP policy and check for Allow Procced if SCP allows Resource Policies Check for Allow and execute if it is allowed Procced if Allow is not there Permission boundaries Check for disallowed action form boundary policy Procced with Session policy if no permission boundary Session Policies Check for Session Policy Procced with Identity Policy if allow Identity Policies Check for Allow or no Allow Procced with resource execution Same Account Different Accounts","title":"Permissions Evaluation"},{"location":"permission-and-accounts/permissions-evaluation/#overview","text":"Explicit Deny Check for Explicit Deny Procced with SCP if there is no explicit deny SCP Check for any SCP policy and check for Allow Procced if SCP allows Resource Policies Check for Allow and execute if it is allowed Procced if Allow is not there Permission boundaries Check for disallowed action form boundary policy Procced with Session policy if no permission boundary Session Policies Check for Session Policy Procced with Identity Policy if allow Identity Policies Check for Allow or no Allow Procced with resource execution","title":"Overview"},{"location":"permission-and-accounts/permissions-evaluation/#same-account","text":"","title":"Same Account"},{"location":"permission-and-accounts/permissions-evaluation/#different-accounts","text":"","title":"Different Accounts"},{"location":"permission-and-accounts/policy-interpretation/","text":"deny -> allow -> allow not explicit allow - default is implicit deny Deny and allow has overlapping actions. Deny takes priority over allow but with condition. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" :[ \"s3:PutObject\" , \"s3:PutObjectAcl\" , \"s3:GetObject\" , \"s3:GetObjectAcl\" , \"s3:DeleteObject\" ], \"Resource\" : \"arn:aws:s3:::holidaygifts/*\" }, { \"Effect\" : \"Deny\" , \"Action\" : [ \"s3:GetObject\" , \"s3:GetObjectAcl\" ], \"Resource\" : \"arn:aws:s3:::holidaygifts/*\" , \"Condition\" : { \"DateGreaterThan\" : { \"aws:CurrentTime\" : \"2020-12-01T00:00:00Z\" }, \"DateLessThan\" : { \"aws:CurrentTime\" : \"2020-12-25T06:00:00Z\" } } } ] } Deny only with inverse action and inverse condition DENY does not apply for global services like ( iam, cloudfront, route53 and support ) DENY does apply for rest of the services if they are not in eu-west-1 or ap-southeast-2 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"DenyNonApprovedRegions\" , \"Effect\" : \"Deny\" , \"NotAction\" : [ \"cloudfront:*\" , \"iam:*\" , \"route53:*\" , \"support:*\" ], \"Resource\" : \"*\" , \"Condition\" : { \"StringNotEquals\" : { \"aws:RequestedRegion\" : [ \"ap-southeast-2\" , \"eu-west-1\" ] } } } ] } allow list the buckets allow list the content of home and user dir only allow to do all operations for user dir { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListAllMyBuckets\" , \"s3:GetBucketLocation\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"s3:ListBucket\" , \"Resource\" : \"arn:aws:s3:::cl-animals4life\" , \"Condition\" : { \"StringLike\" : { \"s3:prefix\" : [ \"\" , \"home/\" , \"home/${aws:username}/*\" ] } } }, { \"Effect\" : \"Allow\" , \"Action\" : \"s3:*\" , \"Resource\" : [ \"arn:aws:s3:::cl-animals4life/home/${aws:username}\" , \"arn:aws:s3:::cl-animals4life/home/${aws:username}/*\" ] } ] }","title":"Policy Interpretation"},{"location":"permission-and-accounts/resource-access-manager/","text":"Resource Access Manager (RAM) allows AWS resources to be shared securely between AWS Accounts. Overview Allows AWS resources to be shared securely between AWS Accounts. Supported resource types AWS VPC, AWS App Mesh, Amazon Auror, AWS Route 53 and more Shared resources are accessed natively (ConsoleUI and API) E.x. Shared subets is visible in both accounts Shared with Principals with all of the other accounts in the organization with only those contained by one or more specified organizational units (OUs) with specific AWS accounts by account ID NO CHARGE for RAM - cost is only associated with the AWS shared resource Provides substantial changes to traditional AWS architectures can be used to support certain common architectures such as a Shared Services VPC TGW and VPC peering is being used traditionally Sharing the resource Owner account creates a share , provides a name Owner retains full ownership - full read/write Other account will have subset of the permisssion Defines the principal with whom to share if the principal is within an AWS org with sharing enabled, it is accepted automatically if the principal is non ORG account or sharing is not enabled in AWS ORG , the principal will have to accept an invite Shared Services VPC Shared resource like subnet is owned and can only be modified by owner account of shared resource Resource like EC2 launched in shared subnet is owned by account owner NOT the shared resoruce owner Depending on the type of shared resource, it might not be sharable with any AWS accounts rather only could be shared with accounts within an AWS ORG Availability Zone ID (AZ IDs) Same AZ name for 2 different AWS accounts can be mapped to 2 different physical locations by AWS. AZ IDs are consistent accross all AWS accounts and can be used for shared infranstructure deployment .","title":"AWS Resource Access Manager"},{"location":"permission-and-accounts/resource-access-manager/#overview","text":"Allows AWS resources to be shared securely between AWS Accounts. Supported resource types AWS VPC, AWS App Mesh, Amazon Auror, AWS Route 53 and more Shared resources are accessed natively (ConsoleUI and API) E.x. Shared subets is visible in both accounts Shared with Principals with all of the other accounts in the organization with only those contained by one or more specified organizational units (OUs) with specific AWS accounts by account ID NO CHARGE for RAM - cost is only associated with the AWS shared resource Provides substantial changes to traditional AWS architectures can be used to support certain common architectures such as a Shared Services VPC TGW and VPC peering is being used traditionally","title":"Overview"},{"location":"permission-and-accounts/resource-access-manager/#sharing-the-resource","text":"Owner account creates a share , provides a name Owner retains full ownership - full read/write Other account will have subset of the permisssion Defines the principal with whom to share if the principal is within an AWS org with sharing enabled, it is accepted automatically if the principal is non ORG account or sharing is not enabled in AWS ORG , the principal will have to accept an invite","title":"Sharing the resource"},{"location":"permission-and-accounts/resource-access-manager/#shared-services-vpc","text":"Shared resource like subnet is owned and can only be modified by owner account of shared resource Resource like EC2 launched in shared subnet is owned by account owner NOT the shared resoruce owner Depending on the type of shared resource, it might not be sharable with any AWS accounts rather only could be shared with accounts within an AWS ORG","title":"Shared Services VPC"},{"location":"permission-and-accounts/resource-access-manager/#availability-zone-id-az-ids","text":"Same AZ name for 2 different AWS accounts can be mapped to 2 different physical locations by AWS. AZ IDs are consistent accross all AWS accounts and can be used for shared infranstructure deployment .","title":"Availability Zone ID (AZ IDs)"},{"location":"permission-and-accounts/security-token-service/","text":"STS Generates temporary credentials ( sts:AssumeRole* ) Credentials are short lived unlike AccessKeyId and SeretAccessKey for a user Credentials are used to access AWS resources more limited policy can be provided with IAM policy Requested by an Identity (AWS or EXTERNAL) identity could be AWS services , AWS user/role or external web identity STS Credentials Trust policy is attached to the IAM role specifies which identity can assume the role May be MFA ???? Permission policy also attached to the IAM role specifies the permission of role for AWS resources STS generates temporay credentials which can access AWS Resource until until expiration . It authorise access based on the permission policy If the permission of the role changes, the permission for the creds will also change. STS has ability to revoke active sessions and credentials for a role adding a policy with using a time stamp AWSRevokeOlderSessions Revoking temporary credentials Many identities can assume the same role all the identities get same permission defined in the permission policy Temproray credentials cannot be cancelled temporary creds comes with expiration date How to remediate credential leak?? Limitations Updating the trust policy has not impact on existing credentials Changing the permissions policy impacts ALL credentials Using AWSRevokeOlderSessions Permission policy update with AWSRevokeOlderSessions inline DENY for any sessions older than NOW DENY all the credentials generated before NOW Identities can reassume the role after enforcing AWSRevokeOlderSessions Example { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Deny\" , \"Action\" : [ \"*\" ], \"Resource\" : [ \"*\" ], \"Condition\" : { \"DateLessThan\" :{ \"aws:TokeIssueTime\" : \"2021-05-21T02:24:21.3702\" } } } ] } metadata url: http://169.254.169.254/latest/metadata/iam","title":"AWS STS"},{"location":"permission-and-accounts/security-token-service/#sts","text":"Generates temporary credentials ( sts:AssumeRole* ) Credentials are short lived unlike AccessKeyId and SeretAccessKey for a user Credentials are used to access AWS resources more limited policy can be provided with IAM policy Requested by an Identity (AWS or EXTERNAL) identity could be AWS services , AWS user/role or external web identity","title":"STS"},{"location":"permission-and-accounts/security-token-service/#sts-credentials","text":"Trust policy is attached to the IAM role specifies which identity can assume the role May be MFA ???? Permission policy also attached to the IAM role specifies the permission of role for AWS resources STS generates temporay credentials which can access AWS Resource until until expiration . It authorise access based on the permission policy If the permission of the role changes, the permission for the creds will also change. STS has ability to revoke active sessions and credentials for a role adding a policy with using a time stamp AWSRevokeOlderSessions","title":"STS Credentials"},{"location":"permission-and-accounts/security-token-service/#revoking-temporary-credentials","text":"Many identities can assume the same role all the identities get same permission defined in the permission policy Temproray credentials cannot be cancelled temporary creds comes with expiration date","title":"Revoking temporary credentials"},{"location":"permission-and-accounts/security-token-service/#how-to-remediate-credential-leak","text":"Limitations Updating the trust policy has not impact on existing credentials Changing the permissions policy impacts ALL credentials Using AWSRevokeOlderSessions Permission policy update with AWSRevokeOlderSessions inline DENY for any sessions older than NOW DENY all the credentials generated before NOW Identities can reassume the role after enforcing AWSRevokeOlderSessions Example { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Deny\" , \"Action\" : [ \"*\" ], \"Resource\" : [ \"*\" ], \"Condition\" : { \"DateLessThan\" :{ \"aws:TokeIssueTime\" : \"2021-05-21T02:24:21.3702\" } } } ] } metadata url: http://169.254.169.254/latest/metadata/iam","title":"How to remediate credential leak??"},{"location":"permission-and-accounts/service-quotas/","text":"Key Points Defines the limit on how many of AWS services can run Each service has a default per-region quota some services can be per account Most service quotas can be increased as needed some services quota cannot be changed like 5000 IAM user/account for IAM users > use identity provider Higher increases results in more process and time needed Important Links Service endpoints and quotas Service Quotas Console - also request change for quotas Service Quotas request templates - can be applied for accoutns in AWS organizations (reduced ) AWS cli for service quotas Service Quotas and Amazon CloudWatch alarms","title":"Service Quotas"},{"location":"permission-and-accounts/service-quotas/#key-points","text":"Defines the limit on how many of AWS services can run Each service has a default per-region quota some services can be per account Most service quotas can be increased as needed some services quota cannot be changed like 5000 IAM user/account for IAM users > use identity provider Higher increases results in more process and time needed","title":"Key Points"},{"location":"permission-and-accounts/service-quotas/#important-links","text":"Service endpoints and quotas Service Quotas Console - also request change for quotas Service Quotas request templates - can be applied for accoutns in AWS organizations (reduced ) AWS cli for service quotas Service Quotas and Amazon CloudWatch alarms","title":"Important Links"},{"location":"security-config/acm/","text":"BackGround HTTP - simple and insecure HTTP S - SSL/TLS Layer of encryption added to HTTP Data is encrypted in-transit Certificates prove identity Chain of trust - Signed by a trusted authority Overview ACM lets you run a public or private Certificate Authority (CA) Private CA - Applications need to trust your private CA Public CA - Browsers trust a list of provider, which can trust other providers. ACM can generate or import certificates if generated, it can automatically renew if imported, you are responsible for importing it Certificates can be deployed only to supported services Cloudfront, ALB ... NOT EC2 ACM is a regional service Certs cannnot leave the region once they are generated or imported in Eg. To use a cert with ALB in ap-southeast-2 you need a cert in ACM in ap-southeast-2 ** Global Services such as CloudFront operates as though within 'us-east-1'","title":"AWS Certificate Manager"},{"location":"security-config/acm/#background","text":"HTTP - simple and insecure HTTP S - SSL/TLS Layer of encryption added to HTTP Data is encrypted in-transit Certificates prove identity Chain of trust - Signed by a trusted authority","title":"BackGround"},{"location":"security-config/acm/#overview","text":"ACM lets you run a public or private Certificate Authority (CA) Private CA - Applications need to trust your private CA Public CA - Browsers trust a list of provider, which can trust other providers. ACM can generate or import certificates if generated, it can automatically renew if imported, you are responsible for importing it Certificates can be deployed only to supported services Cloudfront, ALB ... NOT EC2 ACM is a regional service Certs cannnot leave the region once they are generated or imported in Eg. To use a cert with ALB in ap-southeast-2 you need a cert in ACM in ap-southeast-2 ** Global Services such as CloudFront operates as though within 'us-east-1'","title":"Overview"},{"location":"security-config/cloud-hsm/","text":"CloudHSM HSM - hardware security module to manage cryprtographic operations AWS provisioned it but full managed by customer Federal Information Processing Standard Publication 140-2, (FIPS PUB 140-2) Fully FIPS 140-2 Level 3 ( KMS is L2 overall, some L3 ) KMS can use CloudHSM as a custom key store , CloudHSM integration with KMS (newer feature) CloudHSM KMS FIPS 140-2 level3 FIPS 140-2 level 2 (some L3) Industry Standard API - PKCS#11 ,Java Cryptography Extenstion ( JCE ), Microsoft CryptoNG (CNG) libraries Communicated via AWS API Configure HSM in cluster mode to ensure HA mode (multi AZ). By default, it does not run on HA mode. Use Cases No native AWS integration .. e.g. no s3 SSE client side encryption can be used before uploading Offload the SSL/TLS Processing of Web Servers. Enable Transparent Data Encryption (TDE) for Oracle Databases Protect the Private Keys for an Issuing Certificate Authority (CA)","title":"CloudHSM"},{"location":"security-config/cloud-hsm/#cloudhsm","text":"HSM - hardware security module to manage cryprtographic operations AWS provisioned it but full managed by customer Federal Information Processing Standard Publication 140-2, (FIPS PUB 140-2) Fully FIPS 140-2 Level 3 ( KMS is L2 overall, some L3 ) KMS can use CloudHSM as a custom key store , CloudHSM integration with KMS (newer feature) CloudHSM KMS FIPS 140-2 level3 FIPS 140-2 level 2 (some L3) Industry Standard API - PKCS#11 ,Java Cryptography Extenstion ( JCE ), Microsoft CryptoNG (CNG) libraries Communicated via AWS API Configure HSM in cluster mode to ensure HA mode (multi AZ). By default, it does not run on HA mode.","title":"CloudHSM"},{"location":"security-config/cloud-hsm/#use-cases","text":"No native AWS integration .. e.g. no s3 SSE client side encryption can be used before uploading Offload the SSL/TLS Processing of Web Servers. Enable Transparent Data Encryption (TDE) for Oracle Databases Protect the Private Keys for an Issuing Certificate Authority (CA)","title":"Use Cases"},{"location":"security-config/config/","text":"AWS Config AWS Config is a service which records the configuration of resources over time (configuration items) into configuration histories. Record configuration changes over time on resources Auditing of changes, compliance with standards Deos not prevent changes from happening .. no protection - Regional Service .. supoorts cross region and account aggregation Changes can generate SNS Notifications and near-realtime events via EventBridge and Lambda All the config is stored regionally in s3 bucket which can be interacted with aws config api","title":"AWS Config"},{"location":"security-config/config/#aws-config","text":"AWS Config is a service which records the configuration of resources over time (configuration items) into configuration histories. Record configuration changes over time on resources Auditing of changes, compliance with standards Deos not prevent changes from happening .. no protection - Regional Service .. supoorts cross region and account aggregation Changes can generate SNS Notifications and near-realtime events via EventBridge and Lambda All the config is stored regionally in s3 bucket which can be interacted with aws config api","title":"AWS Config"},{"location":"security-config/guardduty/","text":"Amazon GuardDuty Guard Duty is an automatic threat detection service which reviews data from supported services and attempts to identify any events outside of the 'norm' for a given AWS account or Accounts. Continous security monitoring service - once enabled Analyses supported Data Sources uses AL/ML and threat intelligence feeds Identifies unexpected and unauthorized activity either notify or event-driven protection/remediation Support multiple accounts ( MASTER and MEMBER ) AL/ML - Artificial Intelligence/ Machine Learning","title":"AWS Guard Duty"},{"location":"security-config/guardduty/#amazon-guardduty","text":"Guard Duty is an automatic threat detection service which reviews data from supported services and attempts to identify any events outside of the 'norm' for a given AWS account or Accounts. Continous security monitoring service - once enabled Analyses supported Data Sources uses AL/ML and threat intelligence feeds Identifies unexpected and unauthorized activity either notify or event-driven protection/remediation Support multiple accounts ( MASTER and MEMBER ) AL/ML - Artificial Intelligence/ Machine Learning","title":"Amazon GuardDuty"},{"location":"security-config/inspector/","text":"AWS Inspector Scans EC2 instances and the instance OS scans for vulnerabilities and deviations against best practice Assessment varying period.. 15mins, 1 hours, 8/12 hours or 1 day Provides a report of findings ordered by priority Network Assessment ( Agentless ) Network + Host Assessment ( Agent ) Rules packages determine what is checked Network Reachibility package ( no agent required ) agent can provide additional OS visibility Check reachibility end to end. EC2, ALB, DX, ELB, ENI IGW, ACLs, RT's, SG's, Subnets, VPCs, VGWs & VPC Peering Network Reachability package findings RecognizedPortWithListener : exposed to public and OS listening to the port RecognizedPortNoListener : exposed to public but OS not listening RecognizedPortNoAgent : exposed but no agent to confirm if OS is listening UnrecognizedPortWithListener : unrecognized port exposed and OS listening to the port Host Assessment package ( agent required ) CVE - Common Vulnerabilities & Exposures CIS benchmarks - Center for Internet Security Security best practices for Amazon Inspector","title":"AWS Inspector"},{"location":"security-config/inspector/#aws-inspector","text":"Scans EC2 instances and the instance OS scans for vulnerabilities and deviations against best practice Assessment varying period.. 15mins, 1 hours, 8/12 hours or 1 day Provides a report of findings ordered by priority Network Assessment ( Agentless ) Network + Host Assessment ( Agent ) Rules packages determine what is checked Network Reachibility package ( no agent required ) agent can provide additional OS visibility Check reachibility end to end. EC2, ALB, DX, ELB, ENI IGW, ACLs, RT's, SG's, Subnets, VPCs, VGWs & VPC Peering Network Reachability package findings RecognizedPortWithListener : exposed to public and OS listening to the port RecognizedPortNoListener : exposed to public but OS not listening RecognizedPortNoAgent : exposed but no agent to confirm if OS is listening UnrecognizedPortWithListener : unrecognized port exposed and OS listening to the port Host Assessment package ( agent required ) CVE - Common Vulnerabilities & Exposures CIS benchmarks - Center for Internet Security Security best practices for Amazon Inspector","title":"AWS Inspector"},{"location":"security-config/key-management-service/","text":"AWS KMS Key Management Service Regional and Public Service Create, Store and Manage Keys Supports both symmetric and asymmetric keys Also supports cryptographic operations - encrypt , decrypt & ... Keys never leave KMS - Provides FIPS 140-2 (L2) FIPS 140-2 & FIPS 140 -3 Customer Master Keys (CMK) CMK - C ustomer M aster K ey CMK is logical - ID, date, policy, desc and date backed by physical key material key material can be generated or imported CMK can be used ( encrypt/decrypt ) for up to 4KB of data CMK is encrypted by AWS before storing in the disk. Data Encryption Keys (DEKs) Workaround for 4KB data limitation GenerateDataKey - works on > 4KB linked to specific CMK KMS does not store DEK in any way generates and discard it once user uses it DEK is provided in 2 versions Plaintext version Ciphertext version Encrypt data using plaintext key then discard the plaintext key Store encrypted key with encrypted ddata S3 create DEK for every object Key Concepts CMKs are isolated to a region and never leave AWS Managed or Customer Managed CMKs Customer keys and AWS keys Customer Managed Keys are more configurable Both types of keys support rotation Type of KMS key Can view KMS key metadata Can manage KMS key Used only for my AWS account Automatic rotation Customer managed key Yes Yes Yes Optional. Every 365 days (1 year). AWS managed key Yes No Yes Required. Every 1095 days (3 years). AWS owned key No No No Varies CMK itself contains Backing Key as well as previous backing keys Aliases for CMKs - Per Region Key Policies and Securities Key Policies (Resource) Every CMK has a key policy Customer manage CMK policy can be adjusted CMK key policy explicity told to trust AWS account IAM policies to ensure IAM role/user have access to KMS operation on the key","title":"Key Management Service"},{"location":"security-config/key-management-service/#aws-kms","text":"","title":"AWS KMS"},{"location":"security-config/key-management-service/#key-management-service","text":"Regional and Public Service Create, Store and Manage Keys Supports both symmetric and asymmetric keys Also supports cryptographic operations - encrypt , decrypt & ... Keys never leave KMS - Provides FIPS 140-2 (L2) FIPS 140-2 & FIPS 140 -3","title":"Key Management Service"},{"location":"security-config/key-management-service/#customer-master-keys-cmk","text":"CMK - C ustomer M aster K ey CMK is logical - ID, date, policy, desc and date backed by physical key material key material can be generated or imported CMK can be used ( encrypt/decrypt ) for up to 4KB of data CMK is encrypted by AWS before storing in the disk.","title":"Customer Master Keys (CMK)"},{"location":"security-config/key-management-service/#data-encryption-keys-deks","text":"Workaround for 4KB data limitation GenerateDataKey - works on > 4KB linked to specific CMK KMS does not store DEK in any way generates and discard it once user uses it DEK is provided in 2 versions Plaintext version Ciphertext version Encrypt data using plaintext key then discard the plaintext key Store encrypted key with encrypted ddata S3 create DEK for every object","title":"Data Encryption Keys (DEKs)"},{"location":"security-config/key-management-service/#key-concepts","text":"CMKs are isolated to a region and never leave AWS Managed or Customer Managed CMKs Customer keys and AWS keys Customer Managed Keys are more configurable Both types of keys support rotation Type of KMS key Can view KMS key metadata Can manage KMS key Used only for my AWS account Automatic rotation Customer managed key Yes Yes Yes Optional. Every 365 days (1 year). AWS managed key Yes No Yes Required. Every 1095 days (3 years). AWS owned key No No No Varies CMK itself contains Backing Key as well as previous backing keys Aliases for CMKs - Per Region","title":"Key Concepts"},{"location":"security-config/key-management-service/#key-policies-and-securities","text":"Key Policies (Resource) Every CMK has a key policy Customer manage CMK policy can be adjusted CMK key policy explicity told to trust AWS account IAM policies to ensure IAM role/user have access to KMS operation on the key","title":"Key Policies and Securities"},{"location":"security-config/parameter-store/","text":"AWS Systems Manager Parameter Store Storage for configuration and stores Different types of parameters to be stored String StringList SecureString Ex: Licencse codes, Database Strings, Full Configs & Passwords Hierrarchies & Versioning Allows to store parameters using Hierrarchial Order Allows to store different versions the parameter Can strore both Plaintext and Ciphertext KMS in turn can use Ciphertext Public Parameters are available. E.x. - Latest AMI per region","title":"AWS Parameter Store"},{"location":"security-config/parameter-store/#aws-systems-manager-parameter-store","text":"Storage for configuration and stores Different types of parameters to be stored String StringList SecureString Ex: Licencse codes, Database Strings, Full Configs & Passwords Hierrarchies & Versioning Allows to store parameters using Hierrarchial Order Allows to store different versions the parameter Can strore both Plaintext and Ciphertext KMS in turn can use Ciphertext Public Parameters are available. E.x. - Latest AMI per region","title":"AWS Systems Manager Parameter Store"},{"location":"security-config/secrets-manager/","text":"AWS Secrets Manager Shares functionality with Parameter Store Designed for secrets .. (passwords, API Keys) Usable via Console, CLI, API or SDK's (integration) Supports automatic rotation .. this uses Lambda Directly integrates with some AWS produts like RDS sync the authectication creds as well Permissions controlled by IAM Secrets are encrypted using KMS","title":"AWS Secrets Manager"},{"location":"security-config/secrets-manager/#aws-secrets-manager","text":"Shares functionality with Parameter Store Designed for secrets .. (passwords, API Keys) Usable via Console, CLI, API or SDK's (integration) Supports automatic rotation .. this uses Lambda Directly integrates with some AWS produts like RDS sync the authectication creds as well Permissions controlled by IAM Secrets are encrypted using KMS","title":"AWS Secrets Manager"},{"location":"security-config/vpc-flowlogs/","text":"VPC Flow Logs Capture packet Metadata .. NOT packet contents Different monitoring points to apply... VPC - all interfaces in the VPC Subnet - all interfaces in that subnet Interface directly VPC Flow Logs are NOT realtime Destination can be S3 or Cloudwatch Logs","title":"VPC Flow Logs"},{"location":"security-config/vpc-flowlogs/#vpc-flow-logs","text":"Capture packet Metadata .. NOT packet contents Different monitoring points to apply... VPC - all interfaces in the VPC Subnet - all interfaces in that subnet Interface directly VPC Flow Logs are NOT realtime Destination can be S3 or Cloudwatch Logs","title":"VPC Flow Logs"},{"location":"security-config/waf-shield/","text":"AWS Shield Provides AWS resources with DDoS protection AWS Shield Standard AWS Shield Advance Free with Route53 and CloudFront $3000/month/organization which also supports - EC2, ELB, Global Accelerator in addition to Cloudfront & R53 Protect against Layer 3 and Layer 4 DDoS attacks Protect against Layer 3 and Layer 4 DDoS attacks Provides 24/7 (365 days) advance response team to deal with DDoS atack Financial Insurance for any increased AWS cost incurred by the attack AWS WAF Web Application Firewall Layer 7 (HTTP/s) Firewall Protect against complex Layer 7 attacks/exploits SQL Injections, Cross-Site Scripting , Geo Blocks, Rate Awareness Web Access Control List ( WEBACL ) integrated with ALB, API Gateway and CloudFront WAF provides WEBACL Rules are added to WEBACL and evaluated when traffic arrives WAF Rules WEBACL => RULE GROUPS* => RULES WEBACL Capacity Units (WCU) Simpler rule uses fewer WCU More complex rule uses more WCU Limit on how many WCU on WEBACL Charged for following.. WEBACL - monthly fee for each ACL Rules - monthly fee for each rule Based on no. of Requests WEBACL has default action ALLOW or BLOCK COUNT - how many times a rule is matched Come with 2 different types of rules Regular Rule Rate-Based Rule - frequency at which an IP could go through ACL Conditions ( Layer 7 attributes ) for rule can be based on IP Addresses Country or GEO Length of the request SQL code in the request Strings in the request Scripts and headers in the request AWS Shield and WAF in action Exam Points AWS Shield AWS WAF DDoS Layer 7 (HTTP/HTTPS) Rate Limiter and Layer 7 aspects","title":"AWS Shield & WAF"},{"location":"security-config/waf-shield/#aws-shield","text":"Provides AWS resources with DDoS protection AWS Shield Standard AWS Shield Advance Free with Route53 and CloudFront $3000/month/organization which also supports - EC2, ELB, Global Accelerator in addition to Cloudfront & R53 Protect against Layer 3 and Layer 4 DDoS attacks Protect against Layer 3 and Layer 4 DDoS attacks Provides 24/7 (365 days) advance response team to deal with DDoS atack Financial Insurance for any increased AWS cost incurred by the attack","title":"AWS Shield"},{"location":"security-config/waf-shield/#aws-waf","text":"Web Application Firewall Layer 7 (HTTP/s) Firewall Protect against complex Layer 7 attacks/exploits SQL Injections, Cross-Site Scripting , Geo Blocks, Rate Awareness Web Access Control List ( WEBACL ) integrated with ALB, API Gateway and CloudFront WAF provides WEBACL Rules are added to WEBACL and evaluated when traffic arrives WAF Rules WEBACL => RULE GROUPS* => RULES WEBACL Capacity Units (WCU) Simpler rule uses fewer WCU More complex rule uses more WCU Limit on how many WCU on WEBACL Charged for following.. WEBACL - monthly fee for each ACL Rules - monthly fee for each rule Based on no. of Requests WEBACL has default action ALLOW or BLOCK COUNT - how many times a rule is matched Come with 2 different types of rules Regular Rule Rate-Based Rule - frequency at which an IP could go through ACL Conditions ( Layer 7 attributes ) for rule can be based on IP Addresses Country or GEO Length of the request SQL code in the request Strings in the request Scripts and headers in the request","title":"AWS WAF"},{"location":"security-config/waf-shield/#aws-shield-and-waf-in-action","text":"","title":"AWS Shield and WAF in action"},{"location":"security-config/waf-shield/#exam-points","text":"AWS Shield AWS WAF DDoS Layer 7 (HTTP/HTTPS) Rate Limiter and Layer 7 aspects","title":"Exam Points"},{"location":"serverless/serverless-application-model/","text":"Serverless Application Model - SAM open-source framework for building serverless appliations AWS SAM template specification extenstion of CloudFormation transform, globals & resources (CFN & SAM) SAM CLI locally build SAM apps local testing deploy to AWS Notes SES begins with sandbox mode in sandbox will require whitelisting email addresses either verify with individual email or remove it from sandbox (takes longer)","title":"AWS SAM"},{"location":"serverless/serverless-application-model/#serverless-application-model-sam","text":"open-source framework for building serverless appliations AWS SAM template specification extenstion of CloudFormation transform, globals & resources (CFN & SAM) SAM CLI locally build SAM apps local testing deploy to AWS","title":"Serverless Application Model - SAM"},{"location":"serverless/serverless-application-model/#notes","text":"SES begins with sandbox mode in sandbox will require whitelisting email addresses either verify with individual email or remove it from sandbox (takes longer)","title":"Notes"},{"location":"serverless/step-functions/","text":"AWS Step Functions - Long running serverless workflows Limitations of lambda lambda -> FAAS 15 min - max execution time functions can be chained together. (scaling is tedious and difficult to maintain) runtime environments are stateless State Machines Serverless workflow ... START -> STATES -> END States are THINGS which occur Maximum duration - 1 year Standard Workflow & Express Workflow Started via API gateway, IOT rules, EventBridge, Lambda ... Amazon State Language (ASL) - JSON Template IAM Role is used for permissios","title":"Step Functions"},{"location":"serverless/step-functions/#limitations-of-lambda","text":"lambda -> FAAS 15 min - max execution time functions can be chained together. (scaling is tedious and difficult to maintain) runtime environments are stateless","title":"Limitations of lambda"},{"location":"serverless/step-functions/#state-machines","text":"Serverless workflow ... START -> STATES -> END States are THINGS which occur Maximum duration - 1 year Standard Workflow & Express Workflow Started via API gateway, IOT rules, EventBridge, Lambda ... Amazon State Language (ASL) - JSON Template IAM Role is used for permissios","title":"State Machines"},{"location":"storage-services/ebs/","text":"General Purpose - SSD 1 IOPS => 1 IO (16KB) in 1 second Initial capacity - 5.4 million IO Credits => 3000 IOPs great for boots and initial workloads gp3 is cheaper than gp2 for base performance gp2 gp3 Volume 1GB-16TB 1GB-16TB IOPs credit Baseline performance (100 IO credits/s) + 3 IO Credits/s per GB Volumes > 1000GB does not use credit system no credit bucket system Baseline performance - 3000 IOPS + 125 MiB/s Extra cost upto 16,000 and 1000 MiB/s Max IOPS per volume (16KiB I/O) 16,000 16,000 Max throughput per volume 250 MiB/s * 1,000 MiB/s Boot volume Supported Supported Use Cases Low-latency interactive apps Development and test environments + virutal machine single instance DB Amazon EBS Multi-attach Not supported Not supported Provisioned IOPS SSD (io1/2) IOPS can be adjusted independently of size Consistent Low Latency & jitter block express io1/io2 Volume 4GB- 64TB 4GB-16TB Max IOPS per volume (16KiB I/O) 256,000 64,000 Amazon EBS Multi-attach Supported Supported Max throughput per volume 4000 MiB/s * 1,000 MiB/s Use Cases Workloads that require: Sub-millisecond latency Sustained IOPS performance More than 64,000 IOPS or 1,000 MiB/s of throughput Workloads that require sustained IOPS performance or more than 16,000 IOPS I/O-intensive database workloads Hard Disk Drive (HDD) st2 (throughput optimized) and sc2 (cold HDD) Instance Store An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data , and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Block Storage Devices Physically connected to one EC2 Host Instances on that host can access them Highest storage performance in AWS already included with the instance price CAN ONLY BE ATTACHED AT LAUNCH TIME Instances moving between hosts will lose the access to the ephemeral volumes if hardware hosting the ephemeral volume is lost, the data in the volume is also lost NOT all the instances support instance store volumes, larger the size of the instance, larger no. of instance volumes can be allocated EBS vs Instance Store EBS Instance Store Persistence Reslience Storage isolated from instance lifecycle Resilence w/App In-built Replication... ... it depends High performance needs... ... it depends Super high performance needs Cost already included within instance price Cheap = ST1 or SC1 Throughput..streaming - ST1 Boot - not ST1 or SC1 GP2/3 - up to 16,000 IOPS io1/2 - up to 64,000 IOPS (*256,000) RAID0 + EBS up to 260,000 IOPS (io1/2-BE/GP2GP3) raid-config-options limited by instance size + EBS More than 260,000 IOPS - INSTANCE STORE","title":"EBS"},{"location":"storage-services/ebs/#general-purpose-ssd","text":"1 IOPS => 1 IO (16KB) in 1 second Initial capacity - 5.4 million IO Credits => 3000 IOPs great for boots and initial workloads gp3 is cheaper than gp2 for base performance gp2 gp3 Volume 1GB-16TB 1GB-16TB IOPs credit Baseline performance (100 IO credits/s) + 3 IO Credits/s per GB Volumes > 1000GB does not use credit system no credit bucket system Baseline performance - 3000 IOPS + 125 MiB/s Extra cost upto 16,000 and 1000 MiB/s Max IOPS per volume (16KiB I/O) 16,000 16,000 Max throughput per volume 250 MiB/s * 1,000 MiB/s Boot volume Supported Supported Use Cases Low-latency interactive apps Development and test environments + virutal machine single instance DB Amazon EBS Multi-attach Not supported Not supported","title":"General Purpose - SSD"},{"location":"storage-services/ebs/#provisioned-iops-ssd-io12","text":"IOPS can be adjusted independently of size Consistent Low Latency & jitter block express io1/io2 Volume 4GB- 64TB 4GB-16TB Max IOPS per volume (16KiB I/O) 256,000 64,000 Amazon EBS Multi-attach Supported Supported Max throughput per volume 4000 MiB/s * 1,000 MiB/s Use Cases Workloads that require: Sub-millisecond latency Sustained IOPS performance More than 64,000 IOPS or 1,000 MiB/s of throughput Workloads that require sustained IOPS performance or more than 16,000 IOPS I/O-intensive database workloads","title":"Provisioned IOPS SSD (io1/2)"},{"location":"storage-services/ebs/#hard-disk-drive-hdd","text":"st2 (throughput optimized) and sc2 (cold HDD)","title":"Hard Disk Drive (HDD)"},{"location":"storage-services/ebs/#instance-store","text":"An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data , and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Block Storage Devices Physically connected to one EC2 Host Instances on that host can access them Highest storage performance in AWS already included with the instance price CAN ONLY BE ATTACHED AT LAUNCH TIME Instances moving between hosts will lose the access to the ephemeral volumes if hardware hosting the ephemeral volume is lost, the data in the volume is also lost NOT all the instances support instance store volumes, larger the size of the instance, larger no. of instance volumes can be allocated","title":"Instance Store"},{"location":"storage-services/ebs/#ebs-vs-instance-store","text":"EBS Instance Store Persistence Reslience Storage isolated from instance lifecycle Resilence w/App In-built Replication... ... it depends High performance needs... ... it depends Super high performance needs Cost already included within instance price Cheap = ST1 or SC1 Throughput..streaming - ST1 Boot - not ST1 or SC1 GP2/3 - up to 16,000 IOPS io1/2 - up to 64,000 IOPS (*256,000) RAID0 + EBS up to 260,000 IOPS (io1/2-BE/GP2GP3) raid-config-options limited by instance size + EBS More than 260,000 IOPS - INSTANCE STORE","title":"EBS vs Instance Store"},{"location":"storage-services/file-system/","text":"FSx for Windows File Server Fully managed native windows file servers/shares - not emulated Designed for integration with windows evironments Integrates with Directory Service or Self-Managed AD Single or Multi-AZ within a VPC On-demand and Scheduled Backups Accesible using VPC, Peering, VPN, Direct Connect Advanced shared file system accessible over SMB (Server Message Block) Native windows file system, supports de-duplication (sub file), Distributed File System (DFS), KMS at-rest encryption and enforced encryption in-transit VSS - User-Driven Restores (Windows stuff) Windows permission model Managed - no file server admin FSx for Lustre Managed Lustre - Designed for HPC (High Perfromance Computing) LINUX clients ( POSIX ) Machine Learning, Big Data, Financial Modeling 100's GB/s throughput & sub millisecond latency Deployment types - Persistent or Scratch Scratch - Highly optmised for Short term with no replication but fast - pure performance Persistent - longer term, HA (in one AZ) , self-healing with replication Accessible over VPN or Direct Connect S3 can be set as repository for file system and data is lazy loaded and hsm_archive can be used to export data back to S3. NO AUTOMATIC SYNCHRONIZATION Backup to S3 with both deployment types (Manual of Automatic 0-35 day retention) Data Storage Metadata stored on Metadata Targets ( MDTs ) Objects are stored on called object storage targets ( OSTs ) ( 1.17TiB ) Baseline Performance based on size Size - min 1.2TiB then increments of 2.4TiB For Scratch - Base 200 MB/s per TiB of storage Persistent offers 50MB/s, 100Mb/s and 200MB/s per TiB of storage Burst upto 1300 Mb/s per TiB (Credit systems) Elastic File System (EFS) NFS - Network File System EFS is an implementation of NFSv4 - Linux Only EFS Filesystems can be mounted in Linux Shared between many EC2 instances Private service, integrated via mount targets inside a VPC mutliple mount targets in multiple AZ for HA Access via - VPN or DX General Purpose and Max I/O Performance Modes Bursting and Provisioned Throughput Modes Standard and Infrequent Access (IA) Classes Lifecycle Policies can be used to move between the classes","title":"File Systems"},{"location":"storage-services/file-system/#fsx-for-windows-file-server","text":"Fully managed native windows file servers/shares - not emulated Designed for integration with windows evironments Integrates with Directory Service or Self-Managed AD Single or Multi-AZ within a VPC On-demand and Scheduled Backups Accesible using VPC, Peering, VPN, Direct Connect Advanced shared file system accessible over SMB (Server Message Block) Native windows file system, supports de-duplication (sub file), Distributed File System (DFS), KMS at-rest encryption and enforced encryption in-transit VSS - User-Driven Restores (Windows stuff) Windows permission model Managed - no file server admin","title":"FSx for Windows File Server"},{"location":"storage-services/file-system/#fsx-for-lustre","text":"Managed Lustre - Designed for HPC (High Perfromance Computing) LINUX clients ( POSIX ) Machine Learning, Big Data, Financial Modeling 100's GB/s throughput & sub millisecond latency Deployment types - Persistent or Scratch Scratch - Highly optmised for Short term with no replication but fast - pure performance Persistent - longer term, HA (in one AZ) , self-healing with replication Accessible over VPN or Direct Connect S3 can be set as repository for file system and data is lazy loaded and hsm_archive can be used to export data back to S3. NO AUTOMATIC SYNCHRONIZATION Backup to S3 with both deployment types (Manual of Automatic 0-35 day retention) Data Storage Metadata stored on Metadata Targets ( MDTs ) Objects are stored on called object storage targets ( OSTs ) ( 1.17TiB ) Baseline Performance based on size Size - min 1.2TiB then increments of 2.4TiB For Scratch - Base 200 MB/s per TiB of storage Persistent offers 50MB/s, 100Mb/s and 200MB/s per TiB of storage Burst upto 1300 Mb/s per TiB (Credit systems)","title":"FSx for Lustre"},{"location":"storage-services/file-system/#elastic-file-system-efs","text":"NFS - Network File System EFS is an implementation of NFSv4 - Linux Only EFS Filesystems can be mounted in Linux Shared between many EC2 instances Private service, integrated via mount targets inside a VPC mutliple mount targets in multiple AZ for HA Access via - VPN or DX General Purpose and Max I/O Performance Modes Bursting and Provisioned Throughput Modes Standard and Infrequent Access (IA) Classes Lifecycle Policies can be used to move between the classes","title":"Elastic File System (EFS)"},{"location":"storage-services/macie/","text":"Overview Data Security and Data Privacy Service Discover, Monitor and Protect Data .. stored in S3 buckets Automated discovery of data PII - Personal Identifiable Information PHI - Personal Health Information Finance Managed data identifiers - Built-in - ML/Patterns maintained by AWS credentials, finanace, health, personal identifiers Custom data identifiers - Proprietray - Regex Based created by customer Regex - defines a pattern to mach in data e.g. [A-Z]-\\d{8} Keywords - optional sequences that need to be in proximity to regex match Maximum Match Distance - how close keywords are to regex pattern Ignore words - if regex match contains ingore words, it's ignored Both identifiers generates findings which can be dealt with interactively integrates with Security Hub finding events to EventBridge Centrally Managed account - either via AWS ORG or one Macie Account Inviting Discovery jobs with data identifiers (managed and/or customer) runs on s3 buckets and generates findings Policy Findings - related to policy update Policy:IAMUser/ S3BlockPublicAccessDisabled Policy:IAMUser/ S3BucketEncryptionDisabled Policy:IAMUser/ S3BucketPublic Policy:IAMUser/ S3BucketSharedExternally Sensitive data findings - any sensitive data in buckets SensitiveData: S3Object/Credentials SensitiveData: S3Object/CustomIdentifier SensitiveData: S3Object/Financial SensitiveData: S3Object/Multiple SensitiveData: S3Object/Personal","title":"AWS Macie"},{"location":"storage-services/macie/#overview","text":"Data Security and Data Privacy Service Discover, Monitor and Protect Data .. stored in S3 buckets Automated discovery of data PII - Personal Identifiable Information PHI - Personal Health Information Finance Managed data identifiers - Built-in - ML/Patterns maintained by AWS credentials, finanace, health, personal identifiers Custom data identifiers - Proprietray - Regex Based created by customer Regex - defines a pattern to mach in data e.g. [A-Z]-\\d{8} Keywords - optional sequences that need to be in proximity to regex match Maximum Match Distance - how close keywords are to regex pattern Ignore words - if regex match contains ingore words, it's ignored Both identifiers generates findings which can be dealt with interactively integrates with Security Hub finding events to EventBridge Centrally Managed account - either via AWS ORG or one Macie Account Inviting Discovery jobs with data identifiers (managed and/or customer) runs on s3 buckets and generates findings Policy Findings - related to policy update Policy:IAMUser/ S3BlockPublicAccessDisabled Policy:IAMUser/ S3BucketEncryptionDisabled Policy:IAMUser/ S3BucketPublic Policy:IAMUser/ S3BucketSharedExternally Sensitive data findings - any sensitive data in buckets SensitiveData: S3Object/Credentials SensitiveData: S3Object/CustomIdentifier SensitiveData: S3Object/Financial SensitiveData: S3Object/Multiple SensitiveData: S3Object/Personal","title":"Overview"},{"location":"storage-services/s3/","text":"Storage Object Classes By default, objects are replicated across at least 3 AZs 200 OK for successful response TRANSFER IN is free but $ per GB is charged for TRANSFER OUT + a price per 1000 requests Comparing the Amazon S3 storage classes Storage class Designed for Durability (designed for) Availability (designed for) Availability Zones Min storage duration Min billable object size Other considerations S3 Standard Frequently accessed data (more than once a month) with millsecond access 99.999999999% 99.99% >= 3 None None None S3 Standard-IA Long-lived, infrequently accessed data (once a month) with millisecond access 99.999999999% 99.9% >= 3 30 days 128 KB Per GB retrieval fees apply. S3 Intelligent-Tiering Data with unknown, changing, or unpredictable access patterns 99.999999999% 99.9% >= 3 None None Monitoring and automation fees per object apply. No retrieval fees. S3 One Zone-IA Recreatable, infrequently accessed data (once a month) with millisecond access 99.999999999% 99.5% 1 30 days 128 KB Per GB retrieval fees apply. Not resilient to the loss of the Availability Zone. S3 Glacier Instant Retrieval Long-lived, archive data accessed once a quarter with millisecond access 99.999999999% 99.9% >= 3 90 days 128 KB Per GB retrieval fees apply. S3 Glacier Flexible Retrieval Long-lived archive data accessed once a year with retrieval times of minutes to hours 99.999999999% 99.99% (after you restore objects) >= 3 90 days 40 KB Per GB retrieval fees apply. You must first restore archived objects before you can access them. For information, see Restoring an archived object . S3 Glacier Deep Archive Long-lived archive data accessed less than once a year with retrieval times of hours 99.999999999% 99.99% (after you restore objects) >= 3 180 days 40 KB Per GB retrieval fees apply. You must first restore archived objects before you can access them. For information, see Restoring an archived object . RRS (not recommended) Noncritical, frequently accessed data with millisecond access 99.99% 99.99% >= 3 None None None S3 Standard, Standard IA, S3 One Zone-IA S3 Glacier Objects cannot be made publicly accessible any access of data (beyond object metadata) requires a retrieval process S3 Intelligent-Tiering S3 Lifecycle Configuration Set of rules on a bucket or groups of objects Actions - tranisition or expiration Cost management to automatically move one storage class to another or expire objects when no longer needed Downward directions for transition considerations smaller objects can cost more - minimumm size limitation Minimum of 30 days in standard before transition Single rule cannot transition from Starndard-IA to S-IA,, intelligent or OneZone-IA and then to either glacier type .. within 30 days (duration minimums) - Two different rules can achive this S3 Replication CRR - Cross Region Replication SRR - Same Region Replication Replication configuration applies to source bucket IAM role is assigned for replcation - with trust to s3.amazonaws.com permission policy to move objects in both buckets For replication between different accounts destination bucket should have proper bucket policy to allow IAM role in source bucket acccount to write and replicate objects Replicaiton could be applied to all objects or a subset of objects (filter with prefix and/or tags ) Storage class in destination bucket - default to same as source bucket (could be configured) Ownership of the destination objects - deafult is the soruce account (could be changed to different destination account) Replication Time Control (RTC) - 15 mins ETA Only applies to the objects after the replication is enabled and VERSIONING needs to be enabled in both buckets One-way replication - from source to destination Handles unencrypted objects as well as objects encrypted with SSE-S3 & SSE-KMS (with extra config) Source bucket owner needs permissions to objects Does not replicate - systems events, Glacier or Glacier Deep Archive Does not replicate DELETE Why use replication .. ? SRR CRR Log aggregation Global Resiliency Improvements Prod and Test Sync Latency Reduction Resilience with strict sovereignty - regional S3 Object encryption Encryption at-rest Client-Side Server-side Encrypted data is sent from client to S3 Data is only encrypted from s3 endpoint. Data is not encrypted (at-rest) before uploading to s3 endpoint Key management is entirely done by customer Key management is partially/entirely done by S3 (AWS) Server-Side encryption SSE-C - customer-provided keys customer uploads object with keys s3 encrypts and decrypts before uploading or downloading the objects reduces the overhead of encryption and decryption on the customer side SSE-S3 - amazon S3-managed keys customer uploads object S3 uses master key to create a unique Key to encrypt the object - AES256 S3 then encrypt the unique Key and the original unique Key is discarded both encrypted key and object is now stored in the S3 customer has no control over master and encrypted keys SSE-KMS - customer master keys ( CMKs ) stored in AWS KMS service Segregation of duties - sysops vs admin Bucket Default Encryption - set DEFAULT=AES256 if x-amz-server-side-encrypion is not provided - use bucket policy to enforce object encryption for the bucket S3 Presigned URLs","title":"S3"},{"location":"storage-services/s3/#storage-object-classes","text":"By default, objects are replicated across at least 3 AZs 200 OK for successful response TRANSFER IN is free but $ per GB is charged for TRANSFER OUT + a price per 1000 requests","title":"Storage Object Classes"},{"location":"storage-services/s3/#comparing-the-amazon-s3-storage-classes","text":"Storage class Designed for Durability (designed for) Availability (designed for) Availability Zones Min storage duration Min billable object size Other considerations S3 Standard Frequently accessed data (more than once a month) with millsecond access 99.999999999% 99.99% >= 3 None None None S3 Standard-IA Long-lived, infrequently accessed data (once a month) with millisecond access 99.999999999% 99.9% >= 3 30 days 128 KB Per GB retrieval fees apply. S3 Intelligent-Tiering Data with unknown, changing, or unpredictable access patterns 99.999999999% 99.9% >= 3 None None Monitoring and automation fees per object apply. No retrieval fees. S3 One Zone-IA Recreatable, infrequently accessed data (once a month) with millisecond access 99.999999999% 99.5% 1 30 days 128 KB Per GB retrieval fees apply. Not resilient to the loss of the Availability Zone. S3 Glacier Instant Retrieval Long-lived, archive data accessed once a quarter with millisecond access 99.999999999% 99.9% >= 3 90 days 128 KB Per GB retrieval fees apply. S3 Glacier Flexible Retrieval Long-lived archive data accessed once a year with retrieval times of minutes to hours 99.999999999% 99.99% (after you restore objects) >= 3 90 days 40 KB Per GB retrieval fees apply. You must first restore archived objects before you can access them. For information, see Restoring an archived object . S3 Glacier Deep Archive Long-lived archive data accessed less than once a year with retrieval times of hours 99.999999999% 99.99% (after you restore objects) >= 3 180 days 40 KB Per GB retrieval fees apply. You must first restore archived objects before you can access them. For information, see Restoring an archived object . RRS (not recommended) Noncritical, frequently accessed data with millisecond access 99.99% 99.99% >= 3 None None None","title":"Comparing the Amazon S3 storage classes"},{"location":"storage-services/s3/#s3-standard-standard-ia-s3-one-zone-ia","text":"","title":"S3 Standard, Standard IA, S3 One Zone-IA"},{"location":"storage-services/s3/#s3-glacier","text":"Objects cannot be made publicly accessible any access of data (beyond object metadata) requires a retrieval process","title":"S3 Glacier"},{"location":"storage-services/s3/#s3-intelligent-tiering","text":"","title":"S3 Intelligent-Tiering"},{"location":"storage-services/s3/#s3-lifecycle-configuration","text":"Set of rules on a bucket or groups of objects Actions - tranisition or expiration Cost management to automatically move one storage class to another or expire objects when no longer needed Downward directions for transition considerations smaller objects can cost more - minimumm size limitation Minimum of 30 days in standard before transition Single rule cannot transition from Starndard-IA to S-IA,, intelligent or OneZone-IA and then to either glacier type .. within 30 days (duration minimums) - Two different rules can achive this","title":"S3 Lifecycle Configuration"},{"location":"storage-services/s3/#s3-replication","text":"CRR - Cross Region Replication SRR - Same Region Replication Replication configuration applies to source bucket IAM role is assigned for replcation - with trust to s3.amazonaws.com permission policy to move objects in both buckets For replication between different accounts destination bucket should have proper bucket policy to allow IAM role in source bucket acccount to write and replicate objects Replicaiton could be applied to all objects or a subset of objects (filter with prefix and/or tags ) Storage class in destination bucket - default to same as source bucket (could be configured) Ownership of the destination objects - deafult is the soruce account (could be changed to different destination account) Replication Time Control (RTC) - 15 mins ETA Only applies to the objects after the replication is enabled and VERSIONING needs to be enabled in both buckets One-way replication - from source to destination Handles unencrypted objects as well as objects encrypted with SSE-S3 & SSE-KMS (with extra config) Source bucket owner needs permissions to objects Does not replicate - systems events, Glacier or Glacier Deep Archive Does not replicate DELETE Why use replication .. ? SRR CRR Log aggregation Global Resiliency Improvements Prod and Test Sync Latency Reduction Resilience with strict sovereignty - regional","title":"S3 Replication"},{"location":"storage-services/s3/#s3-object-encryption","text":"Encryption at-rest Client-Side Server-side Encrypted data is sent from client to S3 Data is only encrypted from s3 endpoint. Data is not encrypted (at-rest) before uploading to s3 endpoint Key management is entirely done by customer Key management is partially/entirely done by S3 (AWS) Server-Side encryption SSE-C - customer-provided keys customer uploads object with keys s3 encrypts and decrypts before uploading or downloading the objects reduces the overhead of encryption and decryption on the customer side SSE-S3 - amazon S3-managed keys customer uploads object S3 uses master key to create a unique Key to encrypt the object - AES256 S3 then encrypt the unique Key and the original unique Key is discarded both encrypted key and object is now stored in the S3 customer has no control over master and encrypted keys SSE-KMS - customer master keys ( CMKs ) stored in AWS KMS service Segregation of duties - sysops vs admin Bucket Default Encryption - set DEFAULT=AES256 if x-amz-server-side-encrypion is not provided - use bucket policy to enforce object encryption for the bucket","title":"S3 Object encryption"},{"location":"storage-services/s3/#s3-presigned-urls","text":"","title":"S3 Presigned URLs"},{"location":"storage-services/s3/access-points/","text":"Amazon S3 Access Points , a feature of S3, simplifies managing data access at scale for applications using shared data sets on S3. Access points are unique hostnames that customers create to enforce distinct permissions and network controls for any request made through the access point. Simplify managing access to S3 Buckets/Objects By default, 1 bucket with 1 bucket policy create many access points each with different policies each with different netwrork access controls Each access point has its own endpoint address as s3control create-access-point - cli or console aws s3control create-access-point \\ --account-id 123456789012 \\ --bucket business-records \\ --name finance-ap","title":"Access points"},{"location":"storage-services/s3/encryption/","text":"S3 Object encryption Encryption at-rest Client-Side Server-side Encrypted data is sent from client to S3 Data is only encrypted from s3 endpoint. Data is not encrypted (at-rest) before uploading to s3 endpoint Key management is entirely done by customer Key management is partially/entirely done by S3 (AWS) Server-Side encryption SSE-C - customer-provided keys customer uploads object with keys s3 encrypts and decrypts before uploading or downloading the objects reduces the overhead of encryption and decryption on the customer side SSE-S3 - amazon S3-managed keys customer uploads object S3 uses master key to create a unique Key to encrypt the object - AES256 S3 then encrypt the unique Key and the original unique Key is discarded both encrypted key and object is now stored in the S3 customer has no control over master and encrypted keys SSE-KMS - customer master keys ( CMKs ) stored in AWS KMS service Segregation of duties - sysops vs admin Bucket Default Encryption - set DEFAULT=AES256 if x-amz-server-side-encrypion is not provided - use bucket policy to enforce object encryption for the bucket","title":"Object Encryption"},{"location":"storage-services/s3/encryption/#s3-object-encryption","text":"Encryption at-rest Client-Side Server-side Encrypted data is sent from client to S3 Data is only encrypted from s3 endpoint. Data is not encrypted (at-rest) before uploading to s3 endpoint Key management is entirely done by customer Key management is partially/entirely done by S3 (AWS) Server-Side encryption SSE-C - customer-provided keys customer uploads object with keys s3 encrypts and decrypts before uploading or downloading the objects reduces the overhead of encryption and decryption on the customer side SSE-S3 - amazon S3-managed keys customer uploads object S3 uses master key to create a unique Key to encrypt the object - AES256 S3 then encrypt the unique Key and the original unique Key is discarded both encrypted key and object is now stored in the S3 customer has no control over master and encrypted keys SSE-KMS - customer master keys ( CMKs ) stored in AWS KMS service Segregation of duties - sysops vs admin Bucket Default Encryption - set DEFAULT=AES256 if x-amz-server-side-encrypion is not provided - use bucket policy to enforce object encryption for the bucket","title":"S3 Object encryption"},{"location":"storage-services/s3/lifecycle-configuration/","text":"S3 Lifecycle Configuration Set of rules on a bucket or groups of objects Actions - tranisition or expiration Cost management to automatically move one storage class to another or expire objects when no longer needed Downward directions for transition considerations smaller objects can cost more - minimumm size limitation Minimum of 30 days in standard before transition Single rule cannot transition from Starndard-IA to S-IA,, intelligent or OneZone-IA and then to either glacier type .. within 30 days (duration minimums) - Two different rules can achive this","title":"Lifecycle Configuration"},{"location":"storage-services/s3/lifecycle-configuration/#s3-lifecycle-configuration","text":"Set of rules on a bucket or groups of objects Actions - tranisition or expiration Cost management to automatically move one storage class to another or expire objects when no longer needed Downward directions for transition considerations smaller objects can cost more - minimumm size limitation Minimum of 30 days in standard before transition Single rule cannot transition from Starndard-IA to S-IA,, intelligent or OneZone-IA and then to either glacier type .. within 30 days (duration minimums) - Two different rules can achive this","title":"S3 Lifecycle Configuration"},{"location":"storage-services/s3/object-lock/","text":"Overview Object Lock enabled on new buckets existing bucket needs to go via AWS request support versioning is enabled by default and once object lock is enabled, versioning cannnot be supspended or diabled the object lock WORM - Write Once Read Many No Delete, No Overwrite Requires versioning - invdividual versions are locked Can have both , one or the other , or none Retention Period Legal Hold A bucket can have default Object Lock Settings Retention Specify DAYS & YEARS - A Retention Period COMPLIANCE - Can't be adjusted, deleted, overwritten not even by the root user until the rentention period expires for compliance - financial records GOVERNANCE - special pemissions can be granted allowing lock settings to be adjusted s3:BypassGovernanceRetention - access x-amz-bypass-governance-retention:ture accidental deletion, testing mode for compliance mode Legal Hold Set on a object version - either ON or OFF no retention NO Deletes or Changes until removed s3:PutObjectLegalHold is required to add or remove Prevent accidental deletion of critical object versions","title":"Object lock"},{"location":"storage-services/s3/object-lock/#overview","text":"Object Lock enabled on new buckets existing bucket needs to go via AWS request support versioning is enabled by default and once object lock is enabled, versioning cannnot be supspended or diabled the object lock WORM - Write Once Read Many No Delete, No Overwrite Requires versioning - invdividual versions are locked Can have both , one or the other , or none Retention Period Legal Hold A bucket can have default Object Lock Settings","title":"Overview"},{"location":"storage-services/s3/object-lock/#retention","text":"Specify DAYS & YEARS - A Retention Period COMPLIANCE - Can't be adjusted, deleted, overwritten not even by the root user until the rentention period expires for compliance - financial records GOVERNANCE - special pemissions can be granted allowing lock settings to be adjusted s3:BypassGovernanceRetention - access x-amz-bypass-governance-retention:ture accidental deletion, testing mode for compliance mode","title":"Retention"},{"location":"storage-services/s3/object-lock/#legal-hold","text":"Set on a object version - either ON or OFF no retention NO Deletes or Changes until removed s3:PutObjectLegalHold is required to add or remove Prevent accidental deletion of critical object versions","title":"Legal Hold"},{"location":"storage-services/s3/presigned-urls/","text":"S3 Presigned URL Presigned URL's are a feature of S3 which allows the system to generate a URL with access permissions encoded into it, for a specific bucket and object, valid for a certain time period. Temporary acces to specific S3 object Presigned URL is generated using credentials from IAM user/role access to object is temporary access to object is also determined by access assigned to user/role used to generate the URL( current permission ) Can create a URL for an object you have no access too can create a URL for non-existing object as well Don't generate with a role.. URL stops working when temporary credentials expire (role session token has short-term validity) - use long-term indentity like IAM user Using CLI IAM creds from role could expire before the expiry time in the presigned URL aws s3 presign command aws s3 presign s3://<s3-bucket>/<object-key> --expires-in <time-in-seconds-to-expire>","title":"Presigned URLs"},{"location":"storage-services/s3/presigned-urls/#s3-presigned-url","text":"Presigned URL's are a feature of S3 which allows the system to generate a URL with access permissions encoded into it, for a specific bucket and object, valid for a certain time period. Temporary acces to specific S3 object Presigned URL is generated using credentials from IAM user/role access to object is temporary access to object is also determined by access assigned to user/role used to generate the URL( current permission ) Can create a URL for an object you have no access too can create a URL for non-existing object as well Don't generate with a role.. URL stops working when temporary credentials expire (role session token has short-term validity) - use long-term indentity like IAM user Using CLI IAM creds from role could expire before the expiry time in the presigned URL aws s3 presign command aws s3 presign s3://<s3-bucket>/<object-key> --expires-in <time-in-seconds-to-expire>","title":"S3 Presigned URL"},{"location":"storage-services/s3/replication/","text":"S3 Replication CRR - Cross Region Replication SRR - Same Region Replication Replication configuration applies to source bucket IAM role is assigned for replcation - with trust to s3.amazonaws.com permission policy to move objects in both buckets For replication between different accounts destination bucket should have proper bucket policy to allow IAM role in source bucket acccount to write and replicate objects Replicaiton could be applied to all objects or a subset of objects (filter with prefix and/or tags ) Storage class in destination bucket - default to same as source bucket (could be configured) Ownership of the destination objects - deafult is the soruce account (could be changed to different destination account) Replication Time Control (RTC) - 15 mins ETA Only applies to the objects after the replication is enabled and VERSIONING needs to be enabled in both buckets One-way replication - from source to destination Handles unencrypted objects as well as objects encrypted with SSE-S3 & SSE-KMS (with extra config) Source bucket owner needs permissions to objects Does not replicate - systems events, Glacier or Glacier Deep Archive Does not replicate DELETE Why use replication .. ? SRR CRR Log aggregation Global Resiliency Improvements Prod and Test Sync Latency Reduction Resilience with strict sovereignty - regional","title":"Replication"},{"location":"storage-services/s3/replication/#s3-replication","text":"CRR - Cross Region Replication SRR - Same Region Replication Replication configuration applies to source bucket IAM role is assigned for replcation - with trust to s3.amazonaws.com permission policy to move objects in both buckets For replication between different accounts destination bucket should have proper bucket policy to allow IAM role in source bucket acccount to write and replicate objects Replicaiton could be applied to all objects or a subset of objects (filter with prefix and/or tags ) Storage class in destination bucket - default to same as source bucket (could be configured) Ownership of the destination objects - deafult is the soruce account (could be changed to different destination account) Replication Time Control (RTC) - 15 mins ETA Only applies to the objects after the replication is enabled and VERSIONING needs to be enabled in both buckets One-way replication - from source to destination Handles unencrypted objects as well as objects encrypted with SSE-S3 & SSE-KMS (with extra config) Source bucket owner needs permissions to objects Does not replicate - systems events, Glacier or Glacier Deep Archive Does not replicate DELETE Why use replication .. ? SRR CRR Log aggregation Global Resiliency Improvements Prod and Test Sync Latency Reduction Resilience with strict sovereignty - regional","title":"S3 Replication"},{"location":"storage-services/s3/s3-select/","text":"S3 and Glacier Select allow you to use a SQL-Like statement to retrieve partial objects from S3 and Glacier. S3 can store HUGE objects (upto 5TB) You often want to retrieve the entire object Retrieving a 5TB object .. takes time, uses 5TB S3 Select S3/Glacier select let you use SQL-like statements Part of the object is pre-filtered by S3 Suports CSV, JSON and Parquet BZIP2 compression for CSV and JSON Cheaper and faster because S3 is pre-filtering the results before sending it to the client","title":"S3 select"},{"location":"storage-services/s3/s3-select/#s3-select","text":"S3/Glacier select let you use SQL-like statements Part of the object is pre-filtered by S3 Suports CSV, JSON and Parquet BZIP2 compression for CSV and JSON Cheaper and faster because S3 is pre-filtering the results before sending it to the client","title":"S3 Select"},{"location":"storage-services/s3/storage-classes/","text":"Storage Object Classes By default, objects are replicated across at least 3 AZs 200 OK for successful response TRANSFER IN is free but $ per GB is charged for TRANSFER OUT + a price per 1000 requests Comparing the Amazon S3 storage classes Storage class Designed for Durability (designed for) Availability (designed for) Availability Zones Min storage duration Min billable object size Other considerations S3 Standard Frequently accessed data (more than once a month) with millsecond access 99.999999999% 99.99% >= 3 None None None S3 Standard-IA Long-lived, infrequently accessed data (once a month) with millisecond access 99.999999999% 99.9% >= 3 30 days 128 KB Per GB retrieval fees apply. S3 Intelligent-Tiering Data with unknown, changing, or unpredictable access patterns 99.999999999% 99.9% >= 3 None None Monitoring and automation fees per object apply. No retrieval fees. S3 One Zone-IA Recreatable, infrequently accessed data (once a month) with millisecond access 99.999999999% 99.5% 1 30 days 128 KB Per GB retrieval fees apply. Not resilient to the loss of the Availability Zone. S3 Glacier Instant Retrieval Long-lived, archive data accessed once a quarter with millisecond access 99.999999999% 99.9% >= 3 90 days 128 KB Per GB retrieval fees apply. S3 Glacier Flexible Retrieval Long-lived archive data accessed once a year with retrieval times of minutes to hours 99.999999999% 99.99% (after you restore objects) >= 3 90 days 40 KB Per GB retrieval fees apply. You must first restore archived objects before you can access them. For information, see Restoring an archived object . S3 Glacier Deep Archive Long-lived archive data accessed less than once a year with retrieval times of hours 99.999999999% 99.99% (after you restore objects) >= 3 180 days 40 KB Per GB retrieval fees apply. You must first restore archived objects before you can access them. For information, see Restoring an archived object . RRS (not recommended) Noncritical, frequently accessed data with millisecond access 99.99% 99.99% >= 3 None None None S3 Standard, Standard IA, S3 One Zone-IA S3 Glacier Objects cannot be made publicly accessible any access of data (beyond object metadata) requires a retrieval process S3 Intelligent-Tiering","title":"Storage Classes"},{"location":"storage-services/s3/storage-classes/#storage-object-classes","text":"By default, objects are replicated across at least 3 AZs 200 OK for successful response TRANSFER IN is free but $ per GB is charged for TRANSFER OUT + a price per 1000 requests","title":"Storage Object Classes"},{"location":"storage-services/s3/storage-classes/#comparing-the-amazon-s3-storage-classes","text":"Storage class Designed for Durability (designed for) Availability (designed for) Availability Zones Min storage duration Min billable object size Other considerations S3 Standard Frequently accessed data (more than once a month) with millsecond access 99.999999999% 99.99% >= 3 None None None S3 Standard-IA Long-lived, infrequently accessed data (once a month) with millisecond access 99.999999999% 99.9% >= 3 30 days 128 KB Per GB retrieval fees apply. S3 Intelligent-Tiering Data with unknown, changing, or unpredictable access patterns 99.999999999% 99.9% >= 3 None None Monitoring and automation fees per object apply. No retrieval fees. S3 One Zone-IA Recreatable, infrequently accessed data (once a month) with millisecond access 99.999999999% 99.5% 1 30 days 128 KB Per GB retrieval fees apply. Not resilient to the loss of the Availability Zone. S3 Glacier Instant Retrieval Long-lived, archive data accessed once a quarter with millisecond access 99.999999999% 99.9% >= 3 90 days 128 KB Per GB retrieval fees apply. S3 Glacier Flexible Retrieval Long-lived archive data accessed once a year with retrieval times of minutes to hours 99.999999999% 99.99% (after you restore objects) >= 3 90 days 40 KB Per GB retrieval fees apply. You must first restore archived objects before you can access them. For information, see Restoring an archived object . S3 Glacier Deep Archive Long-lived archive data accessed less than once a year with retrieval times of hours 99.999999999% 99.99% (after you restore objects) >= 3 180 days 40 KB Per GB retrieval fees apply. You must first restore archived objects before you can access them. For information, see Restoring an archived object . RRS (not recommended) Noncritical, frequently accessed data with millisecond access 99.99% 99.99% >= 3 None None None","title":"Comparing the Amazon S3 storage classes"},{"location":"storage-services/s3/storage-classes/#s3-standard-standard-ia-s3-one-zone-ia","text":"","title":"S3 Standard, Standard IA, S3 One Zone-IA"},{"location":"storage-services/s3/storage-classes/#s3-glacier","text":"Objects cannot be made publicly accessible any access of data (beyond object metadata) requires a retrieval process","title":"S3 Glacier"},{"location":"storage-services/s3/storage-classes/#s3-intelligent-tiering","text":"","title":"S3 Intelligent-Tiering"}]}